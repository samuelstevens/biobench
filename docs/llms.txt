# biobench/interfaces.py

```python
"""
Common interfaces for models and tasks so that it's easy to add new models (which will work right away with all tasks) and easy to add new tasks (which will work right away with all models).

The model interface is `VisionBackbone`.
See `biobench.third_party_models` for examples of how to subclass it, and note that you have to call `biobench.register_vision_backbone` for it to show up.

The benchmark interface is informal, but is a function that matches the following signature:

```py
def benchmark(args: Args, model_args: tuple[str, str]) -> tuple[tuple[str, str], interfaces.TaskReport]:
    ...
```

In a Haskell-like signature, this is more like `Args -> (str, str) -> ((str, str), TaskReport)`.
"""

import dataclasses
import socket
import subprocess
import sys
import time
import typing

import beartype
import numpy as np
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from biobench import helpers


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class EncodedImgBatch:
    """The output of a `VisionBackbone`'s `VisionBackbone.img_encode()` method."""

    img_features: Float[Tensor, "batch img_dim"]
    """Image-level features. Each image is represented by a single vector."""
    patch_features: Float[Tensor, "batch n_patches patch_dim"] | None
    """Patch-level features. Only ViTs have patch-level features. These features might be a different dimension that the image features because of projection heads or such."""


@jaxtyped(typechecker=beartype.beartype)
class VisionBackbone(torch.nn.Module):
    """
    A frozen vision model that embeds batches of images into batches of vectors.

    To add new models to the benchmark, you can simply create a new class that satisfies this interface and register it.
    See `biobench.registry` for a tutorial on adding new vision backbones.
    """

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> EncodedImgBatch:
        """Encode a batch of images."""
        err_msg = f"{self.__class__.__name__} must implemented img_encode()."
        raise NotImplementedError(err_msg)

    def make_img_transform(self):
        """
        Return whatever function the backbone wants for image preprocessing.
        This should be an evaluation transform, not a training transform, because we are using the output features of this backbone as data and not updating this backbone.
        """
        err_msg = f"{self.__class__.__name__} must implemented make_img_transform()."
        raise NotImplementedError(err_msg)


def get_git_hash() -> str:
    """
    Returns the hash of the current git commit, assuming we are in a git repo.
    """
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("ascii").strip()


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Prediction:
    """An individual test prediction."""

    id: str
    """Whatever kind of ID; used to find the original image/example."""
    score: float
    """Test score; typically 0 or 1 for classification tasks."""
    info: dict[str, object]
    """Any additional information included. This might be the original class, the true label, etc."""


@beartype.beartype
def default_calc_mean_score(predictions: list[Prediction]) -> float:
    return np.mean([prediction.score for prediction in predictions]).item()


@beartype.beartype
def get_gpu_name() -> str:
    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).name
    else:
        return ""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TaskReport:
    """
    The result of running a benchmark task.
    """

    # Actual details of the report
    name: str
    """The benchmark name."""
    max_examples: int
    """The maximum number of training examples used."""
    predictions: list[Prediction]
    """A list of (example_id, score, info) objects"""
    _: dataclasses.KW_ONLY
    calc_mean_score: typing.Callable[[list[Prediction]], float] = (
        default_calc_mean_score
    )
    """A way to calculate mean score from a list of predictions."""
    splits: dict[str, float] = dataclasses.field(default_factory=dict)
    """Other scores that you would like to report. These do not have confidence intervals."""

    # Stuff for trying to reproduce this result. These are filled in by default.
    argv: list[str] = dataclasses.field(default_factory=lambda: sys.argv)
    """Command used to get this report."""
    commit: str = get_git_hash()
    """Git commit for this current report."""
    posix_time: float = dataclasses.field(default_factory=time.time)
    """Time when this report was constructed."""
    gpu_name: str = dataclasses.field(default_factory=get_gpu_name)
    """Name of the GPU that ran this experiment."""
    hostname: str = dataclasses.field(default_factory=socket.gethostname)
    """Machine hostname that ran this experiment."""

    def __repr__(self):
        return f"Report({self.name} with {len(self.predictions)} predictions)"

    def __str__(self):
        return repr(self)

    def get_mean_score(self) -> float:
        """
        Get the mean score of all predictions.
        """
        return self.calc_mean_score(self.predictions)

    def get_confidence_interval(
        self,
        statistic="mean",
        confidence: float = 95,
        n_resamples: int = 500,
        seed: int = 42,
    ) -> tuple[float, float]:
        """
        Get the confidence interval for the statistics (mean) by bootstrapping individual scores of the predictions.

        NOTE: it's crazy how much easier this would be in Jax. PyTrees of Predictions would simply contains batch dimensions, and then I would `jax.vmap(get_mean_score)(batched_predictions)`.
        """

        rng = np.random.default_rng(seed=seed)
        choices = rng.choice(
            len(self.predictions),
            size=(n_resamples, len(self.predictions)),
            replace=True,
        )

        scores = []
        for choice in helpers.progress(choices, desc=f"CI for {self.name}"):
            scores.append(self.calc_mean_score([self.predictions[i] for i in choice]))

        percentiles = (100 - confidence) / 2, (100 - confidence) / 2 + confidence
        lower, upper = np.percentile(scores, percentiles).tolist()

        return lower, upper

    def to_dict(self) -> dict[str, object]:
        """
        Returns a json-encodable dictionary representation of self.
        """
        return {
            "name": self.name,
            "predictions": [
                dataclasses.asdict(prediction) for prediction in self.predictions
            ],
            "argv": self.argv,
            "commit": self.commit,
            "posix_time": self.posix_time,
            "gpu_name": self.gpu_name,
            "hostname": self.hostname,
        }


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ModelArgsCvml:
    org: str
    ckpt: str


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ModelArgsVlm:
    ckpt: str
    temp: float = 1.0

```

# biobench/simpleshot.py

```python
"""
Implements normalized nearest-centroid classifiers, as described in [this paper](https://arxiv.org/abs/1911.04623).

If you use this work, be sure to cite the original work:

```
@article{wang2019simpleshot,
  title={Simpleshot: Revisiting nearest-neighbor classification for few-shot learning},
  author={Wang, Yan and Chao, Wei-Lun and Weinberger, Kilian Q and Van Der Maaten, Laurens},
  journal={arXiv preprint arXiv:1911.04623},
  year={2019}
}
```
"""

import collections.abc

import beartype
import numpy as np
import sklearn.neighbors
import torch
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[Tensor, "n_examples dim"],
) -> Float[Tensor, "n_examples dim"]:
    """L2-normalize a batch of features.

    Args:
        features: batch of $d$-dimensional vectors.

    Returns:
        batch of $d$-dimensional vectors with unit L2 norm.
    """
    norms = np.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@jaxtyped(typechecker=beartype.beartype)
def simpleshot(
    x_train: Float[Tensor, "n_train dim"],
    y_train: Int[Tensor, " n_train"],
    x_test: Float[Tensor, "n_test dim"],
    y_test: Int[Tensor, " n_test"],
    batch_size: int,
    device: str,
) -> Float[Tensor, " n_test"]:
    """
    Applies simpleshot to features. Returns the list of scores for x_test.

    Args:
        ...

    Returns:
        A tensor of 0/1 scores, one for each test example, only considering exact match.
    """
    x_mean = x_train.mean(axis=0, keepdims=True)

    x_train = x_train - x_mean
    x_train = l2_normalize(x_train)

    x_test = x_test - x_mean
    x_test = l2_normalize(x_test)

    clf = sklearn.neighbors.NearestCentroid()
    clf.fit(x_train, y_train)

    # Do this next step on the GPU to make it fast.
    # Goes from 1 batch/sec to 77 batch/sec
    centroids = torch.from_numpy(clf.centroids_).to(device)
    x_test = x_test.to(device)
    y_test = y_test.to(device)

    scores = []
    for start, stop in batched_idx(len(x_test), batch_size):
        x_batch = x_test[start:stop]
        y_batch = y_test[start:stop]
        distances = torch.linalg.vector_norm(x_batch[:, None] - centroids, axis=2)
        preds = torch.argmin(distances, dim=1)

        scores.append((preds == y_batch).type(torch.float32))

    return torch.cat(scores, axis=0)

```

# biobench/registry.py

```python
"""
Stores all vision backbones.
Users can register new custom backbones from their code to evaluate on biobench using `register_vision_backbone`.
As long as it satisfies the `biobench.interfaces.VisionBackbone` interface, it will work will all tasks.

.. include:: ./tutorial.md
"""

import logging

import beartype

from . import interfaces

logger = logging.getLogger(__name__)

_global_backbone_registry: dict[str, type[interfaces.VisionBackbone]] = {}


@beartype.beartype
def load_vision_backbone(model_org: str, ckpt: str) -> interfaces.VisionBackbone:
    """
    Load a pretrained vision backbone.
    """
    if model_org not in _global_backbone_registry:
        raise ValueError(f"Org '{model_org}' not found.")

    cls = _global_backbone_registry[model_org]
    return cls(ckpt)


def register_vision_backbone(model_org: str, cls: type[interfaces.VisionBackbone]):
    """
    Register a new vision backbone class.
    """
    if model_org in _global_backbone_registry:
        logger.warning("Overwriting key '%s' in registry.", model_org)
    _global_backbone_registry[model_org] = cls


def list_vision_backbones() -> list[str]:
    """
    List all vision backbone model orgs.
    """
    return list(_global_backbone_registry.keys())

```

# biobench/third_party_models.py

```python
import logging
import os

import beartype
from jaxtyping import Float, jaxtyped
from torch import Tensor

from biobench import interfaces

logger = logging.getLogger("third_party")


@beartype.beartype
def get_cache_dir() -> str:
    cache_dir = ""
    for var in ("BIOBENCH_CACHE", "HF_HOME", "HF_HUB_CACHE"):
        cache_dir = cache_dir or os.environ.get(var, "")
    return cache_dir or "."


@beartype.beartype
def get_ssl() -> bool:
    """
    Checks whether BIOBENCH_DISABLE_SSL is present in the environment.

    We use environment variables rather than a boolean argument because

    1. This is only needed on some systems, like OSC.
    2. Every benchmark needs it in exactly the same way, so it would show up in every benchmark script as more "noise".
    3. It is not manipulated throughout the running of the program. It's a global variable that's set at the start of the jobs.

    But in general, we should not use environment variables to manage program state.

    Returns:
        A boolean that's true if we should use SSL and false if not.
    """
    disable = os.environ.get("BIOBENCH_DISABLE_SSL", None)
    return not disable


@jaxtyped(typechecker=beartype.beartype)
class OpenClip(interfaces.VisionBackbone):
    """
    Loads checkpoints from [open_clip](https://github.com/mlfoundations/open_clip), an open-source reproduction of the original [CLIP](https://arxiv.org/abs/2103.00020) paper.

    Checkpoints are in the format `<ARCH>/<CKPT>`.
    Look at the [results file](https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv) for the pretrained models.
    For example, to load a ViT-B/16 train on Apple's Data Filtering Networks dataset, you would use `ViT-B-16/dfn2b`.
    """

    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import open_clip

        if not get_ssl():
            logger.warning("Ignoring SSL certs. Try not to do this!")
            # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761
            # Ideally we don't have to disable SSL but we are only downloading weights.
            import ssl

            ssl._create_default_https_context = ssl._create_unverified_context

        if ckpt.startswith("hf-hub:"):
            clip, self.img_transform = open_clip.create_model_from_pretrained(ckpt)
        else:
            arch, ckpt = ckpt.split("/")
            clip, self.img_transform = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=get_cache_dir()
            )

        self.model = clip.visual
        self.model.output_tokens = True  # type: ignore

    def make_img_transform(self):
        return self.img_transform

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> interfaces.EncodedImgBatch:
        result = self.model(batch)
        # Sometimes the model does not return patch features if it has none.
        if isinstance(result, tuple):
            img, patches = result
            return interfaces.EncodedImgBatch(img, patches)
        else:
            return interfaces.EncodedImgBatch(result, None)


@jaxtyped(typechecker=beartype.beartype)
class TimmVit(interfaces.VisionBackbone):
    """ """

    # TODO: docs + describe the ckpt format.
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import timm

        err_msg = "You are trying to load a non-ViT checkpoint; the `img_encode()` method assumes `model.forward_features()` will return features with shape (batch, n_patches, dim) which is not true for non-ViT checkpoints."
        assert "vit" in ckpt, err_msg
        self.model = timm.create_model(ckpt, pretrained=True)

        data_cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)
        self.img_transform = timm.data.create_transform(**data_cfg)

    def make_img_transform(self):
        return self.img_transform

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> interfaces.EncodedImgBatch:
        patches = self.model.forward_features(batch)
        # Use [CLS] token if it exists, otherwise do a maxpool
        if self.model.num_prefix_tokens > 0:
            img = patches[:, 0, ...]
        else:
            img = patches.max(axis=1).values

        # Remove all non-image patches, like the [CLS] token or registers
        patches = patches[:, self.model.num_prefix_tokens :, ...]

        return interfaces.EncodedImgBatch(img, patches)


@jaxtyped(typechecker=beartype.beartype)
class TorchvisionModel(interfaces.VisionBackbone):
    def __init__(self, ckpt: str):
        import torchvision

        arch, weights = ckpt.split("/")
        self.model = getattr(torchvision, arch)(weights=weights)
        self.model.eval()

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> interfaces.EncodedImgBatch:
        breakpoint()

    def make_img_transform(self):
        # Per the docs, each set of weights has its own transform: https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models
        return self.model.weights.transforms()

```

# biobench/__init__.py

```python
"""
Package for organizing all the code to run benchmarks.

Submodules are either tasks or helpers for organizing code.

The most important modules to understand are:

* `benchmark` because it is the launch script that runs all the tasks.
* `interfaces` because it defines how everything hooks together.
* Any of the task modules--`newt` is well documented.
* Any of the vision modules--`third_party_models.OpenClip` is highly relevant to anyone using the [open_clip](https://github.com/mlfoundations/open_clip) codebase to train models.

# Task Modules

* `ages`: Classify images of juvenile birds using only adult birds for training.
* `beluga`: Re-identification of Beluga whales using nearest neighbors.
* `birds525`: 1-shot classification with nearest-neighbor of the Kaggle Birds dataset.
* `fishnet`: Predict presense/absence of 9 different traits in images of fish.
* `iwildcam`: Species classification from camera trap imagery using multiclass ridge regression.
* `kabr`: Behavior classification of single-subject animals using simpleshot of mean frame representations.
* `newt`: 164 binary classification tasks using an binary SVM trained on image features.
* `plantnet`: long-tail classification of plant species.
* `plankton`: classification of microscopic images of phytoplankton.
# Helper Modules

* `interfaces`: All the interfaces that describe how the different modules communicate.
* `registry`: How to add/load vision models to BioBench.
* `simpleshot`: An implementation of nearest-centroid classification from [Simpleshot](https://arxiv.org/abs/1911.04623).
* `third_party_models`: Some pre-written vision backbones that load checkpoints from [open_clip](https://github.com/mlfoundations/open_clip) and [timm](https://github.com/huggingface/pytorch-image-models). You can use these as inspiration for writing your own vision backbones or check out `biobench.registry` for a tutorial.

# Future Tasks

These are tasks that I plan on adding but are not yet done.

* `rarespecies`: Waiting on a [bug](https://huggingface.co/datasets/imageomics/rare-species/discussions/8) in Huggingface's datasets library.
* [FishVista](https://github.com/Imageomics/Fish-Vista): I want to add trait classification: given patch-level features, can we train a linear probe to predict the presence/absense of a feature?

# Discussion of Task Importance

(TODO)

Main points:

* Some tasks are real tasks (KABR, Beluga).
* Others are made up (Birds525, RareSpecies).
* Some are in-between (NeWT, FishNet).
* Some are made up and we hypothesize that they predict performance on real tasks (ImageNet-1K, iNat2021).
* Likely, the real tasks are most important, but they are also the most specialized (and thus least likely to predict performance on other tasks).
* We don't know if ImageNet-1K or iNat2021 still predict performance on real, specialized tasks. They definitely did 2-5 years ago---is that still true with new models?

We can also compare tasks by the fundamental task and the types of images used.

| Name | Fundamental Task | Image Distribution |
|---|---|---|
| `ages` | Classification | object-centric in-situ RGB photos |
| `beluga` | Re-identification (via KNN) | top-down cropped RGB photos |
| `birds525` | Classification | object-centric in-situ RGB photos |
| `fishnet` | Functional trait prediction | lab specimen RGB photos |
| `imagenet` | Classification | object-centric in-situ RGB photos |
| `inat21` | Classification | object-centric in-situ RGB photos |
| `iwildcam` | Classification | camera-trap images, black-and-white |
| `kabr` | Classification (Behavior) | |
| `leopard` | Re-identification (via SVM) |
| `newt` | | object-centric in-situ RGB photos |
| `plantnet` | Classification | object-centric in-situ RGB photos |
| `plankton` | Classification | microscopic black-and-white slides |
| `rarespecies` | Classification | object-centric in-situ RGB photos |

Most of our tasks are species classification.
The only non-species classification tasks are Belgua whale and leopard re-identification, FishNet's functional trait prediction, and KABR's *behavior* classification.
The age task includes a train/test mismatch and iWildCam and the plankton task are different image distributions.

But Birds525, ImageNet, iNat21, Pl@ntNet, and RareSpecies all fill the same role.
I would expect performance on these tasks to be extremely correlated.

This raises the question: why do we have multiple tasks that fill the same role?

.. include:: ./confidence-intervals.md
"""

import typing

import tyro

from . import interfaces, kabr, newt, third_party_models
from .registry import (
    list_vision_backbones,
    load_vision_backbone,
    register_vision_backbone,
)

register_vision_backbone("timm-vit", third_party_models.TimmVit)
register_vision_backbone("open-clip", third_party_models.OpenClip)

# Some helpful types
if typing.TYPE_CHECKING:
    # Static type seen by language servers, type checkers, etc.
    ModelOrg = str
else:
    # Runtime type used by tyro.
    ModelOrg = tyro.extras.literal_type_from_choices(list_vision_backbones())


__all__ = [
    "interfaces",
    "load_vision_backbone",
    "register_vision_backbone",
    "list_vision_backbones",
    "newt",
    "kabr",
    "ModelOrg",
]

```

# biobench/llms.py

```python
import asyncio
import base64
import collections
import dataclasses
import io
import logging
import time

import beartype
import litellm
from PIL import Image

from . import interfaces

logger = logging.getLogger("vlms")

# Disable logging for packages that yap a lot.
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("LiteLLM").setLevel(logging.WARNING)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Example:
    image: Image.Image
    user: str
    assistant: str

    def to_history(self) -> list[object]:
        return [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": image_to_base64(self.image)},
                    },
                    {"type": "text", "text": self.user},
                ],
            },
            {"role": "assistant", "content": self.assistant},
        ]


@beartype.beartype
def fits(
    args: interfaces.ModelArgsVlm,
    examples: list[Example],
    image: Image.Image,
    user: str,
) -> bool:
    max_tokens = get_max_tokens(args)
    messages = []
    for example in examples:
        messages.extend(example.to_history())
    messages.append({
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {"url": image_to_base64(image)},
            },
            {"type": "text", "text": user},
        ],
    })
    n_tokens = litellm.token_counter(model=args.ckpt, messages=messages)
    return n_tokens <= max_tokens


@beartype.beartype
def get_max_tokens(args: interfaces.ModelArgsVlm) -> int:
    try:
        return litellm.get_max_tokens(args.ckpt)
    except Exception:
        pass

    if args.ckpt.endswith("google/gemini-2.0-flash-001"):
        return 1_000_000
    if args.ckpt.endswith("google/gemini-flash-1.5-8b"):
        return 1_000_000
    else:
        err_msg = f"Model {args.ckpt} isn't mapped yet by biobench or litellm."
        raise ValueError(err_msg)


@beartype.beartype
class RateLimiter:
    def __init__(self, max_rate: int, window_s: float = 1.0):
        self.max_rate = max_rate
        self.window_s = window_s
        self.timestamps = collections.deque()

    async def acquire(self):
        now = time.monotonic()

        # Remove timestamps older than our window
        while self.timestamps and now - self.timestamps[0] > self.window_s:
            self.timestamps.popleft()

        # If we're at max capacity, wait until oldest timestamp expires
        if len(self.timestamps) >= self.max_rate:
            wait_time = self.timestamps[0] + self.window_s - now
            if wait_time > 0:
                logger.info("Sleeping for %.2f seconds.", wait_time)
                await asyncio.sleep(wait_time)

        # Add current timestamp
        self.timestamps.append(time.monotonic())


@beartype.beartype
async def send(
    args: interfaces.ModelArgsVlm,
    examples: list[Example],
    image: Image.Image,
    user: str,
    *,
    max_retries: int = 5,
) -> str:
    """
    Send a message to the LLM and get the response.

    Args:
        args: Args for the VLM.
        examples: Few-shot examples.
        image: The input image.
        user: The user request.

    Returns:
        The LLM's response as a string.

    Raises:
        ValueError: If required settings are missing
        RuntimeError: If LLM call fails
    """

    # Get optional settings with defaults.
    temperature = 0.7 if args.temp is None else args.temp

    # Format messages for chat completion
    messages = []
    for example in examples:
        messages.extend(example.to_history())

    # Add current message
    messages.append({
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {"url": image_to_base64(image)},
            },
            {"type": "text", "text": user},
        ],
    })
    # Make LLM call with retries
    last_err = None

    for attempt in range(max_retries):
        try:
            if attempt > 0:
                # Exponential backoff: 2, 4, 8, 16, ... seconds
                wait_time_s = 2**attempt
                logger.warning(
                    "LLM call failed (attempt %d/%d): %s. Retrying in %d seconds.",
                    attempt,
                    max_retries,
                    last_err,
                    wait_time_s,
                )
                await asyncio.sleep(wait_time_s)

            # Make LLM call
            response = await litellm.acompletion(
                model=args.ckpt,
                messages=messages,
                temperature=temperature,
            )
        except RuntimeError as err:
            last_err = err
            if attempt == max_retries - 1:
                raise RuntimeError(f"Max retries ({max_retries}) exceeded: {err}")

        except litellm.APIConnectionError as err:
            should_retry = litellm._should_retry(err.status_code)
            if should_retry:
                last_err = err
                if attempt == max_retries - 1:
                    raise RuntimeError(f"Max retries ({max_retries}) exceeded: {err}")
                continue
            raise RuntimeError(f"Non-retryable API connection error: {err}") from err

        except (litellm.APIError, litellm.BadRequestError) as err:
            # For some godforsaken reason, litellm does not parse the rate-limit error response from OpenRouter.
            # It just raises a litellm.APIError, which happens when it gets a bad response.
            # So in the interest of not failing, if err.llm_provider is 'openrouter' we assume it's a rate limit, and try again.
            if getattr(err, "llm_provider", None) == "openrouter":
                # Treat as rate limit for OpenRouter
                last_err = err
                if attempt == max_retries - 1:
                    raise RuntimeError(f"Max retries ({max_retries}) exceeded: {err}")
                continue
            # For other providers, raise the error
            raise RuntimeError(f"API error: {err}") from err

        # Extract response and update history
        response = response.choices[0].message.content
        if response is None:
            return ""
        return response


@beartype.beartype
def image_to_base64(image: Image.Image) -> str:
    buf = io.BytesIO()
    image.save(buf, format="webp")
    b64 = base64.b64encode(buf.getvalue())
    s64 = b64.decode("utf8")
    return "data:image/webp;base64," + s64

```

# biobench/helpers.py

```python
"""
Useful helpers for more than two tasks that don't fit anywhere else.
"""

import collections.abc
import logging
import os.path
import time

import beartype


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress"):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)

    def __iter__(self):
        start = time.time()
        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if isinstance(self.it, collections.abc.Sized):
                    pred_min = (len(self) - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        len(self),
                        (i + 1) / len(self) * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        return len(self.it)


@beartype.beartype
def fs_safe(string: str) -> str:
    """Makes a string safe for filesystems by removing typical special characters."""
    return string.replace(":", "_").replace("/", "_")


@beartype.beartype
def write_hparam_sweep_plot(
    task: str,
    model: str,
    clf,
    x: str = "param_ridgeclassifier__alpha",
    y: str = "mean_test_score",
) -> str:
    import matplotlib.pyplot as plt
    import polars as pl

    df = pl.DataFrame(clf.cv_results_)

    fig, ax = plt.subplots()

    if "n_resources" in df.columns:
        for n_resources in df.get_column("n_resources").unique().sort():
            ax.scatter(
                x=df.filter(pl.col("n_resources") == n_resources)[x],
                y=df.filter(pl.col("n_resources") == n_resources)[y],
                label=f"{n_resources} ex.",
            )
        fig.legend()
    else:
        ax.scatter(x=df[x], y=df[y])

    ax.set_xlabel(x)
    ax.set_ylabel(y)
    ax.set_xscale("log")
    ax.set_title(model)

    fig.tight_layout()
    filepath = os.path.join("logs", f"{task}_{fs_safe(model)}_hparam.png")
    fig.savefig(filepath)
    return filepath

```

# biobench/inat21/download.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the iNat21 (mini) dataset.

Run with:

1. `python biobench/inat21/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.inat21.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import tarfile

import beartype
import requests
import tqdm
import tyro

val_images_url = "https://ml-inat-competition-datasets.s3.amazonaws.com/2021/val.tar.gz"
train_mini_images_url = (
    "https://ml-inat-competition-datasets.s3.amazonaws.com/2021/train_mini.tar.gz"
)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    val: bool = True
    """Whether to download validation images [8.4GB]."""
    train: bool = True
    """Whether to download (mini) train images [42GB]."""


@beartype.beartype
def download_tar(url: str, tar_path: str, chunk_size: int):
    r = requests.get(url, stream=True)
    r.raise_for_status()

    n_bytes = int(r.headers["content-length"])

    with open(tar_path, "wb") as fd:
        for chunk in tqdm.tqdm(
            r.iter_content(chunk_size=chunk_size),
            total=n_bytes / chunk_size,
            unit="b",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading",
        ):
            fd.write(chunk)


def extract_tar(tar_path: str, n_images: int, dir: str):
    with tarfile.open(tar_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=n_images + 10_000):
            tar.extract(member, path=dir, filter="data")


@beartype.beartype
def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    train_tar_path = os.path.join(args.dir, "train_mini.tar.gz")
    val_tar_path = os.path.join(args.dir, "val.tar.gz")

    if args.val:
        download_tar(val_images_url, val_tar_path, chunk_size)
        print(f"Downloaded validation images: {val_tar_path}.")

    extract_tar(val_tar_path, 100_000, args.dir)
    print("Extracted validation images.")

    if args.train:
        download_tar(train_mini_images_url, train_tar_path, chunk_size)
        print(f"Downloaded train (mini) images: {train_tar_path}.")

    extract_tar(train_tar_path, 500_000, args.dir)
    print("Extracted training images.")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/inat21/__init__.py

```python
"""
Trains a simple ridge regression classifier on visual representations for the iNat21 challenge.
In the challenge, there are 10K different species (classes).
We use the mini training set with 50 images per species, and test on the validation set, which has 10 images per species.

This task is a benchmark: it should help you understand how general a vision backbone's representations are.
This is not a true, real-world task.

If you use this task, be sure to cite the original iNat21 dataset paper:

```
@misc{inat2021,
  author={Van Horn, Grant and Mac Aodha, Oisin},
  title={iNat Challenge 2021 - FGVC8},
  publisher={Kaggle},
  year={2021},
  url={https://kaggle.com/competitions/inaturalist-2021}
}
```
"""

import dataclasses
import logging
import os

import beartype
import numpy as np
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
import torchvision.datasets
from jaxtyping import Float, Int, Shaped, jaxtyped

from biobench import helpers, interfaces, registry

logger = logging.getLogger("inat21")

n_classes = 10_000


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""

    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using validation data.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(*model_args)

    # 1. Get features
    val_features = get_features(args, backbone, is_train=False)
    train_features = get_features(args, backbone, is_train=True)

    # 2. Fit model.
    clf = init_clf()
    clf.fit(train_features.x, train_features.y)

    helpers.write_hparam_sweep_plot("inat21", model_args.ckpt, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = val_features.y
    pred_labels = clf.predict(val_features.x)

    examples = [
        interfaces.Example(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(
            helpers.progress(val_features.ids, desc="making Example()s", every=1_000),
            pred_labels,
            true_labels,
        )
    ]

    return model_args, interfaces.TaskReport("iNat21", examples)


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torchvision.datasets.ImageFolder):
    """
    Subclasses ImageFolder so that `__getitem__` includes the path, which we use as the ID.
    """

    def __getitem__(self, index: int) -> tuple[str, object, object]:
        """
        Args:
            index (int): Index

        Returns:
            tuple: (path, sample, target) where target is class_index of the target class.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return path, sample, target


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    split = "train_mini" if is_train else "val"
    root = os.path.join(args.datadir, split)
    if not os.path.isdir(root):
        msg = f"Path '{root}' doesn't exist. Did you download the iNat21 dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--inat21-args.datadir'; see --help for more."
        raise ValueError(msg)
    dataset = Dataset(root, img_transform)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=True,
    )

    all_ids, all_features, all_labels = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every, desc=split):
        ids, images, labels = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())
        all_labels.extend(labels)
        all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu().numpy()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels).numpy()

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(args: Args):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if args.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        factor=3,
    )

```

# biobench/plantnet/download.py

```python
import dataclasses
import os.path
import zipfile

import requests
import tqdm
import tyro

images_url = "https://zenodo.org/records/5645731/files/plantnet_300K.zip"


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images [29.5GB]."""
    unzip: bool = True
    """whether to unzip images."""


def main(args: Args):
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_zip_path = os.path.join(args.dir, "plantnet_300K.zip")

    if args.download:
        # Download images.
        r = requests.get(images_url, stream=True)
        r.raise_for_status()

        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_zip_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_zip_path}.")

    if args.unzip:
        # Unzip images.
        zip_file = zipfile.ZipFile(images_zip_path)
        names = zip_file.namelist()
        for filename in tqdm.tqdm(names, desc="Unzipping images."):
            zip_file.extract(filename, path=args.dir)


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/plantnet/__init__.py

```python
"""
Pl@ntNet is a "dataset with high label ambiguity and a long-tailed distribution" from NeurIPS 2021.
We fit a ridge classifier from scikit-learn to a backbone's embeddings and evaluate on the validation split.


There are two pieces that make Pl@ntNet more than a simple classification task:

1. Because of the long tail, we use `class_weight='balanced'` which adjusts weights based on class frequency.
2. We use macro F1 both to choose the alpha parameter and to evaluate the final classifier rather than accuracy due to the massive class imbalance.

If you use this task, please cite the original paper:

@inproceedings{plantnet-300k,
    author={Garcin, Camille and Joly, Alexis and Bonnet, Pierre and Lombardo, Jean-Christophe and Affouard, Antoine and Chouet, Mathias and Servajean, Maximilien and Lorieul, Titouan and Salmon, Joseph},
    booktitle={NeurIPS Datasets and Benchmarks 2021},
    title={{Pl@ntNet-300K}: a plant image dataset with high label ambiguity and a long-tailed distribution},
    year={2021},
}
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import sklearn.experimental.enable_halving_search_cv
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Float, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from biobench import helpers, interfaces, registry

logger = logging.getLogger("plantnet")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    labels: Shaped[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using cross validation splits.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(*model_args)

    # 1. Get features
    val_features = get_features(args, backbone, split="val")
    train_features = get_features(args, backbone, split="train")

    encoder = sklearn.preprocessing.OrdinalEncoder()
    all_labels = np.concatenate((val_features.labels, train_features.labels))
    encoder.fit(all_labels.reshape(-1, 1))

    # 2. Fit model.
    clf = init_clf(args)
    clf.fit(train_features.x, train_features.y(encoder))

    helpers.write_hparam_sweep_plot("plantnet", model_args.ckpt, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = val_features.y(encoder)
    pred_labels = clf.predict(val_features.x)

    examples = [
        interfaces.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(
            helpers.progress(val_features.ids, desc="Making examples", every=1_000),
            pred_labels,
            true_labels,
        )
    ]

    report = interfaces.TaskReport(
        "Pl@ntNet", examples, calc_mean_score=calc_macro_top1
    )
    return model_args, report


def calc_macro_top1(examples: list[interfaces.Prediction]) -> float:
    """
    Macro top-1 accuracy.
    """
    cls_examples = {}
    for example in examples:
        true_cls = example.info["y_true"]
        if true_cls not in cls_examples:
            cls_examples[true_cls] = []

        cls_examples[true_cls].append(example)

    cls_accs = []
    for examples in cls_examples.values():
        cls_accs.append(np.mean([example.score for example in examples]))
    return np.mean(cls_accs).item()


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    transform: typing.Any | None
    """Optional function function that transforms an image into a format expected by a neural network."""
    samples: list[tuple[str, str, str]]
    """List of all image ids, image paths, and classnames."""

    def __init__(self, root: str, transform):
        self.transform = transform
        self.samples = []
        if not os.path.exists(root) or not os.path.isdir(root):
            msg = f"Path '{root}' doesn't exist. Did you download the Pl@ntNet dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path as --dataset-dir PATH"
            raise RuntimeError(msg)

        for dirpath, dirnames, filenames in os.walk(root):
            image_class = os.path.relpath(dirpath, root)
            for filename in filenames:
                image_id = filename.removesuffix(".jpg")
                image_path = os.path.join(dirpath, filename)
                self.samples.append((image_id, image_path, image_class))

    def __getitem__(self, i: int) -> tuple[str, Float[Tensor, "3 width height"], str]:
        image_id, image_path, image_class = self.samples[i]
        image = Image.open(image_path)
        if self.transform is not None:
            image = self.transform(image)
        return image_id, image, image_class

    def __len__(self) -> int:
        return len(self.samples)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, *, split: str
) -> Features:
    images_dir_path = os.path.join(args.datadir, "images", split)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = Dataset(images_dir_path, img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    all_ids, all_features, all_labels = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(
        range(total), every=args.log_every, desc=f"Embed {split}"
    ):
        ids, images, labels = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features
            all_features.append(features.cpu())

        all_ids.extend(ids)

        all_labels.extend(labels)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(args: Args):
    alpha = np.pow(2.0, np.arange(-15, 11))
    if args.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0, class_weight="balanced"),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        # This uses sklearn.metrics.f1_score with average="macro"
        scoring="f1_macro",
        factor=3,
    )

```

# biobench/iwildcam/download.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "wilds",
#     "tqdm",
#     "tyro",
# ]
# ///
import dataclasses

import tyro
import wilds


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """where to save data."""
    download: bool = True
    """whether to download the data."""


def main(args: Args):
    wilds.get_dataset(dataset="iwildcam", download=args.download, root_dir=args.dir)


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/iwildcam/__init__.py

```python
"""
Fits a linear classifier that is trained using cross-entropy on the training set of iWildCam 2020.


"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
import wilds
import wilds.common.data_loaders
from jaxtyping import Float, Int, Shaped, jaxtyped

from biobench import helpers, interfaces, registry

logger = logging.getLogger("iwildcam")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Arguments for the iWildCam task."""

    batch_size: int = 2048
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
class MeanScoreCalculator:
    def __call__(self, examples: list[interfaces.Prediction]) -> float:
        y_pred = np.array([example.info["y_pred"] for example in examples])
        y_true = np.array([example.info["y_true"] for example in examples])
        score = sklearn.metrics.f1_score(
            y_true, y_pred, average="macro", labels=np.unique(y_true)
        )
        return score.item()


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    backbone = registry.load_vision_backbone(*model_args)

    # 1. Load dataloaders.
    transform = backbone.make_img_transform()
    if not os.path.exists(args.datadir) or not os.path.isdir(args.datadir):
        msg = f"Path '{args.datadir}' doesn't exist. Did you download the iWildCam dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path as --datadir PATH"
        raise RuntimeError(msg)
    dataset = wilds.get_dataset(
        dataset="iwildcam", download=False, root_dir=args.datadir
    )

    test_data = dataset.get_subset("test", transform=transform)
    test_dataloader = wilds.common.data_loaders.get_eval_loader(
        "standard",
        test_data,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
    )
    test_features = get_features(args, backbone, test_dataloader)
    logger.info("Got test features.")

    train_dataset = dataset.get_subset("train", transform=transform)
    train_dataloader = wilds.common.data_loaders.get_train_loader(
        "standard",
        train_dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
    )
    train_features = get_features(args, backbone, train_dataloader)
    logger.info("Got train features.")

    # 2. Fit model.
    clf = init_clf(args)
    clf.fit(train_features.x, train_features.y)

    helpers.write_hparam_sweep_plot("iwildcam", model_args, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    examples = [
        interfaces.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(
            helpers.progress(test_features.ids, desc="making examples", every=1_000),
            pred_labels,
            true_labels,
        )
    ]

    return model_args, interfaces.TaskReport(
        "iWildCam", examples, calc_mean_score=MeanScoreCalculator()
    )


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, dataloader
) -> Features:
    backbone = torch.compile(backbone.to(args.device))

    all_features, all_labels, all_ids = [], [], []

    # I don't do `for ... in dataloader` because early breaks were throwing exceptions.
    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every):
        images, labels, _ = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features
            all_features.append(features.cpu())

        all_labels.extend(labels)

        ids = (np.arange(len(labels)) + b * args.batch_size).astype(str)
        all_ids.append(ids)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = torch.tensor(all_labels).numpy()
    all_ids = np.concatenate(all_ids, axis=0)

    return Features(all_features, all_labels, all_ids)


def init_clf(args: Args):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if args.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.GridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        # This uses sklearn.metrics.f1_score with average="macro", just like our final score calculator.
        scoring="f1_macro",
    )

```

# biobench/plankton/download.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the SYKE-plankton_IFCB_2022 dataset.

Run with:

1. `python biobench/plankton/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.plankton.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os
import shutil
import zipfile

import requests
import tqdm
import tyro

train_url = "https://b2share.eudat.eu/api/files/63a79aff-4194-48c8-8055-0a73ecfcf183/phytoplankton_labeled.zip"
val_url = "https://b2share.eudat.eu/api/files/4a62bb1b-9bd0-4005-9217-7472ee6ed92c/phytoplankton_Ut%C3%B6_2021_labeled.zip"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""


def main(args: Args):
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    train_zip = os.path.join(args.dir, "train.zip")
    val_zip = os.path.join(args.dir, "val.zip")

    for filepath, url in [(train_zip, train_url), (val_zip, val_url)]:
        r = requests.get(url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(filepath, "wb") as fd:
            # Need to specify a manual progress bar in order to get units and such working.
            t = tqdm.tqdm(
                total=n_bytes,
                unit="B",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading images",
            )
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
            t.close()

        with zipfile.ZipFile(filepath, "r") as zip:
            for member in tqdm.tqdm(
                zip.infolist(), unit="img", desc="Extracting images"
            ):
                zip.extract(member, args.dir)

    # Move images to particular split-named folders.
    val_folder = "phytoplankton_Utö_2021_labeled"
    move(os.path.join(args.dir, val_folder), os.path.join(args.dir, "val"))
    train_folder = "labeled_20201020"
    move(os.path.join(args.dir, train_folder), os.path.join(args.dir, "train"))

    print(f"Downloaded, extracted and organized images in {args.dir}.")


def move(src: str, dst: str):
    """
    Moves _src_ to _dst_. If _dst_ exists, it will be overwritten.
    """
    if os.path.isdir(dst):
        shutil.rmtree(dst)
    os.rename(src, dst)


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/plankton/__init__.py

```python
"""
Classification of phytoplankton using ridge classifiers.
This task is particularly challenging because the image distribution is very different to typical pre-training datasets; it's all microscopic images in mono-channel (black and white).

If you use this task, please cite the original paper to propose this train/test split and the original datasets as well:

Paper:

```
@article{kaisa2022towards,
    author={Kraft, Kaisa  and Velhonoja, Otso  and Eerola, Tuomas  and Suikkanen, Sanna  and Tamminen, Timo  and Haraguchi, Lumi  and Ylöstalo, Pasi  and Kielosto, Sami  and Johansson, Milla  and Lensu, Lasse  and Kälviäinen, Heikki  and Haario, Heikki  and Seppälä, Jukka },
    title={Towards operational phytoplankton recognition with automated high-throughput imaging, near-real-time data processing, and convolutional neural networks},
    journal={Frontiers in Marine Science},
    volume={9},
    year={2022},
    url={https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2022.867695},
    doi={10.3389/fmars.2022.867695},
    issn={2296-7745},
}
```

Training data:

```
@misc{kaisa2022syke
    doi = {10.23728/B2SHARE.ABF913E5A6AD47E6BAA273AE0ED6617A},
    url = {https://b2share.eudat.eu/records/abf913e5a6ad47e6baa273ae0ed6617a},
    author = {Kraft, Kaisa and Velhonoja, Otso and Seppälä, Jukka and Hällfors, Heidi and Suikkanen, Sanna and Ylöstalo, Pasi and Anglès, Sílvia and Kielosto, Sami and Kuosa, Harri and Lehtinen, Sirpa and Oja, Johanna and Tamminen, Timo},
    keywords = {3.1.21 → Biology → Marine biology, phytoplankton image data set, imaging flow cytometry, Imaging FlowCytobot, IFCB, phytoplankton, Baltic Sea, image data, SYKE, Finnish Environment Institute, Marine Research Centre, Marine Ecological Research Laboratory, plankton image data, FINMARI},
    title = {SYKE-plankton_IFCB_2022},
    publisher = {https://b2share.eudat.eu},
    year = {2022},
    copyright = {open}
}
```

Evaluation data:

```
@misc{kaisa2021syke,
  doi = {10.23728/B2SHARE.7C273B6F409C47E98A868D6517BE3AE3},
  url = {https://b2share.eudat.eu/records/7c273b6f409c47e98a868d6517be3ae3},
  author = {Kraft, Kaisa and Haraguchi, Lumi and Velhonoja, Otso and Seppälä, Jukka},
  keywords = {3.1.21 → Biology → Marine biology, phytoplankton image data set, imaging flow cytometry, Imaging FlowCytobot, IFCB, Baltic Sea, image data, SYKE, Finnish Environment Institute, Marine Research Centre, Marine Ecological Research Laboratory, plankton image data, FINMARI, phytoplankton},
  title = {SYKE-plankton_IFCB_Utö_2021},
  publisher = {https://b2share.eudat.eu},
  year = {2022},
  copyright = {open}
}
```

This task was added because of interesting conversations with [Ekaterina Nepovinnykh](https://scholar.google.com/citations?user=lmYki4gAAAAJ) and [Heikki Kälviäinen](https://www.lut.fi/en/profiles/heikki-kalviainen).
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.svm
import torch
from jaxtyping import Float, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from biobench import helpers, interfaces, registry

logger = logging.getLogger("plankton")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Plankton task arguments."""

    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    labels: Shaped[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using cross validation splits.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(*model_args)

    # 1. Get features
    train_features = get_features(args, backbone, split="train")
    val_features = get_features(args, backbone, split="val")

    encoder = sklearn.preprocessing.OrdinalEncoder()
    all_labels = np.concatenate((val_features.labels, train_features.labels))
    encoder.fit(all_labels.reshape(-1, 1))

    # 2. Fit model.
    clf = init_clf(args)
    clf.fit(train_features.x, train_features.y(encoder))

    helpers.write_hparam_sweep_plot("plankton", model_args.ckpt, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    # 3. Predict.
    pred_labels = clf.predict(val_features.x)
    logger.info("Predicted classes for %d examples.", len(val_features.x))
    true_labels = val_features.y(encoder)

    examples = [
        interfaces.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(
            helpers.progress(val_features.ids, desc="Making examples", every=1_000),
            pred_labels,
            true_labels,
        )
    ]

    return model_args, interfaces.TaskReport("Plankton", examples)


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    transform: typing.Any | None
    """Optional function function that transforms an image into a format expected by a neural network."""
    samples: list[tuple[str, str, str]]
    """List of all image ids, image paths, and classnames."""

    def __init__(self, root: str, transform):
        self.transform = transform
        self.samples = []
        if not os.path.exists(root) or not os.path.isdir(root):
            msg = f"Path '{root}' doesn't exist. Did you download the plankton dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path as --plankton-args.datadir PATH."
            raise RuntimeError(msg)

        for dirpath, dirnames, filenames in os.walk(root):
            # TODO: there are random PDFs in these directories. You have to be careful to only get directories that are actually full of images.
            # Also need to assign the same integers to the same classnames.
            image_class = os.path.relpath(dirpath, root)
            for filename in filenames:
                if not filename.endswith(".png"):
                    continue
                image_id = filename.removesuffix(".png")
                image_path = os.path.join(dirpath, filename)
                self.samples.append((image_id, image_path, image_class))

    def __getitem__(self, i: int) -> tuple[str, Float[Tensor, "3 width height"], str]:
        image_id, image_path, image_class = self.samples[i]
        image = Image.open(image_path).convert("RGB")
        if self.transform is not None:
            image = self.transform(image)
        return image_id, image, image_class

    def __len__(self) -> int:
        return len(self.samples)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, *, split: str
) -> Features:
    images_dir_path = os.path.join(args.datadir, split)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = Dataset(images_dir_path, img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    all_ids, all_features, all_labels = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(
        range(total), every=args.log_every, desc=f"Embed {split}"
    ):
        ids, images, labels = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features
            all_features.append(features.cpu())

        all_ids.extend(ids)

        all_labels.extend(labels)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(args: Args):
    """
    Make a grid search cross-validation version of a RidgeClassifier.
    """
    alpha = np.pow(2.0, np.arange(-15, 11))
    if args.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.GridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0, class_weight="balanced"),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
    )

```

# biobench/ages/__init__.py

```python
"""
This task measures changes in performance with respect to the stage of life of a bird.
Specifically, we measure classification accuracy among 11 species in multiple settings:

1. Training images are adult, evaluation images are adult. This is the baseline.
2. Training images are juvenile, evaluation images are juvenile. Any drop in performance is likely a reflection on pre-training data distribution.
3. Training images are adult, evaluation images are juvenile. This measures whether model representations are robust to changes in stage of life, which is the opposite of what the original NeWT task measures. We report this number as the primary score.

We use the 11 juvenile vs adult tasks from NeWT, so if you use this task, be sure to cite that work (below).
We use a multiclass SVM from scikit learn.

To download the original data, follow the instructions in `biobench.newt.download`.

```
@inproceedings{van2021benchmarking,
  title={Benchmarking Representation Learning for Natural World Image Collections},
  author={Van Horn, Grant and Cole, Elijah and Beery, Sara and Wilber, Kimberly and Belongie, Serge and Mac Aodha, Oisin},
  booktitle={Computer Vision and Pattern Recognition},
  year={2021}
}
```
"""

import asyncio
import collections.abc
import dataclasses
import difflib
import logging
import os
import random
import re

import beartype
import numpy as np
import polars as pl
import scipy.stats
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.svm
import torch
from jaxtyping import Float, Int, Integer, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import helpers, interfaces, llms, registry

logger = logging.getLogger("ages")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Ages task arguments."""

    data: str = ""
    """dataset directory; where you downloaded this task's data to."""
    batch_size_cv: int = 256
    """batch size for computer vision model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    seed: int = 42
    """random seed."""
    max_examples: int = -1
    """Number of maximum training samples. Negative number means use all of them."""
    parallel: int = 5
    """Concurrent requests per second."""

    # Computed at runtime.
    device: str = "cuda"
    """(computed at runtime) which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) whether to run in debug mode."""


@beartype.beartype
def benchmark_cvml(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Run benchmark.

    Args:
        args: configuration for age task.
        model_args: args to load vision backbone.

    Returns:
        A tuple of model_args and the report describing the results.
    """
    # 1. Load model
    backbone = registry.load_vision_backbone(*model_args)

    # 2. Get features.
    tasks = get_all_tasks_cvml(args, backbone)

    # 3. For each task outlined above, evaluate representation quality.
    splits = {}
    for name, train, test in tasks:
        clf = init_clf()

        clf.fit(train.x, train.y)
        y_pred = clf.predict(test.x)
        examples = [
            interfaces.Example(str(id), float(pred == true), {})
            for id, pred, true in zip(test.ids, y_pred, test.y)
        ]
        test_acc = np.mean(y_pred == test.y)
        splits[name] = test_acc.item()

    return model_args, interfaces.TaskReport("Ages", examples, splits=splits)


@beartype.beartype
def benchmark_vlm(
    args: Args, model_args: interfaces.ModelArgsVlm
) -> tuple[interfaces.ModelArgsVlm, interfaces.TaskReport]:
    rng = random.Random(args.seed)

    splits = {}
    with asyncio.Runner() as loop:
        for task, dataset in get_all_tasks_vlm(args):
            limiter = llms.RateLimiter(args.parallel)
            semaphore = asyncio.Semaphore(args.parallel)

            @beartype.beartype
            async def run_one(
                fewshot_examples: list[llms.Example], test_example: SampleVlm
            ) -> interfaces.Prediction:
                async with semaphore:
                    await limiter.acquire()
                    assistant = await llms.send(
                        model_args,
                        fewshot_examples,
                        test_example.image,
                        test_example.make_user(rng),
                    )
                    pred = test_example.parse_assistant(assistant)
                    breakpoint()
                    return interfaces.Prediction(
                        test_example.image_id,
                        float(pred == test_example.classname),
                        info={
                            "task": task.name,
                            "gold": test_example.classname,
                            "pred": pred,
                        },
                    )

            @beartype.beartype
            async def run_all(
                submissions: list[tuple[list[llms.Example], SampleVlm]],
            ) -> list[interfaces.Prediction]:
                if args.debug:
                    submissions = submissions[:10]
                tasks = [asyncio.create_task(run_one(*args)) for args in submissions]
                preds = []
                for task in helpers.progress(tasks, every=1):
                    pred: interfaces.Prediction = await task
                    preds.append(pred)
                return preds

            llm_args = []
            i_train = rng.choices(task.train, k=args.max_examples)

            for i in task.test:
                test_example = dataset[i]

                # Try to fit them into a prompt.
                n_examples = 0
                fewshot_examples = []
                while llms.fits(
                    model_args,
                    fewshot_examples,
                    test_example.image,
                    test_example.make_user(rng),
                ) and (args.max_examples < 0 or n_examples < args.max_examples):
                    # Add another example.
                    n_examples += 1
                    fewshot_examples = [
                        dataset[j].to_example(rng) for j in i_train[:n_examples]
                    ]

                llm_args.append((fewshot_examples, test_example))

            preds = loop.run(run_all(llm_args))
            test_acc = np.mean([pred.score for pred in preds]).item()
            splits[task.name] = test_acc

    return model_args, interfaces.TaskReport(
        "Ages", args.max_examples, preds, splits=splits
    )


#########
# CV/ML #
#########


@jaxtyped(typechecker=beartype.beartype)
class DatasetCvml(torch.utils.data.Dataset):
    """
    A dataset that returns `(example id, image tensor, integer label)` tuples.
    """

    def __init__(self, dir: str, df, transform):
        self.transform = transform
        self.image_ids = df.get_column("id").to_list()
        self.labels = df.get_column("species_label").to_list()
        self.dir = dir

    def __getitem__(self, i: int) -> tuple[str, Float[Tensor, "3 width height"], int]:
        image_id = self.image_ids[i]
        image = Image.open(os.path.join(self.dir, f"{image_id}.jpg"))
        if self.transform is not None:
            image = self.transform(image)
        return image_id, image, self.labels[i]

    def __len__(self) -> int:
        return len(self.image_ids)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    """Inputs and outputs for a given task."""

    x: Float[np.ndarray, " n dim"]
    """Input features; from a `biobench.interfaces.VisionBackbone`."""
    y: Int[np.ndarray, " n"]
    """Class label."""
    ids: Shaped[np.ndarray, " n"]
    """Array of ids; could be strings, could be ints, etc."""


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_all_tasks_cvml(
    args: Args, backbone: interfaces.VisionBackbone
) -> collections.abc.Iterator[tuple[str, Features, Features]]:
    """
    Gets train and test features for all the different tasks being evaluated.

    Args:
        args: configuration for the ages task.
        backbone: the particular vision backbone being evaluated.

    Returns:
        An iterator of (taskname, train features, test features) tuples, one for each task (described in this module's docstring).
    """
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.data, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.data, images_dir_name)

    if not os.path.isfile(labels_csv_path):
        msg = f"Path '{labels_csv_path}' doesn't exist. Did you download the Newt dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--ages-args.data'; see --help for more."
        raise RuntimeError(msg)

    df = pl.read_csv(labels_csv_path).with_row_index()
    # Only get tasks about age.
    df = df.filter(pl.col("task").str.contains("ml_age"))
    # Add integer label for species (0-indexed).
    df = df.with_columns(species_label=pl.col("task").rank("dense") - 1)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = DatasetCvml(images_dir_path, df, img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    all_features, all_labels, all_ids = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every, desc="Embedding"):
        ids, images, labels = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features
            features = torch.nn.functional.normalize(features, dim=-1)
            all_features.append(features.cpu())

        all_ids.extend(ids)
        all_labels.extend(labels)

    all_features = torch.cat(all_features, dim=0).cpu().numpy()
    all_labels = torch.tensor(all_labels).numpy()
    all_ids = np.array(all_ids)

    tasks = (("adult", "adult"), ("not_adult", "not_adult"), ("adult", "not_adult"))
    for train, test in tasks:
        train_i = (
            df.select((pl.col("split") == "train") & (pl.col("text_label") == train))
            .to_numpy()
            .squeeze()
        )
        test_i = (
            df.select((pl.col("split") == "test") & (pl.col("text_label") == test))
            .to_numpy()
            .squeeze()
        )

        yield (
            f"{train}/{test}",
            Features(all_features[train_i], all_labels[train_i], all_ids[train_i]),
            Features(all_features[test_i], all_labels[test_i], all_ids[test_i]),
        )


def init_clf():
    """
    Create a new, randomly initialized SVM with a random hyperparameter search over kernel, C and gamma. It uses only 16 jobs in parallel to prevent overloading the CPUs on a shared machine.
    """
    return sklearn.model_selection.RandomizedSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.svm.SVC(C=1.0, kernel="rbf"),
        ),
        {
            "svc__C": scipy.stats.loguniform(a=1e-3, b=1e1),
            "svc__kernel": ["rbf", "linear", "sigmoid", "poly"],
            "svc__gamma": scipy.stats.loguniform(a=1e-4, b=1e-3),
        },
        n_iter=100,
        n_jobs=16,
        random_state=42,
    )


#######
# VLM #
#######

RAW_TO_CLASSNAME = {
    "ml_age_coopers_hawk": "Cooper's hawk",
    "ml_age_black_bellied_plover": "black-bellied plover",
    "ml_age_semipalmated_plover": "semipalmated plover",
    "ml_age_whimbrel": "whimbrel",
    "ml_age_rough_legged_hawk": "rough-legged hawk",
    "ml_age_swainsons_hawk": "Swainson's hawk",
    "ml_age_bald_eagle": "bald eagle",
    "ml_age_sanderling": "sanderling",
    "ml_age_dunlin": "dunlin",
    "ml_age_western_sandpiper": "western sandpiper",
    "ml_age_least_sandpiper": "least sandpiper",
    "ml_age_sharp_shinned_hawk": "sharp-shinned hawk",
}

CLASSNAMES = list(RAW_TO_CLASSNAME.values())


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class SampleVlm:
    image_id: str
    image: Image.Image
    classname: str

    def make_user(self, rng: random.Random) -> str:
        classnames = rng.sample(CLASSNAMES, k=len(CLASSNAMES))
        return f"What is this a picture of, {', '.join(classnames[:-1])} or {classnames[-1]}? Respond with your answer in bold."

    @property
    def assistant(self) -> str:
        return f"**{self.classname}**"

    def parse_assistant(self, assistant: str) -> str:
        pattern = re.compile(r"\*\*(.*)\*\*")
        match = pattern.match(assistant)
        if match:
            # Return the closest classname in bold.
            pred = difflib.get_close_matches(match.group(1), CLASSNAMES, cutoff=0.0)[0]
        else:
            # Get the closest classname.
            pred = difflib.get_close_matches(assistant, CLASSNAMES, cutoff=0.0)[0]

        return pred

    def to_example(self, rng: random.Random) -> llms.Example:
        return llms.Example(
            image=self.image,
            user=self.make_user(rng),
            assistant=self.assistant,
        )


@jaxtyped(typechecker=beartype.beartype)
class DatasetVlm(torch.utils.data.Dataset):
    """
    A dataset that returns SampleVlms.
    """

    def __init__(self, root: str, df):
        self.root = root

        self.image_ids = df.get_column("id").to_list()
        self.text_labels = df.get_column("task").to_list()

    def __getitem__(self, i: int) -> SampleVlm:
        image_id = self.image_ids[i]
        classname = RAW_TO_CLASSNAME[self.text_labels[i]]

        image = Image.open(os.path.join(self.root, f"{image_id}.jpg"))

        return SampleVlm(image_id, image, classname)

    def __len__(self) -> int:
        return len(self.image_ids)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TaskVlm:
    """
    Task is a group of indices for a VLM with a train/test split.
    """

    name: str
    train: Integer[np.ndarray, " n_train"]
    test: Integer[np.ndarray, " n_test"]

    def __repr__(self) -> str:
        return f"Task(name={self.name}, n_train={len(self.train)}, n_test={len(self.test)})"


@beartype.beartype
def get_all_tasks_vlm(
    args: Args,
) -> collections.abc.Iterator[tuple[TaskVlm, DatasetVlm]]:
    """
    Gets train and test features for all the different tasks being evaluated.

    Args:
        args: configuration for the ages task.
        backbone: the particular vision backbone being evaluated.

    Returns:
        An iterator of (taskname, train features, test features) tuples, one for each task (described in this module's docstring).
    """
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.data, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.data, images_dir_name)

    if not os.path.isfile(labels_csv_path):
        msg = f"Path '{labels_csv_path}' doesn't exist. Did you download the Newt dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--ages-args.data'; see --help for more."
        raise RuntimeError(msg)

    df = pl.read_csv(labels_csv_path).with_row_index()
    # Only get tasks about age.
    df = df.filter(pl.col("task").str.contains("ml_age"))
    # Add integer label for species (0-indexed).

    dataset = DatasetVlm(images_dir_path, df)

    tasks = (("adult", "adult"), ("not_adult", "not_adult"), ("adult", "not_adult"))
    for train, test in tasks:
        train_i = (
            df.select((pl.col("split") == "train") & (pl.col("text_label") == train))
            .to_numpy()
            .squeeze()
            .nonzero()[0]
        )
        test_i = (
            df.select((pl.col("split") == "test") & (pl.col("text_label") == test))
            .to_numpy()
            .squeeze()
            .nonzero()[0]
        )

        yield TaskVlm(f"{train}/{test}", train_i, test_i), dataset

```

# biobench/leopard/download.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Downloads the leopard re-id dataset from lila.science.
"""

import dataclasses
import os
import tarfile

import requests
import tqdm
import tyro

train_url = "http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/wild-me/leopard.coco.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configuration."""

    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images."""
    expand: bool = True
    """whether to expand tarfiles into a folder."""


def main(args: Args):
    """Download and unzip the data."""
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_tar_path = os.path.join(args.dir, "leopard.coco.tar.gz")

    if args.download:
        # Download images.
        r = requests.get(train_url, stream=True)
        r.raise_for_status()

        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_tar_path}.")

    if args.expand:
        with tarfile.open(images_tar_path, "r") as tar:
            for member in tqdm.tqdm(
                tar, desc="Extracting images", total=len(tar.getnames())
            ):
                tar.extract(member, path=args.dir, filter="data")

        print(f"Extracted images: {args.dir}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/leopard/__init__.py

```python
"""
Individual re-identification of African leopards (*Panthera pardus*) using [this LILA BC dataset](https://lila.science/datasets/leopard-id-2022/).

We use a simple but computationally expensive method, first proposed by Andrej Karpathy [in this notebook](https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb):

1. Embed all images using a vision backbone.
2. For each image, treat it as a test image and train a new SVC predicting the query image as positive and the other images as negative.
3. Choose the closest negative image based on the SVC's decision boundary as the returned image.
4. Give a score of 1.0 if this returned image is the same individual, otherwise 0.0.

Because this method requires training an SVM on every query image, it will train ~6800 SVMs.
However, this is an embarrassingly parallel task because none of the SVMs depend on each other.
We use the [joblib](https://joblib.readthedocs.io/en/stable/index.html) library and its `joblib.Parallel` class ([see this guide](https://joblib.readthedocs.io/en/stable/parallel.html)).

With 16 jobs, using multiprocessing, it takes about ~20 minutes on my lab's server.
With 16 jobs using *threading* it was predicted to take over 80 minutes.
I let it run for 10 minutes and it was stable in predicting 60+ minutes, so I settled on multiprocessing but with 24 jobs for more speed.
"""

import dataclasses
import logging
import os.path

import beartype
import joblib
import numpy as np
import sklearn.neighbors
import sklearn.preprocessing
import torch
import torchvision.datasets
from jaxtyping import Float, Shaped, jaxtyped
from torch import Tensor

from biobench import helpers, interfaces, registry

logger = logging.getLogger("leopard")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configuration for Leopard re-ID task."""

    batch_size: int = 256
    """Batch size for the vision backbone."""
    n_workers: int = 8
    """Number of dataloader workers."""
    log_every: int = 10
    """How often to log while getting features."""
    n_jobs: int = 16
    """How many SVMs to train in parallel."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Run the leopard re-ID benchmark. See this module's documentation for more details.
    """
    backbone = registry.load_vision_backbone(*model_args)

    # Embed all images.
    features = get_features(args, backbone)
    # Convert string names into integer labels.
    encoder = sklearn.preprocessing.OrdinalEncoder(dtype=int)
    y = encoder.fit_transform(features.labels.reshape(-1, 1)).reshape(-1)

    @beartype.beartype
    def predict(i: int, image_id) -> interfaces.Prediction:
        clf = sklearn.svm.LinearSVC(
            class_weight="balanced", verbose=False, max_iter=10000, tol=1e-6, C=0.1
        )
        svm_y = np.zeros(features.n)
        svm_y[i] = 1
        clf.fit(features.x, svm_y)
        sims = clf.decision_function(features.x)
        # The top result is always i, but we want the second-best result.
        pred_i = np.argsort(sims)[1]
        # TODO: we could also take the top k results and choose the most common.
        # Something like:
        #   pred_i = scipy.stats.mode(np.argsort(sims)[1:args.k+1]).mode

        example = interfaces.Prediction(str(image_id), float(y[pred_i] == y[i]), {})
        return example

    examples = joblib.Parallel(n_jobs=args.n_jobs)(
        joblib.delayed(predict)(i, image_id)
        for i, image_id in enumerate(
            helpers.progress(features.ids, every=args.log_every)
        )
    )

    return model_args, interfaces.TaskReport("LeopardID", examples)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    """
    A block of features.

    Note: In Jax, this could be a tuple of arrays, all with a leading dimension of `n`. Instead, in PyTorch, it's easier to make it its own class. Oh well.
    """

    x: Float[Tensor, "n dim"]
    """Input features; from a `biobench.interfaces.VisionBackbone`."""
    labels: Shaped[np.ndarray, " n"]
    """Individual name."""
    ids: Shaped[np.ndarray, " n"]
    """Array of image ids."""

    @property
    def n(self) -> int:
        return len(self.ids)


@beartype.beartype
@torch.no_grad
def get_features(args: Args, backbone: interfaces.VisionBackbone) -> Features:
    """
    Get a block of features from a vision backbone.

    Args:
        args: LeopardID arguments.
        backbone: visual backbone.
    """
    backbone_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    @jaxtyped(typechecker=beartype.beartype)
    def sample_transform(
        img, metadata: list[dict]
    ) -> tuple[Float[Tensor, "3 w h"], tuple[str, str]]:
        # tgt is always a list for some reason.
        metadata = metadata[0]
        x, y, w, h = metadata["bbox"]
        img = img.crop((x, y, x + w, y + h))
        return backbone_transform(img), (metadata["name"], str(metadata["image_id"]))

    if not os.path.isdir(args.datadir):
        msg = f"Path '{args.datadir}' doesn't exist. Did you download the leopard dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--leopard-args.datadir'; see --help for more."
        raise ValueError(msg)

    dataset = torchvision.datasets.CocoDetection(
        os.path.join(args.datadir, "leopard.coco", "images", "train2022"),
        os.path.join(
            args.datadir, "leopard.coco", "annotations", "instances_train2022.json"
        ),
        transforms=sample_transform,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
    )

    all_features, all_labels, all_ids = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every, desc="embed"):
        images, (labels, ids) = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())
        all_labels.extend(labels)
        all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)

    return Features(all_features, all_labels, all_ids)

```

# biobench/beluga/download.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Downloads the Begula whale dataset from lila.science.
"""

import dataclasses
import os
import tarfile

import requests
import tqdm
import tyro

train_url = "http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/wild-me/beluga.coco.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configuration."""

    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images."""
    expand: bool = True
    """whether to expand tarfiles into a folder."""


def main(args: Args):
    """Download and unzip the data."""
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_tar_path = os.path.join(args.dir, "beluga.coco.tar.gz")

    if args.download:
        # Download images.
        r = requests.get(train_url, stream=True)
        r.raise_for_status()
        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_tar_path}.")

    if args.expand:
        with tarfile.open(images_tar_path, "r") as tar:
            for member in tqdm.tqdm(
                tar, desc="Extracting images", total=len(tar.getnames())
            ):
                tar.extract(member, path=args.dir, filter="data")

        print(f"Extracted images: {args.dir}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/beluga/__init__.py

```python
"""
Individual re-identification of Beluga whales (*Delphinapterus leucas*) using [this LILA BC dataset](https://lila.science/datasets/beluga-id-2022/).

We use a very simple method:

1. Embed all images using a vision backbone.
2. For each image, treat it as a test image and find its nearest neighbor (k=1).
3. Give a score of 1.0 if the nearest neighbor is the same individual, otherwise 0.0.

You could improve this with nearest centroid classification, k>1, or any number of fine-tuning techniques.
But we are simply interested in seeing if models embed images of the same individual closer together in representation space.

If you use this task, please cite the original dataset paper and the paper that proposed this evaluation method:

```
@article{algasov2024understanding,
  title={Understanding the Impact of Training Set Size on Animal Re-identification},
  author={Algasov, Aleksandr and Nepovinnykh, Ekaterina and Eerola, Tuomas and K{\"a}lvi{\"a}inen, Heikki and Stewart, Charles V and Otarashvili, Lasha and Holmberg, Jason A},
  journal={arXiv preprint arXiv:2405.15976},
  year={2024}
}

@inproceedings{vcermak2024wildlifedatasets,
  title={WildlifeDatasets: An open-source toolkit for animal re-identification},
  author={{\v{C}}erm{\'a}k, Vojt{\v{e}}ch and Picek, Lukas and Adam, Luk{\'a}{\v{s}} and Papafitsoros, Kostas},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5953--5963},
  year={2024}
}
```
"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import sklearn.neighbors
import torch
import torchvision.datasets
from jaxtyping import Float, Shaped, jaxtyped
from torch import Tensor

from biobench import helpers, interfaces, registry

logger = logging.getLogger("beluga")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configuration for BelugaID task."""

    data: str = ""
    """dataset directory; where you downloaded this task's data to."""
    batch_size_cv: int = 256
    """batch size for computer vision model."""
    n_workers: int = 8
    """Number of dataloader workers."""
    log_every: int = 10
    """How often to log while getting features."""
    seed: int = 42
    """random seed."""
    parallel: int = 5
    """Concurrent requests per second."""

    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@beartype.beartype
def benchmark_cvml(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Run the BelugaID benchmark. See this module's documentation for more details.
    """
    backbone = registry.load_vision_backbone(*model_args)

    # Embed all images.
    features = get_features(args, backbone)
    # Convert string names into integer labels.
    encoder = sklearn.preprocessing.OrdinalEncoder(dtype=int)
    y = encoder.fit_transform(features.labels.reshape(-1, 1)).reshape(-1)

    clf = sklearn.neighbors.NearestNeighbors(n_neighbors=1)
    clf.fit(features.x, y)
    preds = clf.kneighbors(return_distance=False)

    logger.info("Constructing examples.")
    examples = [
        interfaces.Example(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(
            helpers.progress(features.ids, every=1_000), preds, y
        )
    ]
    logger.info("%d examples done.", len(examples))

    return model_args, interfaces.TaskReport("BelugaID", examples)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    """
    A block of features.

    Note: In Jax, this could be a tuple of arrays, all with a leading dimension of `n`. Instead, in PyTorch, it's easier to make it its own class. Oh well.
    """

    x: Float[Tensor, "n dim"]
    """Input features; from a `biobench.interfaces.VisionBackbone`."""
    labels: Shaped[np.ndarray, " n"]
    """Individual name."""
    ids: Shaped[np.ndarray, " n"]
    """Array of image ids."""

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)

    @property
    def n(self) -> int:
        return len(self.ids)


@beartype.beartype
@torch.no_grad
def get_features(args: Args, backbone: interfaces.VisionBackbone) -> Features:
    """
    Get a block of features from a vision backbone.

    Args:
        args: BelugaID arguments.
        backbone: visual backbone.
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    if not os.path.isdir(args.datadir):
        msg = f"Path '{args.datadir}' doesn't exist. Did you download the Beluga dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--beluga-args.datadir'; see --help for more."
        raise ValueError(msg)

    dataset = torchvision.datasets.CocoDetection(
        os.path.join(args.datadir, "beluga.coco", "images", "train2022"),
        os.path.join(
            args.datadir, "beluga.coco", "annotations", "instances_train2022.json"
        ),
        img_transform,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=True,  # We use dataset.shuffle instead
        collate_fn=lambda batch: tuple(zip(*batch)),
    )

    all_features, all_labels, all_ids = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every, desc="embed"):
        images, metadata = next(it)
        images = torch.stack(images).to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        labels = [meta[0]["name"] for meta in metadata]
        ids = [str(meta[0]["image_id"]) for meta in metadata]

        all_features.append(features.cpu())
        all_labels.extend(labels)
        all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = np.array(all_labels)

    return Features(all_features, all_labels, all_ids)

```

# biobench/birds525/download.py

```python
import dataclasses

import beartype
import tyro


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    write_to: str = "."
    """where to save the data."""


@beartype.beartype
def main(args: Args):
    print("Run:")
    print()
    print(
        f"  kaggle datasets download gpiosenka/100-bird-species --path {args.write_to}"
    )
    print()
    print("Then run:")
    print()
    print(f"  cd '{args.write_to}' && unzip 100-bird-species.zip")
    print()
    print()


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/birds525/__init__.py

```python
"""
Species classification of 525 different bird species; this dataset is from [Kaggle](https://www.kaggle.com/datasets/gpiosenka/100-bird-species) (although it was down when I wrote this docstring, several weeks after downloading the data).

We use simpleshot and mimic the setting in [BioCLIP](https://imageomics.github.io/bioclip/): we randomly select 1 example from each class, then evaluate on the validation set.

We are interested in understanding the variance associated with using different training examples.
However, the `biobench.interfaces.TaskReport.get_confidence_interval` method only passes a list of scored test examples, not a set of new training examples.
To get around this, the `benchmark` function re-samples training data and re-runs the simpleshot algorithm 100 times.
Then when the `biobench.interfaces.TaskReport.get_confidence_interval` is called, we randomly return one of the scores.
This gives a pretty tight confidence interval because `biobench.interfaces.TaskReport.get_confidence_interval` will do 500 resamples by default, but at least we re-run training with different training examples.
"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import torch
import torchvision.datasets
from jaxtyping import Float, Int, Shaped, jaxtyped
from torch import Tensor

from biobench import helpers, interfaces, registry, simpleshot

logger = logging.getLogger("birds525")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """
    Arguments for Birds525.
    """

    seed: int = 42
    """random seed."""
    data: str = ""
    """dataset directory; where you downloaded this task's data to."""

    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    n_repeats: int = 100
    """number of times to do 1-shot training."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@beartype.beartype
def benchmark_cvml(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    Runs simpleshot `Args.n_repeats` times (default 100) with 1 training example per class, then evaluates on the validation split.
    """

    backbone = registry.load_vision_backbone(*model_args)
    train_features = get_features(args, backbone, is_train=True)
    test_features = get_features(args, backbone, is_train=False)

    all_scores = []
    for r in range(args.n_repeats):
        i = choose_k_per_class(train_features.y, k=1)

        scores = simpleshot.simpleshot(
            train_features.x[i],
            train_features.y[i],
            test_features.x,
            test_features.y,
            args.batch_size,
            args.device,
        )
        all_scores.append(torch.mean(scores).cpu())
        if (r + 1) % args.log_every == 0:
            logger.info(
                "%d/%d simpleshot finished (%.1f%%)",
                r + 1,
                args.n_repeats,
                (r + 1) / args.n_repeats * 100,
            )

    all_scores = np.array(all_scores)

    # Just choose the last sampled result's examples.
    examples = [
        interfaces.Prediction(str(id), float(score), {})
        for id, score in zip(test_features.ids, scores.tolist())
    ]

    # We sort of cheat here. We run simpleshot n_repeats (100) times, then when we want to calculate the confidence intervals, we just choose the score of one of these simpleshot runs, regardless of what examples are passed.
    return model_args, interfaces.TaskReport(
        "Birds525-1shot",
        examples,
        calc_mean_score=ChooseRandomCachedResult(args.seed, all_scores),
    )


@jaxtyped(typechecker=beartype.beartype)
class ChooseRandomCachedResult:
    """
    We sort of cheat here. We run simpleshot `Args.n_repeats` (100) times, then when we want to calculate the confidence intervals, we just randomly choose the score of one of these simpleshot runs, regardless of what examples are passed.
    """

    def __init__(self, seed, scores: Float[np.ndarray, " n"]):
        self._scores = scores
        self._rng = np.random.default_rng(seed=seed)

    def __call__(self, examples: list[interfaces.Prediction]) -> float:
        return self._rng.choice(self._scores).item()


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    """
    A block of features.

    Note: In Jax, this could be a tuple of arrays, all with a leading dimension of `n`. Instead, in PyTorch, it's easier to make it its own class. Oh well.
    """

    x: Float[Tensor, " n dim"]
    """Input features; from a `biobench.interfaces.VisionBackbone`."""
    y: Int[Tensor, " n"]
    """Class label."""
    ids: Shaped[np.ndarray, " n"]
    """Array of ids; could be strings, could be ints, etc."""


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torchvision.datasets.ImageFolder):
    """
    Subclasses ImageFolder so that `__getitem__` includes the path, which we use as the ID.
    """

    def __getitem__(self, index: int) -> tuple[str, object, object]:
        """
        Args:
            index (int): Index

        Returns:
            tuple: (path, sample, target) where target is class_index of the target class.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return path, sample, target


@beartype.beartype
@torch.no_grad
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, *, is_train: bool
) -> Features:
    """
    Get a block of features from a vision backbone for a split (either train or test).

    Args:
        args: Birds525 arguments.
        backbone: visual backbone.
        is_train: whether you want training data or the test data.
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    split = "train" if is_train else "valid"

    root = os.path.join(args.datadir, split)
    if not os.path.isdir(root):
        msg = f"Path '{root}' doesn't exist. Did you download the Birds525 dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--datadir'; see --help for more."
        raise ValueError(msg)
    dataset = Dataset(os.path.join(args.datadir, split), img_transform)

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=True,
    )

    all_features, all_labels, all_ids = [], [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    logger.debug("Need to embed %d batches of %d images.", total, args.batch_size)
    for b in helpers.progress(range(total), every=args.log_every, desc=split):
        ids, images, labels = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())
        all_labels.extend(labels)
        all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
def choose_k_per_class(labels: Int[Tensor, " n"], *, k: int) -> Int[Tensor, " n_train"]:
    """
    Returns indices for a label set that include at most `k` examples per class.

    Args:
        labels: a list of integer labels for a set of data.
        k: the maximum number of examples per class.

    Returns:
        indices for `labels` such that at most `k` examples per class are in the data.
    """
    classes = np.unique(labels)

    train_indices = np.array([], dtype=int)

    # Iterate through each class to select indices
    for cls in classes:
        # Indices corresponding to the current class
        cls_indices = np.where(labels == cls)[0]
        # Randomly shuffle the indices
        np.random.shuffle(cls_indices)
        # Select the first K indices for the train set
        cls_train_indices = cls_indices[:k]
        # Append the selected indices to the train array
        train_indices = np.concatenate((train_indices, cls_train_indices))

    # Shuffle the indices to mix classes
    np.random.shuffle(train_indices)

    return torch.from_numpy(train_indices)

```

# biobench/newt/download.py

```python
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the NeWT dataset.

Run with:

1. `python biobench/newt/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.newt.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import tarfile

import requests
import tqdm
import tyro

images_url = (
    "https://ml-inat-competition-datasets.s3.amazonaws.com/newt/newt2021_images.tar.gz"
)
labels_url = "https://ml-inat-competition-datasets.s3.amazonaws.com/newt/newt2021_labels.csv.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    images: bool = True
    """Whether to download images [4.1GB]."""
    labels: bool = True
    """Whether to download labels."""


def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    labels_tar_path = os.path.join(args.dir, "labels.tar")
    images_tar_path = os.path.join(args.dir, "images.tar")
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.dir, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.dir, images_dir_name)

    if args.labels:
        # Download labels
        r = requests.get(labels_url, stream=True)
        r.raise_for_status()

        with open(labels_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
        print(f"Downloaded labels: {labels_tar_path}.")

    if args.images:
        # Download images.
        r = requests.get(images_url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(images_tar_path, "wb") as fd:
            for chunk in tqdm.tqdm(
                r.iter_content(chunk_size=chunk_size),
                total=n_bytes / chunk_size,
                unit="b",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading images",
            ):
                fd.write(chunk)
        print(f"Downloaded images: {images_tar_path}.")

    with tarfile.open(labels_tar_path, "r") as tar:
        tar.extract(labels_csv_name, path=args.dir, filter="data")
    print(f"Extracted labels: {labels_csv_path}.")

    with open(labels_csv_path) as fd:
        n_images = len(fd.read().split("\n")) - 1

    with tarfile.open(images_tar_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=n_images):
            tar.extract(member, path=args.dir, filter="data")
    print(f"Extracted images: {images_dir_path}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/newt/__init__.py

```python
"""
# NeWT: Natural World Tasks

NeWT is a collection of 164 binary classification tasks related to visual understanding of the natural world ([CVPR 2021 paper](https://arxiv.org/abs/2103.16483), [code](https://github.com/visipedia/newt/tree/main)).

We evaluate a vision model by extracting visual features for each image, fitting a linear SVM to the training examples, and evaluating on the test data.
We aggregate scores across all 164 tasks.

If you use this evaluation, be sure to cite the original work:

```
@inproceedings{van2021benchmarking,
  title={Benchmarking Representation Learning for Natural World Image Collections},
  author={Van Horn, Grant and Cole, Elijah and Beery, Sara and Wilber, Kimberly and Belongie, Serge and Mac Aodha, Oisin},
  booktitle={Computer Vision and Pattern Recognition},
  year={2021}
}
```
"""

import asyncio
import collections.abc
import dataclasses
import difflib
import itertools
import logging
import os
import random
import re

import beartype
import numpy as np
import polars as pl
import scipy.stats
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.svm
import torch
from jaxtyping import Bool, Float, Int, Integer, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import helpers, interfaces, llms, registry

logger = logging.getLogger("newt")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """NeWT task arguments."""

    data: str = ""
    """dataset directory; where you downloaded this task's data to."""
    batch_size_cv: int = 256
    """batch size for computer vision model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    seed: int = 42
    """random seed."""
    parallel: int = 5
    """Concurrent requests per second."""

    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@beartype.beartype
def benchmark_cvml(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    The NeWT benchmark.
    First, get features for all images.
    Second, select the subsets of features that correspond to different tasks and train an SVM.
    Third, evaluate the SVM and report results.
    """
    # 1. Load model
    backbone = registry.load_vision_backbone(*model_args)

    # 2. Get features.
    all_task_features = get_all_tasks_cvml(args, backbone)

    # Fit SVMs.
    results = []
    for task in all_task_features:
        (x_train, y_train), (x_test, y_test) = task.splits

        x_mean = x_train.mean(axis=0, keepdims=True)

        x_train = x_train - x_mean
        x_train = l2_normalize(x_train)

        x_test = x_test - x_mean
        x_test = l2_normalize(x_test)

        svc = init_svc()

        svc.fit(x_train, y_train)
        y_pred = svc.predict(x_test)
        examples = [
            interfaces.Prediction(
                str(id),
                float(pred == true),
                {"cluster": task.cluster, "task": task.name},
            )
            for id, pred, true in zip(task.example_ids, y_pred, y_test)
        ]
        test_acc = np.mean(y_pred == y_test)

        results.append({
            "task": task.name,
            "cluster": task.cluster,
            "examples": examples,
            "test_acc": test_acc,
        })

    # Removes 'examples' from each dict in results
    examples = list(
        itertools.chain.from_iterable((result.pop("examples") for result in results))
    )

    return model_args, interfaces.TaskReport("NeWT", examples)


@beartype.beartype
def benchmark_vlm(
    args: Args, model_args: interfaces.ModelArgsVlm
) -> tuple[interfaces.ModelArgsVlm, interfaces.TaskReport]:
    rng = random.Random(args.seed)

    results = []
    with asyncio.Runner() as loop:
        for task, dataset in get_all_tasks_vlm(args):
            limiter = llms.RateLimiter(args.parallel)
            semaphore = asyncio.Semaphore(args.parallel)

            @beartype.beartype
            async def run_one(
                fewshot_examples: list[llms.Example], test_example: SampleVlm
            ) -> interfaces.Prediction:
                async with semaphore:
                    await limiter.acquire()
                    assistant = await llms.send(
                        model_args,
                        fewshot_examples,
                        test_example.image,
                        test_example.make_user(rng),
                    )
                    pred_y = test_example.parse_assistant(assistant)
                    return interfaces.Prediction(
                        test_example.image_id,
                        float(pred_y == test_example.label),
                        info={"cluster": task.cluster, "task": task.name},
                    )

            @beartype.beartype
            async def run_all(
                submissions: list[tuple[list[llms.Example], SampleVlm]],
            ) -> list[interfaces.Prediction]:
                tasks = [asyncio.create_task(run_one(*args)) for args in submissions]
                preds = []
                for task in helpers.progress(tasks):
                    pred: interfaces.Prediction = await task
                    preds.append(pred)
                return preds

            llm_args = []
            i_train = rng.choices(task.train, k=args.max_examples)

            for i in task.test:
                test_example = dataset[i]

                # Try to fit them into a prompt.
                n_examples = 0
                fewshot_examples = []
                while (
                    llms.fits(
                        model_args,
                        fewshot_examples,
                        test_example.image,
                        test_example.make_user(rng),
                    )
                    and n_examples < args.max_examples
                ):
                    # Add another example.
                    n_examples += 1
                    fewshot_examples = [
                        dataset[j].to_example(rng) for j in i_train[:n_examples]
                    ]

                llm_args.append((fewshot_examples, test_example))

            preds = loop.run(run_all(llm_args))
            test_acc = np.mean([pred.score for pred in preds]).item()

            results.append({
                "task": task.name,
                "cluster": task.cluster,
                "predictions": preds,
                "test_acc": test_acc,
            })

    predictions = list(
        itertools.chain.from_iterable((result.pop("predictions") for result in results))
    )
    return model_args, interfaces.TaskReport("NeWT", args.max_examples, predictions)


########
# CVML #
########


@jaxtyped(typechecker=beartype.beartype)
class DatasetCvml(torch.utils.data.Dataset):
    """
    A dataset that returns `(example id, image tensor)` tuples.
    """

    def __init__(self, dir: str, df, transform=None):
        self.transform = transform
        self.image_ids = df.get_column("id").to_list()
        self.dir = dir

    def __getitem__(self, i: int) -> tuple[str, Float[Tensor, "3 width height"]]:
        image_id = self.image_ids[i]
        image = Image.open(os.path.join(self.dir, f"{image_id}.jpg"))
        if self.transform is not None:
            image = self.transform(image)
        return image_id, image

    def __len__(self) -> int:
        return len(self.image_ids)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TaskCvml:
    """
    Task is a group of features and labels for an SVM + a train/test split.
    """

    name: str
    cluster: str
    features: Float[np.ndarray, "batch dim"]
    labels: Int[np.ndarray, " batch"]
    is_train: Bool[np.ndarray, " batch"]
    example_ids: Shaped[np.ndarray, " batch"]  # Should be String[...]

    def __repr__(self) -> str:
        return f"Task(task={self.name}, cluster={self.cluster}, features={self.features.shape})"

    @property
    def splits(
        self,
    ) -> tuple[
        tuple[Float[np.ndarray, "n_train dim"], Int[np.ndarray, " n_train"]],
        tuple[Float[np.ndarray, "n_test dim"], Int[np.ndarray, " n_test"]],
    ]:
        """
        The features and labels for train and test splits.

        Returned as `(x_train, y_train), (x_test, y_test)`.
        """
        x_train = self.features[self.is_train]
        y_train = self.labels[self.is_train]
        x_test = self.features[~self.is_train]
        y_test = self.labels[~self.is_train]

        return (x_train, y_train), (x_test, y_test)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_all_tasks_cvml(
    args: Args, backbone: interfaces.VisionBackbone
) -> collections.abc.Iterator[TaskCvml]:
    """ """
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.data, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.data, images_dir_name)

    if not os.path.isfile(labels_csv_path):
        msg = f"Path '{labels_csv_path}' doesn't exist. Did you download the Newt dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--data'; see --help for more."
        raise RuntimeError(msg)

    df = pl.read_csv(labels_csv_path).with_row_index()

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = DatasetCvml(images_dir_path, df, img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    all_features, all_ids = [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every, desc="embed"):
        ids, images = next(it)
        images = images.to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features
            features = torch.nn.functional.normalize(features, dim=-1)
            all_features.append(features.cpu())

        all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)

    for task in df.get_column("task").unique():
        task_df = df.filter(pl.col("task") == task)

        task_idx = task_df.get_column("index").to_numpy()
        features = all_features[task_idx].numpy()
        ids = all_ids[task_idx]

        labels = task_df.get_column("label").to_numpy()
        is_train = task_df.select(pl.col("split") == "train").get_column("split")

        cluster = task_df.item(row=0, column="task_cluster")
        yield TaskCvml(task, cluster, features, labels, is_train.to_numpy(), ids)


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[np.ndarray, "batch dim"],
) -> Float[np.ndarray, "batch dim"]:
    """Normalizes a batch of vectors to have L2 unit norm."""
    norms = np.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


def init_svc():
    """Create a new, randomly initialized SVM with a random hyperparameter search over kernel, C and gamma. It uses only 16 jobs in parallel to prevent overloading the CPUs on a shared machine."""
    return sklearn.model_selection.RandomizedSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.svm.SVC(C=1.0, kernel="rbf"),
        ),
        {
            "svc__C": scipy.stats.loguniform(a=1e-3, b=1e1),
            "svc__kernel": ["rbf", "linear", "sigmoid", "poly"],
            "svc__gamma": scipy.stats.loguniform(a=1e-4, b=1e-3),
        },
        n_iter=100,
        n_jobs=16,
        random_state=42,
    )


#######
# VLM #
#######


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class SampleVlm:
    image_id: str
    image: Image.Image
    label: int

    # TODO: these classnames are not being translated correctly.
    classnames: tuple[str, str]

    @property
    def classname(self) -> str:
        return self.classnames[self.label]

    def make_user(self, rng: random.Random) -> str:
        a, b = self.classnames
        if rng.random() > 0.5:
            a, b = b, a
        return f"What is this a picture of, '{a}' or '{b}'? Respond with your answer in bold."

    @property
    def assistant(self) -> str:
        return f"**{self.classname}**"

    def parse_assistant(self, assistant: str) -> int:
        pattern = re.compile(r"\*\*(.*)\*\*")
        match = pattern.match(assistant)
        if match:
            # Return the closest classname in bold.
            pred = difflib.get_close_matches(
                match.group(1), self.classnames, cutoff=0.0
            )[0]
        else:
            # Get the closest classname.
            pred = difflib.get_close_matches(assistant, self.classnames, cutoff=0.0)[0]

        for i, classname in enumerate(self.classnames):
            if classname == pred:
                return i
        logger.warning("Something is wrong in parse_assistant.")
        return 0

    def to_example(self, rng: random.Random) -> llms.Example:
        return llms.Example(
            image=self.image,
            user=self.make_user(rng),
            assistant=self.assistant,
        )


@jaxtyped(typechecker=beartype.beartype)
class DatasetVlm(torch.utils.data.Dataset):
    """
    A dataset that returns SampleVlms.
    """

    def __init__(self, root: str, df):
        self.root = root

        self.image_ids = df.get_column("id").to_list()
        self.labels = df.get_column("label").to_list()
        self.tasks = df.get_column("task").to_list()

    def __getitem__(self, i: int) -> SampleVlm:
        image_id = self.image_ids[i]
        label = self.labels[i]
        task = self.tasks[i]

        classnames = tuple(text_label_to_classname[task].keys())
        image = Image.open(os.path.join(self.root, f"{image_id}.jpg"))

        return SampleVlm(
            image_id,
            image,
            label,
            classnames,
        )

    def __len__(self) -> int:
        return len(self.image_ids)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TaskVlm:
    """
    Task is a group of indices for a VLM with a train/test split.
    """

    name: str
    cluster: str
    train: Integer[np.ndarray, " n_train"]
    test: Integer[np.ndarray, " n_test"]

    def __repr__(self) -> str:
        return f"Task(name={self.name}, cluster={self.cluster}, n_train={len(self.train)}, n_test={len(self.test)})"


@jaxtyped(typechecker=beartype.beartype)
def get_all_tasks_vlm(
    args: Args,
) -> collections.abc.Iterator[tuple[TaskVlm, DatasetVlm]]:
    """ """
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.data, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.data, images_dir_name)

    if not os.path.isfile(labels_csv_path):
        msg = f"Path '{labels_csv_path}' doesn't exist. Did you download the Newt dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--data'; see --help for more."
        raise RuntimeError(msg)

    df = pl.read_csv(labels_csv_path).with_row_index()
    dataset = DatasetVlm(images_dir_path, df)

    for task in df.get_column("task").unique():
        task_df = df.filter(pl.col("task") == task)

        task_idx = task_df.get_column("index").to_numpy()
        is_train = task_df.select(pl.col("split") == "train").get_column("split")
        cluster = task_df.item(row=0, column="task_cluster")

        yield TaskVlm(task, cluster, task_idx[is_train], task_idx[~is_train]), dataset


text_label_to_classname = {
    # FGVCX
    "fgvcx_plant_pathology_healthy_vs_sick": {
        "healthy": "healthy plant",
        "sick": "sick plant",
    },
    "fgvcx_icassava_healthy_vs_sick": {
        "healthy": "healthy cassava leaf",
        "sick": "sick cassave leaf",
    },
    # ML Photo
    # (sam) we should change the openai templates for this task
    "ml_photo_rating_12_vs_45_v1": {
        "rating_4_or_5": "obvious bird",
        "rating_1_or_2": "obscured bird",
    },
    "ml_photo_rating_12_vs_45_v2": {
        "rating_4_or_5": "obvious bird",
        "rating_1_or_2": "obscured bird",
    },
    "ml_photo_rating_12_vs_45_v3": {
        "rating_4_or_5": "obvious bird",
        "rating_1_or_2": "obscured bird",
    },
    "ml_photo_rating_12_vs_45_v4": {
        "rating_4_or_5": "obvious bird",
        "rating_1_or_2": "obscured bird",
    },
    "ml_photo_rating_12_vs_45_v5": {
        "rating_4_or_5": "obvious bird",
        "rating_1_or_2": "obscured bird",
    },
    # ML Bio
    "ml_bio_has_red_eyes": {
        "has_red_eyes": "bird with red eyes",
        "not_red_eyes": "bird",
    },
    "ml_bio_high_contrast": {
        "high_contrast": "bird with high contrast in its colors",
        "not_high_contrast": "bird with low contrast in its colors",
    },
    "ml_bio_raptor_utility_pole": {
        "neg": "raptor in the wild",
        "raptor_on_pole": "raptor on a utility pole",
    },
    "ml_bio_is_at_flower": {
        "is_at_flower": "bird near a flower",
        "not_at_flower": "bird in the wild",
    },
    # ML Tag
    "ml_tag_back_of_camera": {
        "back_of_camera": "photo of a bird",
        "not_back_of_camera": "bird in the wild",
    },
    "ml_tag_copulation": {
        "not_copulation": "bird(s) in the wild",
        "copulation": "birds mating",
    },
    "ml_tag_feeding_young": {
        "not_feeding_young": "bird in the wild",
        "feeding_young": "bird feeding its young",
    },
    "ml_tag_egg": {
        "egg": "egg",
        "no_egg": "bird",
    },
    "ml_tag_watermark": {
        "no_watermark": "bird",
        "watermark": "bird with a watermark",
    },
    "ml_tag_field_notes_sketch": {
        "field_notes_sketch": "field drawing of a bird",
        "not_field_notes_sketch": "bird",
    },
    "ml_tag_nest": {
        "no_nest": "bird in the wild",
        "nest": "bird in its nest",
    },
    "ml_tag_molting_waterfowl": {
        "has_red_eyes": "molting waterfowl",
        "not_red_eyes": "regular waterfowl",
    },
    "ml_tag_molting_raptors": {
        "molting": "molting raptor",
        "not_molting": "regular raptor",
    },
    "ml_tag_vocalizing": {
        "not_vocalizing": "bird with its mouth closed",
        "vocalizing": "vocalizing bird",
    },
    "ml_tag_dead": {
        "not_dead": "living bird",
        "dead": "dead bird",
    },
    "ml_tag_in_hand": {
        "in_hand": "bird in a human hand",
        "not_in_hand": "bird in the wild",
    },
    "ml_tag_multiple_species": {
        "single_species": "one single species",
        "multiple_species": "multiple different species",
    },
    "ml_tag_carrying_food": {
        "not_carrying_food": "bird",
        "carrying_food": "bird carrying food",
    },
    # (sam) I used typical here instead of regular
    "ml_tag_foraging_waterfowl": {
        "not_waterfowl_foraging": "typical waterfowl",
        "waterfowl_foraging": "foraging waterfowl",
    },
    # I want the non_bird to not include the word bird in it.
    "ml_tag_non_bird": {
        "non_bird": "some thing",
        "not_non_bird": "bird",
    },
    # ML Age
    "ml_age_coopers_hawk": {
        "adult": "adult Cooper's hawk",
        "not_adult": "juvenile Cooper's hawk",
    },
    "ml_age_black_bellied_plover": {
        "not_adult": "juveile black-bellied plover",
        "adult": "adult black-bellied plover",
    },
    "ml_age_semipalmated_plover": {
        "adult": "adult semipalmated plover",
        "not_adult": "juvenile semipalmated plover",
    },
    "ml_age_whimbrel": {
        "not_adult": "juvenile whimbrel",
        "adult": "adult whimbrel",
    },
    "ml_age_rough_legged_hawk": {
        "adult": "adult rough-legged hawk",
        "not_adult": "juvenile rough-legged hawk",
    },
    "ml_age_swainsons_hawk": {
        "not_adult": "juvenile Swainson's hawk",
        "adult": "adult Swainson's hawk",
    },
    "ml_age_bald_eagle": {
        "not_adult": "juvenile bald eagle",
        "adult": "adult bald eagle",
    },
    "ml_age_sanderling": {
        "adult": "adult sanderling",
        "not_adult": "juvenile sanderling",
    },
    "ml_age_dunlin": {
        "adult": "adult dunlin",
        "not_adult": "juvenile dunlin",
    },
    "ml_age_western_sandpiper": {
        "not_adult": "juvenile western sandpiper",
        "adult": "adult wester sandpiper",
    },
    "ml_age_least_sandpiper": {
        "not_adult": "juvenile least sandpiper",
        "adult": "adult least sandpiper",
    },
    "ml_age_sharp_shinned_hawk": {
        "adult": "adult sharp-shinned hawk",
        "not_adult": "juvenile sharp-shinned hawk",
    },
    # NABirds
    "nabirds_species_classification_amekes_merlin": {
        "Merlin": "merlin",
        "American Kestrel": "American kestrel",
    },
    "nabirds_species_classification_botgra_grtgra": {
        "Boat-tailed Grackle": "boat-tailed grackle",
        "Great-tailed Grackle": "great-tailed grackle",
    },
    "nabirds_species_classification_easmea_wesmea": {
        "Eastern Meadowlark": "eastern meadowlark",
        "Western Meadowlark": "western meadowlark",
    },
    "nabirds_species_classification_orcwar_tenwar": {
        "Tennessee Warbler": "Tennessee warbler",
        "Orange-crowned Warbler": "orange-crowned warbler",
    },
    "nabirds_species_classification_houwre_winwre3": {
        "House Wren": "house wren",
        "Winter Wren": "winter wren",
    },
    "nabirds_species_classification_buhvir_casvir": {
        "Blue-headed Vireo": "blue-headed vireo",
        "Cassin's Vireo": "Cassin's vireo",
    },
    "nabirds_species_classification_cavswa_cliswa": {
        "Cave Swallow": "cave swallow",
        "Cliff Swallow": "cliff swallow",
    },
    "nabirds_species_classification_blkvul_turvul": {
        "Turkey Vulture": "turkey vulture",
        "Black Vulture": "black vulture",
    },
    "nabirds_species_classification_bkchum_rthhum": {
        "Black-chinned Hummingbird": "black-chinned hummingbird",
        "Ruby-throated Hummingbird": "ruby-throated hummingbird",
    },
    "nabirds_species_classification_gloibi_whfibi": {
        "Glossy Ibis": "glossy ibis",
        "White-faced Ibis": "white-faced ibis",
    },
    "nabirds_species_classification_brwhaw_reshaw": {
        "Red-shouldered Hawk": "red-shouldered hawk",
        "Broad-winged Hawk": "broad-winged hawk",
    },
    "nabirds_species_classification_bargol_comgol": {
        "Barrow's Goldeneye": "Barrow's goldeneye",
        "Common Goldeneye": "common goldeneye",
    },
    "nabirds_species_classification_amecro_comrav": {
        "American Crow": "American crow",
        "Common Raven": "common raven",
    },
    "nabirds_species_classification_coohaw_shshaw": {
        "Sharp-shinned Hawk": "sharp-shinned hawk",
        "Cooper's Hawk": "Cooper's hawk",
    },
    "nabirds_species_classification_savspa_sonspa": {
        "Song Sparrow": "song sparrow",
        "Savannah Sparrow": "savannah sparrow",
    },
    "nabirds_species_classification_linspa_sonspa": {
        "Lincoln's Sparrow": "Lincoln's sparrow",
        "Song Sparrow": "song sparrow",
    },
    "nabirds_species_classification_gresca_lessca": {
        "Greater Scaup": "greater scaup",
        "Lesser Scaup": "lesser scaup",
    },
    "nabirds_species_classification_eawpew_wewpew": {
        "Eastern Wood-Pewee": "eastern wood-pewee",
        "Western Wood-Pewee": "western wood-pewee",
    },
    "nabirds_species_classification_herthr_swathr": {
        "Hermit Thrush": "hermit thrush",
        "Swainson's Thrush": "Swainson's thrush",
    },
    "nabirds_species_classification_greyel_lesyel": {
        "Lesser Yellowlegs": "lesser yellowlegs",
        "Greater Yellowlegs": "greater yellowlegs",
    },
    "nabirds_species_classification_linspa_savspa": {
        "Lincoln's Sparrow": "Lincoln's sparrow",
        "Savannah Sparrow": "savannah sparrow",
    },
    "nabirds_species_classification_houfin_purfin": {
        "Purple Finch": "purple finch",
        "House Finch": "house finch",
    },
    "nabirds_species_classification_cacgoo1_cangoo": {
        "Canada Goose": "Canada goose",
        "Cackling Goose": "cackling goose",
    },
    "nabirds_species_classification_semsan_wessan": {
        "Semipalmated Sandpiper": "semipalmated sandpiper",
        "Western Sandpiper": "western sandpiper",
    },
    "nabirds_species_classification_canvas_redhea": {
        "Redhead": "redhead",
        "Canvasback": "canvasback",
    },
    "nabirds_species_classification_hergul_ribgul": {
        "Ring-billed Gull": "ring-billed gull",
        "Herring Gull": "herring gull",
    },
    "nabirds_species_classification_truswa_tunswa": {
        "Tundra Swan": "tundra swan",
        "Trumpeter Swan": "trumpeter swan",
    },
    "nabirds_species_classification_bkcchi_carchi": {
        "Carolina Chickadee": "Carolina chickadee",
        "Black-capped Chickadee": "black-capped chickadee",
    },
    "nabirds_species_classification_solsan_sposan": {
        "Spotted Sandpiper": "spotted sandpiper",
        "Solitary Sandpiper": "solitary sandpiper",
    },
    "nabirds_species_classification_rosgoo_snogoo": {
        "Snow Goose": "snow goose",
        "Ross's Goose": "Ross's goose",
    },
    "nabirds_species_classification_dowwoo_haiwoo": {
        "Hairy Woodpecker": "hairy woodpecker",
        "Downy Woodpecker": "downy woodpecker",
    },
    "nabirds_species_classification_buhvir_plsvir": {
        "Plumbeous Vireo": "plumbeous vireo",
        "Blue-headed Vireo": "blue-headed vireo",
    },
    "nabirds_species_classification_casvir_plsvir": {
        "Plumbeous Vireo": "plumbeous vireo",
        "Cassin's Vireo": "Cassin's vireo",
    },
    "nabirds_species_classification_comrav_fiscro": {
        "Fish Crow": "fish crow",
        "Common Raven": "common raven",
    },
    "nabirds_species_classification_rensap_yebsap": {
        "Yellow-bellied Sapsucker": "yellow-bellied sapsucker",
        "Red-naped Sapsucker": "red-naped sapsucker",
    },
    "nabirds_species_classification_sursco_whwsco2": {
        "Surf Scoter": "surf scoter",
        "White-winged Scoter": "white-winged scoter",
    },
    "nabirds_species_classification_commer_rebmer": {
        "Common Merganser": "common merganser",
        "Red-breasted Merganser": "red-breasted merganser",
    },
    "nabirds_species_classification_barswa_cliswa": {
        "Barn Swallow": "barn swallow",
        "Cliff Swallow": "cliff swallow",
    },
    "nabirds_species_classification_amecro_fiscro": {
        "American Crow": "American crow",
        "Fish Crow": "fish crow",
    },
    "nabirds_species_classification_louwat_norwat": {
        "Northern Waterthrush": "northern waterthrush",
        "Louisiana Waterthrush": "Louisiana waterthrush",
    },
    # iNat non-species
    "inat_non_species_dead_jackal": {
        "dead_coyote": "dead coyote",
        "dead_golden_jackal": "dead golden jackal",
    },
    "inat_non_species_white_american_robin": {
        "regular_robin": "regular robin",
        "white_robin": "white robin",
    },
    "inat_non_species_tagged_swan": {
        "not_tagged_swan": "regular swan",
        "tagged_swan": "tagged swan",
    },
    "inat_non_species_intersex_mallards": {
        "not_intersex": "regular mallard",
        "intersex": "intersex mallard",
    },
    "inat_non_species_birds_near_signs": {
        "bird_not_on_sign": "bird in the wild",
        "bird_on_sign": "bird on a man-made sign",
    },
    "inat_non_species_diseased_zebra_finch": {
        "regular_zebra_finch": "regular zebra finch",
        "diseased_zebra_finch": "diseased zebra finch",
    },
    "inat_non_species_mating_chauliognathus_pensylvanicus": {
        "mating": "mating Chauliognathus pensylvanicus",
        "not_mating": "non-mating Chauliognathus pensylvanicus",
    },
    "inat_non_species_mating_common_green_darner": {
        "mating": "mating common green darner",
        "not_mating": "non-mating common green darner",
    },
    "inat_non_species_black_eastern_gray_squirrel": {
        "black_squirrel": "black squirrel",
        "regular_squirrel": "regular squirrel",
    },
    "inat_non_species_dead_striped_skunk": {
        "dead_striped_skunk": "dead striped skunk",
        "dead_hog_nosed_skunk": "dead hog-nosed skunk",
    },
    "inat_non_species_dead_common_garter_snake": {
        "common_garter_snake": "dead common garter snake",
        "gopher_snake": "dead gopher snake",
    },
    "inat_non_species_diseased_leaves": {
        "mulberry_leaf_leaf": "diseased mulberry leaf",
        "red_dock_leaf": "red dock leaf",
    },
    "inat_non_species_mating_bagrada_hilaris": {
        "not_mating": "non-mating Bagrada hilaris",
        "mating": "mating Bagrada hilaris",
    },
    "inat_non_species_mating_hippodamia_convergens": {
        "not_mating": "non-mating Hippodamia convergens",
        "mating": "mating Hippodamia convergens",
    },
    "inat_non_species_mating_harmonia_axyridis": {
        "not_mating": "non-mating Harmonia axyridis",
        "mating": "mating Harmonia axyridis",
    },
    "inat_non_species_white_white_tailed_deer": {
        "regular_deer": "regular deer",
        "white_deer": "white-tailed deer",
    },
    "inat_non_species_mating_oncopeltus_fasciatus": {
        "mating": "mating Oncopeltus fasciatus",
        "not_mating": "non-mating Oncopeltus fasciatus",
    },
    "inat_non_species_mating_aligator_lizard": {
        "mating": "mating alligator lizard",
        "not_mating": "non-mating alligator lizard",
    },
    "inat_non_species_mating_toxomerus_marginatus": {
        "mating": "mating Toxomerus marginatus",
        "not_mating": "non-mating Toxomerus marginatus",
    },
    "inat_non_species_mating_danaus_plexippus": {
        "mating": "mating Danaus plexippus",
        "not_mating": "non-mating Danaus plexippus",
    },
    "inat_non_species_feather_california_scrub_jay_v_quail": {
        "quail_feather": "quail feather",
        "scrub_jay_feather": "scrub jay feather",
    },
    "inat_non_species_mating_argia_vivida": {
        "mating": "mating Argia vivida",
        "not_mating": "non-mating Argia vivida",
    },
    "inat_non_species_mammal_species": {
        "bobcat_feces": "bobcat feces",
        "black_bear_feces": "black bear feces",
    },
    "inat_non_species_deformed_beak": {
        "deformed_beak": "bird with a deformed beak",
        "regular_beak": "bird with a regular beak",
    },
    "inat_non_species_mating_terrapene_carolina": {
        "mating": "mating Terrapene carolina",
        "not_mating": "non-mating Terrapene carolina",
    },
    # iNat Observed
    "inat_observed_Yellow-backed_Spiny_Lizard_vs_Desert_Spiny_Lizard": {
        "Desert Spiny Lizard": "desert spiny lizard",
        "Yellow-backed Spiny Lizard": "yellow-backed spiny lizard",
    },
    "inat_observed_Orange_Jelly_Spot_vs_witch's_butter": {
        "witch's butter": "witch's butter",
        "Orange Jelly Spot": "orange jelly spot",
    },
    "inat_observed_Eastern_Meadowlark_vs_Western_Meadowlark": {
        "Eastern Meadowlark": "eastern meadowlark",
        "Western Meadowlark": "western meadowlark",
    },
    "inat_observed_Groove-billed_Ani_vs_Smooth-billed_Ani": {
        "Smooth-billed Ani": "smooth-billed ani",
        "Groove-billed Ani": "groove-billed ani",
    },
    "inat_observed_Pacific_Banana_Slug_vs_Button's_Banana_Slug": {
        "Button's Banana Slug": "Button's banana slug",
        "Pacific Banana Slug": "Pacific banana slug",
    },
    "inat_observed_Red_Belted_Conk_vs_Northern_Red_Belt": {
        "Northern Red Belt": "northern red belt",
        "Red Belted Conk": "red belted conk",
    },
    "inat_observed_Brown-lipped_Snail_vs_White-lipped_Snail": {
        "Brown-lipped Snail": "brown-lipped snail",
        "White-lipped Snail": "white-lipped snail",
    },
    "inat_observed_Cross_Orbweaver_vs_Hentz's_Orbweaver": {
        "Hentz's Orbweaver": "Hentz's orbweaver",
        "Cross Orbweaver": "cross orbweaver",
    },
    "inat_observed_Common_Grass_Yellow_vs_Three-spotted_Grass_Yellow": {
        "Three-spotted Grass Yellow": "three-spotted grass yellow",
        "Common Grass Yellow": "common grass yellow",
    },
    "inat_observed_southern_cattail_vs_lesser_reedmace": {
        "lesser reedmace": "lesser reedmace",
        "southern cattail": "southern cattail",
    },
    "inat_observed_Blue_Mussel_vs_California_Mussel": {
        "Blue Mussel": "blue mussel",
        "California Mussel": "California mussel",
    },
    "inat_observed_Northern_Two-lined_Salamander_vs_Southern_Two-lined_Salamander": {
        "Southern Two-lined Salamander": "southern two-lined salamander",
        "Northern Two-lined Salamander": "northern two-lined salamander",
    },
    "inat_observed_Belize_Crocodile_vs_American_Crocodile": {
        "American Crocodile": "American crocodile",
        "Belize Crocodile": "Belize crocodile",
    },
    "inat_observed_Jelly_Ear_vs_Ear_fungus": {
        "Jelly Ear": "jelly ear",
        "Ear fungus": "ear fungus",
    },
    "inat_observed_Desert_Blonde_Tarantula_vs_Desert_Tarantula": {
        "Desert Blonde Tarantula": "desert blonde tarantula",
        "Desert Tarantula": "desert tarantula",
    },
    "inat_observed_Northern_Cinnabar_Polypore_vs_Cinnabar_Bracket": {
        "Cinnabar Bracket": "cinnabar bracket",
        "Northern Cinnabar Polypore": "northern cinnabar polypore",
    },
    "inat_observed_Western_Mosquitofish_vs_Eastern_Mosquitofish": {
        "Western Mosquitofish": "western mosquitofish",
        "Eastern Mosquitofish": "eastern mosquitofish",
    },
    "inat_observed_Western_Grey_Kangaroo_vs_Eastern_Grey_Kangaroo": {
        "Western Grey Kangaroo": "western grey kangaroo",
        "Eastern Grey Kangaroo": "eastern grey kangaroo",
    },
    "inat_observed_Eastern_Cane_Toad_vs_Giant_Marine_Toad": {
        "Giant Marine Toad": "giant marine toad",
        "Eastern Cane Toad": "eastern cane Toad",
    },
    "inat_observed_Eastern_Oyster_vs_Pacific_Oyster": {
        "Pacific Oyster": "Pacific oyster",
        "Eastern Oyster": "Eastern oyster",
    },
    "inat_observed_Snakeskin_Chiton_vs_California_Spiny_Chiton": {
        "California Spiny Chiton": "California spiny chiton",
        "Snakeskin Chiton": "snakeskin chiton",
    },
    "inat_observed_Flea_Jumper_vs_Asiatic_Wall_Jumping_Spider": {
        "Asiatic Wall Jumping Spider": "Asiatic wall jumping spider",
        "Flea Jumper": "flea jumper",
    },
    "inat_observed_California_Sea_Lion_vs_Steller_Sea_Lion": {
        "Steller Sea Lion": "Steller sea lion",
        "California Sea Lion": "California sea lion",
    },
    "inat_observed_Southern_Cinnabar_Polypore_vs_Cinnabar_Bracket": {
        "Southern Cinnabar Polypore": "southern cinnabar polypore",
        "Cinnabar Bracket": "cinnabar bracket",
    },
    "inat_observed_Southern_Black_Widow_vs_Western_Black_Widow": {
        "Southern Black Widow": "Southern black widow",
        "Western Black Widow": "Western black widow",
    },
    "inat_observed_Eastern_Ribbonsnake_vs_Western_Ribbon_Snake": {
        "Eastern Ribbonsnake": "eastern ribbonsnake",
        "Western Ribbon Snake": "western ribbonsnake",
    },
    "inat_observed_Brown_House_Spider_vs_False_Black_Widow": {
        "False Black Widow": "false black widow",
        "Brown House Spider": "brown house spider",
    },
    "inat_observed_Allegheny_Mountain_Dusky_Salamander_vs_Dusky_Salamander": {
        "Dusky Salamander": "dusky salamander",
        "Allegheny Mountain Dusky Salamander": "Allegheny Mountain dusky salamander",
    },
    "inat_observed_Rough_Green_Snake_vs_Smooth_Greensnake": {
        "Rough Green Snake": "rough green snake",
        "Smooth Greensnake": "smooth greensnake",
    },
    "inat_observed_Common_Shiny_Woodlouse_vs_Rathke’s_Woodlouse": {
        "Rathke’s Woodlouse": "Rathke’s woodlouse",
        "Common Shiny Woodlouse": "common shiny woodlouse",
    },
    # iNat Unobserved
    "inat_unobserved_armillaria_luteobubalina_v_armillaria_novae-zelandiae": {
        "Armillaria novae-zelandiae": "Armillaria novae-zelandiae",
        "Armillaria luteobubalina": "Armillaria luteobubalina",
    },
    "inat_unobserved_phaeophyscia_orbicularis_v_phaeophyscia_rubropulchra": {
        "Phaeophyscia orbicularis": "Phaeophyscia orbicularis",
        "Phaeophyscia rubropulchra": "Phaeophyscia rubropulchra",
    },
    "inat_unobserved_corvus_orru_v_corvus_sinaloae": {
        "Corvus sinaloae": "Corvus sinaloae",
        "Corvus orru": "Corvus orru",
    },
    "inat_unobserved_lampsilis_cardium_v_lampsilis_siliquoidea": {
        "Lampsilis siliquoidea": "Lampsilis siliquoidea",
        "Lampsilis cardium": "Lampsilis cardium",
    },
    "inat_unobserved_diaea_dorsata_v_diaea_ambara": {
        "Diaea ambara": "Diaea ambara",
        "Diaea dorsata": "Diaea dorsata",
    },
    "inat_unobserved_polystichum_aculeatum_v_polystichum_setiferum": {
        "Polystichum setiferum": "Polystichum setiferum",
        "Polystichum aculeatum": "Polystichum aculeatum",
    },
    "inat_unobserved_pinus_clausa_v_pinus_mugo": {
        "Pinus mugo": "Pinus mugo",
        "Pinus clausa": "Pinus clausa",
    },
    "inat_unobserved_judolia_cordifera_v_judolia_cerambyciformis": {
        "Judolia cerambyciformis": "Judolia cerambyciformis",
        "Judolia cordifera": "Judolia cordifera",
    },
    "inat_unobserved_podarcis_virescens_v_podarcis_guadarramae": {
        "Podarcis virescens": "Podarcis virescens",
        "Podarcis guadarramae": "Podarcis guadarramae",
    },
    "inat_unobserved_thysanotus_tuberosus_v_thysanotus_patersonii": {
        "Thysanotus patersonii": "Thysanotus patersonii",
        "Thysanotus tuberosus": "Thysanotus tuberosus",
    },
    "inat_unobserved_amanita_flavorubens_v_amanita_xanthocephala": {
        "Amanita flavorubens": "Amanita flavorubens",
        "Amanita xanthocephala": "Amanita xanthocephala",
    },
    "inat_unobserved_otiorhynchus_ovatus_v_otiorhynchus_singularis": {
        "Otiorhynchus ovatus": "Otiorhynchus ovatus",
        "Otiorhynchus singularis": "Otiorhynchus singularis",
    },
    "inat_unobserved_tillandsia_balbisiana_v_tillandsia_bartramii": {
        "Tillandsia bartramii": "Tillandsia bartramii",
        "Tillandsia balbisiana": "Tillandsia balbisiana",
    },
    "inat_unobserved_oudemansiella_mucida_v_oudemansiella_furfuracea": {
        "Oudemansiella furfuracea": "Oudemansiella furfuracea",
        "Oudemansiella mucida": "Oudemansiella mucida",
    },
    "inat_unobserved_apodemus_sylvaticus_v_apodemus_agrarius": {
        "Apodemus agrarius": "Apodemus agrarius",
        "Apodemus sylvaticus": "Apodemus sylvaticus",
    },
    "inat_unobserved_lanius_bucephalus_v_lanius_meridionalis": {
        "Lanius meridionalis": "Lanius meridionalis",
        "Lanius bucephalus": "Lanius bucephalus",
    },
    "inat_unobserved_chloris_verticillata_v_chloris_cucullata": {
        "Chloris cucullata": "Chloris cucullata",
        "Chloris verticillata": "Chloris verticillata",
    },
    "inat_unobserved_turdus_torquatus_v_turdus_atrogularis": {
        "Turdus torquatus": "Turdus torquatus",
        "Turdus atrogularis": "Turdus atrogularis",
    },
    "inat_unobserved_panus_conchatus_v_panus_neostrigosus": {
        "Panus conchatus": "Panus conchatus",
        "Panus neostrigosus": "Panus neostrigosus",
    },
    "inat_unobserved_leucorrhinia_dubia_v_leucorrhinia_rubicunda": {
        "Leucorrhinia dubia": "Leucorrhinia dubia",
        "Leucorrhinia rubicunda": "Leucorrhinia rubicunda",
    },
    "inat_unobserved_cortinarius_austrovenetus_v_cortinarius_archeri": {
        "Cortinarius austrovenetus": "Cortinarius austrovenetus",
        "Cortinarius archeri": "Cortinarius archeri",
    },
    "inat_unobserved_emberiza_pusilla_v_emberiza_leucocephalos": {
        "Emberiza pusilla": "Emberiza pusilla",
        "Emberiza leucocephalos": "Emberiza leucocephalos",
    },
    "inat_unobserved_podarcis_liolepis_v_podarcis_bocagei": {
        "Podarcis bocagei": "Podarcis bocagei",
        "Podarcis liolepis": "Podarcis liolepis",
    },
    "inat_unobserved_serinus_canaria_v_serinus_canicollis": {
        "Serinus canaria": "Serinus canaria",
        "Serinus canicollis": "Serinus canicollis",
    },
    "inat_unobserved_cladonia_squamosa_v_cladonia_portentosa": {
        "Cladonia squamosa": "Cladonia squamosa",
        "Cladonia portentosa": "Cladonia portentosa",
    },
    "inat_unobserved_lactarius_torminosus_v_lactarius_turpis": {
        "Lactarius torminosus": "Lactarius torminosus",
        "Lactarius turpis": "Lactarius turpis",
    },
    "inat_unobserved_scopula_umbilicata_v_scopula_ornata": {
        "Scopula umbilicata": "Scopula umbilicata",
        "Scopula ornata": "Scopula ornata",
    },
    "inat_unobserved_aceria_negundi_v_aceria_cephalonea": {
        "Aceria negundi": "Aceria negundi",
        "Aceria cephalonea": "Aceria cephalonea",
    },
    "inat_unobserved_hippolais_icterina_v_hippolais_polyglotta": {
        "Hippolais polyglotta": "Hippolais polyglotta",
        "Hippolais icterina": "Hippolais icterina",
    },
    "inat_unobserved_cuphea_aequipetala_v_cuphea_hyssopifolia": {
        "Cuphea aequipetala": "Cuphea aequipetala",
        "Cuphea hyssopifolia": "Cuphea hyssopifolia",
    },
}

```

# biobench/imagenet/__init__.py

```python
""" """

import dataclasses
import logging
import math

import beartype
import numpy as np
import sklearn.experimental.enable_halving_search_cv
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped

import datasets
from biobench import helpers, interfaces, registry

logger = logging.getLogger("imagenet")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    data: str = ""
    """dataset directory; where you downloaded this task's data to."""
    batch_size_cv: int = 256
    """batch size for computer vision model."""
    n_workers: int = 8
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    backbone = registry.load_vision_backbone(*model_args)
    test_features = get_features(args, backbone, is_train=False)
    train_features = get_features(args, backbone, is_train=True)

    clf = init_clf(args)
    clf.fit(train_features.x, train_features.y)

    helpers.write_hparam_sweep_plot("imagenet", model_args.ckpt, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    examples = [
        interfaces.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(
            helpers.progress(test_features.ids, desc="building exs", every=1_000),
            pred_labels,
            true_labels,
        )
    ]

    return model_args, interfaces.TaskReport("ImageNet-1K", examples)


class Transform:
    def __init__(self, img_transform):
        self._img_transform = img_transform

    def __call__(self, example, index: int):
        example["image"] = example["image"].convert("RGB")
        example["image"] = self._img_transform(example["image"])
        example["id"] = str(index)
        return example


@beartype.beartype
@torch.no_grad
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))
    split = "train" if is_train else "validation"

    dataset = datasets.load_dataset("ILSVRC/imagenet-1k", split=split)
    n_examples = dataset.num_rows

    # Map
    dataset = (
        dataset.to_iterable_dataset(num_shards=args.n_workers)
        .map(Transform(img_transform), with_indices=True)
        .shuffle(seed=args.seed)
        .with_format("torch")
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,
    )

    all_features, all_labels, all_ids = [], [], []

    total = math.ceil(n_examples / args.batch_size) if not args.debug else 20
    it = iter(dataloader)
    logger.debug("Need to embed %d batches of %d images.", total, args.batch_size)
    for b in helpers.progress(
        range(total), every=args.log_every, desc=f"Embedding {split}"
    ):
        batch = next(it)

        images = batch["image"].to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())
        all_labels.extend(batch["label"])
        all_ids.extend(batch["id"])

    all_features = torch.cat(all_features, dim=0).cpu().numpy()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels).numpy()
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(args: Args):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if args.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        factor=3,
    )

```

# biobench/fishnet/download.py

```python
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "gdown",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the FishNet dataset

Run with:

1. `python biobench/fishnet/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.fishnet.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import zipfile

import gdown
import requests
import tqdm
import tyro

dataset_url = "https://drive.google.com/uc?id=1mqLoap9QIVGYaPJ7T_KSBfLxJOg2yFY3"

labels_urls = [
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/train_full_meta_new.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/train.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/test.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/spec_gen_map.csv",
]


@dataclasses.dataclass()
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    images: bool = True
    """Whether to download the images zip file [5.4GB]."""
    labels: bool = True
    """Whether to download the labels."""
    extract: bool = True
    """Whether to extract the zip file."""


def main(args: Args):
    """Download FishNet."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    output_name = "fishnet.zip"
    zipfile_path = os.path.join(args.dir, output_name)

    # Download the zip file.
    if args.images:
        gdown.download(dataset_url, zipfile_path, quiet=False)
        print(f"Downloaded zip file: {zipfile_path}.")

    if args.labels:
        for labels_url in labels_urls:
            r = requests.get(labels_url, stream=True)
            r.raise_for_status()

            labels_path = os.path.join(args.dir, labels_url.split("/")[-1])
            with open(labels_path, "wb") as fd:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    fd.write(chunk)
            print(f"Downloaded labels: {labels_path}.")

    # Extract the zip file.
    if args.extract:
        with zipfile.ZipFile(zipfile_path, "r") as zip:
            for member in tqdm.tqdm(zip.infolist(), desc="Extracting images"):
                zip.extract(member, args.dir)
        print(f"Extracted images: {args.dir}/Image_Library.")


if __name__ == "__main__":
    main(tyro.cli(Args))

```

# biobench/fishnet/__init__.py

```python
"""
# FishNet: Fish Recognition, Detection, and Functional Traits Prediction

FishNet ([paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_FishNet_A_Large-scale_Dataset_and_Benchmark_for_Fish_Recognition_Detection_ICCV_2023_paper.pdf), [code](https://github.com/faixan-khan/FishNet)) is a large-scale diverse dataset containing 94,532 images from 17,357 aquatic species.
It contains three benchmarks: fish classification, fish detection, and functional traits prediction.

We mainly focus on the third task.
We train an two-layer MLP on the visual features extracted by different model backbones to predict the presence or absence of 9 different traits.

If you use this evaluation, be sure to cite the original work:

```
@InProceedings{Khan_2023_ICCV,
    author    = {Khan, Faizan Farooq and Li, Xiang and Temple, Andrew J. and Elhoseiny, Mohamed},
    title     = {FishNet: A Large-scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {20496-20506}
}
```

This task was contributed by [Jianyang Gu](https://vimar-gu.github.io/).
"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import polars as pl
import sklearn
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from biobench import helpers, interfaces, registry

logger = logging.getLogger("fishnet")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """FishNet task arguments."""

    data: str = ""
    """dataset directory; where you downloaded this task's data to."""
    batch_size_cv: int = 256
    """batch size for computer vision model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of epochs) to log progress."""
    n_epochs: int = 100
    """How many epochs to train the MLP classifier."""
    learning_rate: float = 5e-4
    """The learning rate for training the MLP classifier."""
    threshold: float = 0.5
    """The threshold to predicted "presence" rather than "absence"."""
    seed: int = 42
    """random seed."""
    parallel: int = 5
    """Concurrent requests per second."""

    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@jaxtyped(typechecker=beartype.beartype)
class Features(torch.utils.data.Dataset):
    """
    A dataset of learned features (dense vectors).
    """

    x: Float[Tensor, " n dim"]
    """Dense feature vectors from a vision backbone."""
    y: Int[Tensor, " n 9"]
    """0/1 labels of absence/presence of 9 different traits."""
    ids: Shaped[np.ndarray, " n"]
    """Image ids."""

    def __init__(
        self,
        x: Float[Tensor, " n dim"],
        y: Int[Tensor, " n n_classes"],
        ids: Shaped[np.ndarray, " n"],
    ):
        self.x = x
        self.y = y
        self.ids = ids

    @property
    def dim(self) -> int:
        """Dimension of the dense feature vectors."""
        _, dim = self.x.shape
        return dim

    def __len__(self) -> int:
        return len(self.x)

    def __getitem__(
        self, index
    ) -> tuple[Float[Tensor, " dim"], Int[Tensor, " n_classes"], str]:
        return self.x[index], self.y[index], self.ids[index]


@beartype.beartype
def init_classifier(input_dim: int) -> torch.nn.Module:
    """A simple MLP classifier consistent with the design in FishNet."""
    return torch.nn.Sequential(
        torch.nn.Linear(input_dim, 512),
        torch.nn.Dropout(0.5),
        torch.nn.Linear(512, 9),
    )


@beartype.beartype
def calc_macro_f1(examples: list[interfaces.Prediction]) -> float:
    """TODO: docs."""
    y_pred = np.array([example.info["y_pred"] for example in examples])
    y_true = np.array([example.info["y_true"] for example in examples])
    score = sklearn.metrics.f1_score(
        y_true, y_pred, average="macro", labels=np.unique(y_true)
    )
    return score.item()


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """
    The FishNet benchmark.
    """
    # 1. Load model.
    backbone = registry.load_vision_backbone(*model_args)

    # 2. Get features.
    train_dataset = get_features(args, backbone, is_train=True)
    test_dataset = get_features(args, backbone, is_train=False)

    # 3. Set up classifier.
    classifier = init_classifier(train_dataset.dim).to(args.device)

    # 4. Load datasets for classifier.
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=args.batch_size, shuffle=False
    )
    optimizer = torch.optim.Adam(classifier.parameters(), lr=args.learning_rate)
    criterion = torch.nn.BCEWithLogitsLoss()
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

    # 5. Fit the classifier.
    for epoch in range(args.n_epochs):
        total = 2 if args.debug else len(train_loader)
        it = iter(train_loader)
        for b in range(total):
            features, labels, _ = next(it)
            features = features.to(args.device)
            labels = labels.to(args.device, dtype=torch.float)
            output = classifier(features)
            loss = criterion(output, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        scheduler.step()

        # Evaluate the classifier.
        if (epoch + 1) % args.log_every == 0:
            examples = evaluate(args, classifier, test_loader)
            score = calc_macro_f1(examples)
            logger.info("Epoch %d/%d: %.3f", epoch + 1, args.n_epochs, score)

    return model_args, interfaces.TaskReport(
        "FishNet", examples, calc_mean_score=calc_macro_f1
    )


@beartype.beartype
def evaluate(
    args: Args, classifier: torch.nn.Module, dataloader
) -> list[interfaces.Prediction]:
    """
    Evaluates the trained classifier on a test split.

    Returns:
        a list of Examples.
    """
    total = 2 if args.debug else len(dataloader)
    it = iter(dataloader)
    examples = []
    for b in range(total):
        features, labels, ids = next(it)
        features = features.to(args.device)
        labels = labels.numpy()
        ids = ids.numpy()
        with torch.no_grad():
            pred_logits = classifier(features)
        pred_logits = (pred_logits > args.threshold).cpu().numpy()
        for id, pred, true in zip(ids, pred_logits, labels):
            example = interfaces.Prediction(
                str(id),
                float((pred == true).all()),
                {"y_pred": pred.tolist(), "y_true": true.tolist()},
            )
            examples.append(example)

    return examples


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, *, is_train: bool
) -> Features:
    """Extract visual features."""
    if not os.path.isdir(args.datadir):
        msg = f"Path '{args.datadir}' doesn't exist. Did you download the FishNet dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--fishnet-args.datadir'; see --help for more."
        raise ValueError(msg)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    file = "train.csv" if is_train else "test.csv"
    dataset = ImageDataset(args.datadir, file, transform=img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=args.batch_size, num_workers=args.n_workers
    )

    all_features, all_labels, all_ids = [], [], []

    total = 2 if args.debug else len(dataloader)
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=args.log_every, desc=file):
        images, labels, _ = next(it)
        images = images.to(args.device)

        features = backbone.img_encode(images).img_features
        all_features.append(features.cpu())
        all_labels.append(labels)

        ids = np.arange(len(labels)) + b * args.batch_size
        all_ids.append(ids)

    # Keep the Tensor data type for subsequent training
    all_features = torch.cat(all_features, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    all_ids = np.concatenate(all_ids, axis=0)

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
class ImageDataset(torch.utils.data.Dataset):
    """
    A dataset that loads the required attribute labels.
    """

    def __init__(self, root_dir: str, csv_file: str, transform):
        self.root_dir = root_dir
        self.csv_file = os.path.join(self.root_dir, csv_file)
        self.df = pl.read_csv(self.csv_file).with_row_index()
        self.all_columns = [
            "FeedingPath",
            "Tropical",
            "Temperate",
            "Subtropical",
            "Boreal",
            "Polar",
            "freshwater",
            "saltwater",
            "brackish",
        ]
        for col in self.all_columns:
            self.df = self.df.filter(self.df[col].is_not_null())
        self.transform = transform

        # Corresponding column indices
        self.image_col = 4
        self.folder_col = 13
        self.label_cols = [15, 16, 17, 18, 19, 20, 21, 22, 23]
        logger.info("csv file: %s has %d item.", csv_file, len(self.df))

    def __getitem__(
        self, index: int
    ) -> tuple[Float[Tensor, "3 width height"], Float[Tensor, "9"], str]:
        row_data = self.df.row(index)
        image_name = row_data[self.image_col]
        image_name = image_name.split("/")[-1]
        folder = row_data[self.folder_col]
        image_path = os.path.join(self.root_dir, "Image_Library", folder, image_name)
        image = Image.open(image_path)

        # Extract the required attribute labels.
        label = []
        for col in self.label_cols:
            value = row_data[col]
            if col == 15:
                if value == "pelagic":
                    value = 1
                elif value == "benthic":
                    value = 0
                else:
                    raise ValueError("FeedingPath can only be pelagic or benthic.")
            label.append(value)
        label = torch.tensor(label)

        if self.transform:
            image = self.transform(image)

        return image, label, image_path

    def __len__(self) -> int:
        return len(self.df)

```

# biobench/rarespecies/__init__.py

```python
import collections
import dataclasses
import logging
import math

import beartype
import numpy as np
import sklearn.neighbors
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from torch import Tensor

import datasets
from biobench import helpers, interfaces, registry

logger = logging.getLogger("rare-species")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@beartype.beartype
def benchmark(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    backbone = registry.load_vision_backbone(*model_args)
    features = get_features(args, backbone)

    train_i, test_i = make_split(features.y, k=1)

    scores = simpleshot(
        args,
        features.x[train_i],
        features.y[train_i],
        features.x[test_i],
        features.y[test_i],
    )
    examples = [
        interfaces.Prediction(str(id), float(score), {})
        for id, score in zip(features.ids[test_i], scores.tolist())
    ]
    return model_args, interfaces.TaskReport("RareSpecies", examples)


class LabelProcessor:
    def __init__(self):
        self._lookup = {}

    def transform(self, label) -> int:
        if label not in self._lookup:
            self._lookup[label] = len(self._lookup)
        return self._lookup[label]


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[Tensor, " n dim"]
    y: Int[Tensor, " n"]
    ids: Shaped[np.ndarray, " n"]


class Preprocess:
    def __init__(self, img_transform):
        self._img_transform = img_transform

    def __call__(self, example):
        example["image"] = example["image"].convert("RGB")
        example["image"] = self._img_transform(example["image"])
        example["label"] = "-".join(
            example[key]
            for key in [
                "kingdom",
                "phylum",
                "class",
                "order",
                "family",
                "genus",
                "species",
            ]
        )
        return example


@beartype.beartype
@torch.no_grad
def get_features(args: Args, backbone: interfaces.VisionBackbone) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = (
        datasets.load_dataset("imageomics/rare-species", split="train")
        .to_iterable_dataset(num_shards=args.n_workers)
        .map(Preprocess(img_transform))
        .with_format("torch")
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,  # We use dataset.shuffle instead
    )

    label_processor = LabelProcessor()

    all_features, all_labels, all_ids = [], [], []

    total = math.ceil(11984 / args.batch_size) if not args.debug else 2
    it = iter(dataloader)
    logger.debug("Need to embed %d batches of %d images.", total, args.batch_size)
    for b in helpers.progress(range(total), every=args.log_every, desc="embed"):
        batch = next(it)

        images = batch["image"].to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())

        labels = [label_processor.transform(label) for label in batch["label"]]
        all_labels.extend(labels)

        all_ids.extend(batch["rarespecies_id"])

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
def simpleshot(
    args: Args,
    x_train: Float[Tensor, "n_train dim"],
    y_train: Int[Tensor, " n_train"],
    x_test: Float[Tensor, "n_test dim"],
    y_test: Int[Tensor, " n_test"],
) -> Float[Tensor, " n_test"]:
    """
    Applies simpleshot to the video clips. We assign each clip the majority label. Return the list of scores for x_test.
    """
    x_mean = x_train.mean(axis=0, keepdims=True)

    x_train = x_train - x_mean
    x_train = l2_normalize(x_train)

    x_test = x_test - x_mean
    x_test = l2_normalize(x_test)

    clf = sklearn.neighbors.NearestCentroid()
    clf.fit(x_train, y_train)

    # Do this next step on the GPU to make it fast.
    # Goes from 1 batch/sec to 77 batch/sec
    centroids = torch.from_numpy(clf.centroids_).to(args.device)
    x_test = x_test.to(args.device)
    y_test = y_test.to(args.device)

    scores = []
    for start, stop in batched_idx(len(x_test), args.batch_size):
        x_batch = x_test[start:stop]
        y_batch = y_test[start:stop]
        distances = torch.linalg.vector_norm(x_batch[:, None] - centroids, axis=2)
        preds = torch.argmin(distances, dim=1)

        scores.append((preds == y_batch).type(torch.float32))

    return torch.cat(scores, axis=0)


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[Tensor, "n_examples dim"],
) -> Float[Tensor, "n_examples dim"]:
    norms = torch.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@jaxtyped(typechecker=beartype.beartype)
def make_split(
    labels: Int[Tensor, " n_examples"], *, k: int
) -> tuple[Int[Tensor, " n_train"], Int[Tensor, " n_test"]]:
    classes = np.unique(labels)

    train_indices = np.array([], dtype=int)
    test_indices = np.array([], dtype=int)

    # Iterate through each class to select indices
    for cls in classes:
        # Indices corresponding to the current class
        cls_indices = np.where(labels == cls)[0]
        # Randomly shuffle the indices
        np.random.shuffle(cls_indices)
        # Select the first K indices for the train set
        cls_train_indices = cls_indices[:k]
        # The rest go into the test set
        cls_test_indices = cls_indices[k:]
        # Append the selected indices to the train/test arrays
        train_indices = np.concatenate((train_indices, cls_train_indices))
        test_indices = np.concatenate((test_indices, cls_test_indices))

    # Shuffle the indices to mix classes
    np.random.shuffle(train_indices)
    np.random.shuffle(test_indices)

    return torch.from_numpy(train_indices), torch.from_numpy(test_indices)

```

# biobench/kabr/__init__.py

```python
"""
# Kenyan Animal Behavior Recognition (KABR)

KABR is a video recognition task ([paper](https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/papers/Kholiavchenko_KABR_In-Situ_Dataset_for_Kenyan_Animal_Behavior_Recognition_From_Drone_WACVW_2024_paper.pdf), [website](https://kabrdata.xyz/), [Huggingface](https://huggingface.co/datasets/imageomics/KABR)) where the model predicts Kenyan animal behavior in short video segments.

This can be framed as a classification task: given a short video segment of a single animal, which behavior is most common within the segment?

While specialized architectures exist, we train a simple nearest-centroid classifier [which works well with few-shot tasks](https://arxiv.org/abs/1911.04623) over video representations.
We get video representations by embedding each frame of the video and taking the mean over the batch dimension.

## Data

To download the data, you need to use the dataset download script:

1. Copy-paste the [download script](https://huggingface.co/datasets/imageomics/KABR/raw/main/download.py) to your data directory, like `/scratch/KABR/download.py`.
2. Run `python download.py`. It doesn't have any requirements beyond the Python standard library.
"""

import csv
import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import torch
from jaxtyping import Float, Int, jaxtyped
from PIL import Image
from torch import Tensor

from biobench import interfaces, registry, simpleshot

logger = logging.getLogger("kabr")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Arguments for the KABR task."""

    data: str = ""
    """dataset directory; where you downloaded this task's data to."""
    batch_size_cv: int = 256
    """batch size for computer vision model."""
    n_workers: int = 4
    """Number of dataloader worker processes."""
    frame_agg: typing.Literal["mean", "max"] = "mean"
    """How to aggregate features across time dimension."""
    seed: int = 42
    """random seed."""
    parallel: int = 5
    """Concurrent requests per second."""

    # Computed at runtime.
    max_examples: int = -1
    """(computed at runtime) Number of maximum training samples. Negative number means use all of them."""
    device: str = "cuda"
    """(computed at runtime) Which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) Whether to run in debug mode."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Video:
    """A single video instance as a sequence of frames."""

    video_id: int
    frames: list[str]
    """Paths to actual frame images."""
    labels: list[int]
    """Frame-level labels."""

    def __post_init__(self):
        err_msg = f"Video {self.video_id} has a different number of frames ({len(self.frames)} and labels ({len(self.labels)})."
        assert len(self.frames) == len(self.labels), err_msg


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """
    Clips of at most 90 frames in Charades format with each frame stored as an image.
    """

    def __init__(self, path, split: str, transform=None, seed: int = 42):
        self.path = path
        self.split = split
        self.transform = transform
        self.seed = seed

        self.rng = np.random.default_rng(seed=seed)

        self.n_frames = 16
        self.n_every = 5

        # Load videos
        #############

        frames: dict[int, list[str]] = {}
        labels: dict[int, list[int]] = {}

        if not os.path.exists(self.path) or not os.path.isdir(self.path):
            msg = f"Path '{self.path}' doesn't exist. Did you download the KABR dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path as --dataset-dir PATH"
            raise RuntimeError(msg)

        with open(os.path.join(self.path, "annotation", f"{split}.csv")) as fd:
            reader = csv.reader(fd, delimiter=" ")
            next(reader)  # skip headers
            for _, video_id, frame_id, path, label in reader:
                video_id = int(video_id)
                frame_id = int(frame_id)
                label = int(label)

                if video_id not in frames:
                    frames[video_id] = []
                if video_id not in labels:
                    labels[video_id] = []

                if frame_id > len(frames[video_id]) + 1:
                    raise ValueError(f"Video {video_id} is missing a frame.")

                path = os.path.join(self.path, "dataset", "image", path)
                frames[video_id].append(path)
                labels[video_id].append(label)

        self.videos = [
            Video(video_id, frames[video_id], labels[video_id])
            for video_id in frames.keys()
            if len(frames[video_id]) >= self.n_frames
        ]

    def __getitem__(
        self, i: int
    ) -> tuple[list[Float[Tensor, "3 width height"]], list[int]]:
        """
        Returns 16 frames and their labels sampled every 5 frames from a clip. The start of the clip is uniformly sampled. If there are fewer
        """
        n_every = self.n_every

        video = self.videos[i]

        while len(video.frames) < ((self.n_frames - 1) * n_every + 1):
            n_every -= 1

        if n_every <= 0:
            print(n_every, len(video.frames), ((self.n_frames - 1) * n_every + 1))
        assert n_every >= 1

        # margin is the number of extra frames on either size of the 16x5 sampled frames.
        margin = len(video.frames) - ((self.n_frames - 1) * n_every + 1)

        # Pick a random start, then pick n_frames frames every n_every frames.
        # (sam) This is likely not clear and there are probably better ways to express this in Python that is more clear to other video ML devs. Please open a PR if you know a better way!
        start = self.rng.integers(0, margin + 1)
        frames = video.frames[start:None:n_every][: self.n_frames]
        labels = video.labels[start:None:n_every][: self.n_frames]

        images = [Image.open(frame) for frame in frames]

        if self.transform is not None:
            images = [self.transform(image) for image in images]

        return images, labels

    def __len__(self) -> int:
        return len(self.videos)


@torch.no_grad()
@jaxtyped(typechecker=beartype.beartype)
def get_features(
    args: Args, backbone: interfaces.VisionBackbone, dataloader
) -> tuple[
    Float[Tensor, "n_frames n_examples dim"], Int[Tensor, "n_frames n_examples"]
]:
    """
    Gets all model features and true labels for all frames and all examples in the dataloader.

    Returns it as a pair of big tensors; other tasks like `biobench.birds525` use a dedicated class for this, but here it's just a tuple.

    Args:
        args: KABR task arguments.
        backbone: Vision backbone.
        dataloader: Dataloader for whatever data you want to get features for.

    Returns:
        tuple of model features and true labels. See signature for shape.
    """
    backbone = torch.compile(backbone)
    all_features, all_labels = [], []

    total = len(dataloader) if not args.debug else 2
    it = iter(dataloader)
    logger.debug("Need to embed %d batches of %d images.", total, args.batch_size * 16)
    for b in range(total):
        frames, labels = next(it)
        frames = torch.stack(frames, dim=0)
        labels = torch.stack(labels, dim=0)
        frames = frames.to(args.device)

        with torch.amp.autocast("cuda"):
            # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
            n_frames, bsz, c, h, w = frames.shape
            frames = frames.view(bsz * n_frames, c, h, w)
            outputs = backbone.img_encode(frames)
            features = outputs.img_features.view(n_frames, bsz, -1)
            all_features.append(features.cpu())
            all_labels.append(labels.cpu())

        logger.debug("Embedded batch %d/%d", b + 1, total)

    all_features = torch.cat(all_features, dim=1).cpu()
    all_labels = torch.cat(all_labels, dim=1).cpu()

    return all_features, all_labels


@jaxtyped(typechecker=beartype.beartype)
def aggregate_labels(
    args: Args, labels: Int[Tensor, "n_frames n_examples"]
) -> Int[Tensor, " n_examples"]:
    """Aggregate per-frame labels to a per-video label. Uses the most common label (mode)."""
    return torch.mode(labels, dim=0).values


@jaxtyped(typechecker=beartype.beartype)
def aggregate_frames(
    args: Args, features: Float[Tensor, "n_frames n_examples dim"]
) -> Float[Tensor, "n_examples dim"]:
    if args.frame_agg == "mean":
        return torch.mean(features, dim=0)
    elif args.frame_agg == "max":
        return torch.max(features, dim=0).values
    else:
        typing.assert_never(args.frame_agg)


@beartype.beartype
def benchmark_cvml(
    args: Args, model_args: interfaces.ModelArgsCvml
) -> tuple[interfaces.ModelArgsCvml, interfaces.TaskReport]:
    """Runs KABR benchmark."""
    # 1. Load model
    backbone = registry.load_vision_backbone(*model_args)
    img_transform = backbone.make_img_transform()
    backbone = backbone.to(args.device)

    # 2. Load data.
    train_dataset = Dataset(args.datadir, "train", transform=img_transform)
    val_dataset = Dataset(args.datadir, "val", transform=img_transform)

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
    )
    val_dataloader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
    )

    # 3. Get features
    val_features, val_labels = get_features(args, backbone, val_dataloader)
    val_features = aggregate_frames(args, val_features)
    val_labels = aggregate_labels(args, val_labels)

    train_features, train_labels = get_features(args, backbone, train_dataloader)
    train_features = aggregate_frames(args, train_features)
    train_labels = aggregate_labels(args, train_labels)

    # 4. Do simpleshot.
    scores = simpleshot.simpleshot(
        args, train_features, train_labels, val_features, val_labels
    )

    # Return benchmark report.
    video_ids = [video.video_id for video in val_dataset.videos]
    examples = [
        interfaces.Prediction(str(id), float(score), {})
        for id, score in zip(video_ids, scores.tolist())
    ]
    # TODO: include example-specific info (class? something else)
    return model_args, interfaces.TaskReport("KABR", examples)

```

# benchmark.py

```python
"""
Entrypoint for running all tasks in `biobench`.

Most of this script is self documenting.
Run `python benchmark.py --help` to see all the options.

Note that you will have to download all the datasets, but each dataset includes its own download script with instructions.
For example, see `biobench.newt.download` for an example.

.. include:: ./examples.md

.. include:: ./design.md
"""

import dataclasses
import json
import logging
import os
import resource
import sqlite3
import time
import typing

import beartype
import submitit
import tyro

from biobench import (
    ages,
    beluga,
    birds525,
    fishnet,
    imagenet,
    inat21,
    interfaces,
    iwildcam,
    kabr,
    leopard,
    newt,
    plankton,
    plantnet,
    rarespecies,
)

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("biobench")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Params to run one or more benchmarks in a parallel setting."""

    slurm: bool = False
    """whether to use submitit to run jobs on a slurm cluster."""
    slurm_acct: str = "PAS2136"
    """slurm account string."""

    models_cvml: typing.Annotated[
        list[interfaces.ModelArgsCvml], tyro.conf.arg(name="models")
    ] = dataclasses.field(
        default_factory=lambda: [
            interfaces.ModelArgsCvml("open-clip", "RN50/openai"),
            interfaces.ModelArgsCvml("open-clip", "ViT-B-16/openai"),
            interfaces.ModelArgsCvml("open-clip", "ViT-B-16/laion400m_e32"),
            interfaces.ModelArgsCvml("open-clip", "hf-hub:imageomics/bioclip"),
            interfaces.ModelArgsCvml("open-clip", "ViT-B-16-SigLIP/webli"),
            interfaces.ModelArgsCvml(
                "timm-vit", "vit_base_patch14_reg4_dinov2.lvd142m"
            ),
        ]
    )
    """CV models; a pair of model org (interface) and checkpoint."""
    models_vlm: typing.Annotated[
        list[interfaces.ModelArgsVlm], tyro.conf.arg(name="vlms")
    ] = dataclasses.field(
        default_factory=lambda: [
            # interfaces.ModelArgsVlm("openrouter/google/gemini-2.0-flash-001"),
            interfaces.ModelArgsVlm("openrouter/google/gemini-flash-1.5-8b"),
        ]
    )
    """VLM checkpoints."""
    device: typing.Literal["cpu", "cuda"] = "cuda"
    """which kind of accelerator to use."""
    debug: bool = False
    """whether to run in debug mode."""
    max_examples: int = -1
    """Number of maximum training samples. Negative number means use all of them."""
    ssl: bool = True
    """Use SSL when connecting to remote servers to download checkpoints; use --no-ssl if your machine has certificate issues. See `biobench.third_party_models.get_ssl()` for a discussion of how this works."""

    # Individual benchmarks.
    ages_run_cvml: bool = False
    """Whether to run the bird age benchmark with CV+ML."""
    ages_run_vlm: bool = False
    """Whether to run the bird age benchmark with VLM."""
    ages_args: ages.Args = dataclasses.field(default_factory=ages.Args)
    """Arguments for the bird age benchmark."""

    beluga_run_cvml: bool = False
    """Whether to run the Beluga whale re-ID benchmark with CV+ML."""
    beluga_run_vlm: bool = False
    """Whether to run the Beluga whale re-ID benchmark with VLM."""
    beluga_args: beluga.Args = dataclasses.field(default_factory=beluga.Args)
    """Arguments for the Beluga whale re-ID benchmark."""

    birds525_run_cvml: bool = False
    """Whether to run the Birds 525 benchmark with CV+ML."""
    birds525_run_vlm: bool = False
    """Whether to run the Birds 525 benchmark with VLM."""
    birds525_args: birds525.Args = dataclasses.field(default_factory=birds525.Args)
    """Arguments for the Birds 525 benchmark."""

    fishnet_run_cvml: bool = False
    """Whether to run the FishNet benchmark with CV+ML."""
    fishnet_run_vlm: bool = False
    """Whether to run the FishNet benchmark with VLM."""
    fishnet_args: fishnet.Args = dataclasses.field(default_factory=fishnet.Args)
    """Arguments for the FishNet benchmark."""

    imagenet_run_cvml: bool = False
    """Whether to run the ImageNet-1K benchmark with CV+ML."""
    imagenet_run_vlm: bool = False
    """Whether to run the ImageNet-1K benchmark with VLM."""
    imagenet_args: imagenet.Args = dataclasses.field(default_factory=imagenet.Args)
    """Arguments for the ImageNet-1K benchmark."""

    inat21_run_cvml: bool = False
    """Whether to run the iNat21 benchmark with CV+ML."""
    inat21_run_vlm: bool = False
    """Whether to run the iNat21 benchmark with VLM."""
    inat21_args: inat21.Args = dataclasses.field(default_factory=inat21.Args)
    """Arguments for the iNat21 benchmark."""

    iwildcam_run_cvml: bool = False
    """Whether to run the iWildCam benchmark with CV+ML."""
    iwildcam_run_vlm: bool = False
    """Whether to run the iWildCam benchmark with VLM."""
    iwildcam_args: iwildcam.Args = dataclasses.field(default_factory=iwildcam.Args)
    """Arguments for the iWildCam benchmark."""

    kabr_run_cvml: bool = False
    """Whether to run the KABR benchmark with CV+ML."""
    kabr_run_vlm: bool = False
    """Whether to run the KABR benchmark with VLM."""
    kabr_args: kabr.Args = dataclasses.field(default_factory=kabr.Args)
    """Arguments for the KABR benchmark."""

    leopard_run_cvml: bool = False
    """Whether to run the leopard re-ID benchmark with CV+ML."""
    leopard_run_vlm: bool = False
    """Whether to run the leopard re-ID benchmark with VLM."""
    leopard_args: leopard.Args = dataclasses.field(default_factory=leopard.Args)
    """Arguments for the leopard re-ID benchmark."""

    newt_run_cvml: bool = False
    """Whether to run the NeWT benchmark with CV+ML."""
    newt_run_vlm: bool = False
    """Whether to run the NeWT benchmark with VLM."""
    newt_args: newt.Args = dataclasses.field(default_factory=newt.Args)
    """Arguments for the NeWT benchmark."""

    plankton_run_cvml: bool = False
    """Whether to run the Plankton benchmark with CV+ML."""
    plankton_run_vlm: bool = False
    """Whether to run the Plankton benchmark with VLM."""
    plankton_args: plankton.Args = dataclasses.field(default_factory=plankton.Args)
    """Arguments for the Plankton benchmark."""

    plantnet_run_cvml: bool = False
    """Whether to run the Pl@ntNet benchmark with CV+ML."""
    plantnet_run_vlm: bool = False
    """Whether to run the Pl@ntNet benchmark with VLM."""
    plantnet_args: plantnet.Args = dataclasses.field(default_factory=plantnet.Args)
    """Arguments for the Pl@ntNet benchmark."""

    rarespecies_run_cvml: bool = False
    """Whether to run the Rare Species benchmark with CV+ML."""
    rarespecies_run_vlm: bool = False
    """Whether to run the Rare Species benchmark with VLM."""
    rarespecies_args: rarespecies.Args = dataclasses.field(
        default_factory=rarespecies.Args
    )
    """Arguments for the Rare Species benchmark."""

    # Reporting and graphing.
    report_to: str = os.path.join(".", "reports")
    """where to save reports to."""
    graph: bool = True
    """whether to make graphs."""
    graph_to: str = os.path.join(".", "graphs")
    """where to save graphs to."""
    log_to: str = os.path.join(".", "logs")
    """where to save logs to."""

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)

    def get_sqlite_connection(self) -> sqlite3.Connection:
        """Get a connection to the reports database.
        Returns:
            a connection to a sqlite3 database.
        """
        return sqlite3.connect(os.path.join(self.report_to, "reports.sqlite"))

    def update(self, other):
        return dataclasses.replace(
            other, device=self.device, debug=self.debug, max_examples=self.max_examples
        )


@beartype.beartype
def save(
    args: Args,
    model_args: interfaces.ModelArgsCvml | interfaces.ModelArgsVlm,
    report: interfaces.TaskReport,
) -> None:
    """
    Saves the report to disk in a machine-readable SQLite format.

    Args:
        args: launch script arguments.
        model_args: a pair of model_org, model_ckpt strings.
        report: the task report from the model_args.
    """
    conn = args.get_sqlite_connection()
    with open("schema.sql") as fd:
        schema = fd.read()
    conn.execute(schema)

    lower, upper = report.get_confidence_interval()
    values = (
        json.dumps(dataclasses.asdict(model_args)),
        args.max_examples,
        report.name,
        int(time.time()),
        report.get_mean_score(),
        lower,
        upper,
        json.dumps(dataclasses.asdict(args)),
        json.dumps(report.to_dict()),
    )
    conn.execute("INSERT INTO reports VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)", values)
    conn.commit()

    logger.info(
        "%s on %s: %.1f%%", model_args.ckpt, report.name, report.get_mean_score() * 100
    )
    for name, score in report.splits.items():
        logger.info("%s on %s (%s): %.3f", model_args.ckpt, report.name, name, score)


@beartype.beartype
def main(args: Args):
    """
    Launch all jobs, using either a local GPU or a Slurm cluster.
    Then report results and save to disk.
    """
    # 1. Setup executor.
    if args.slurm:
        executor = submitit.SlurmExecutor(folder=args.log_to)
        executor.update_parameters(
            time=30,
            gpus_per_node=1,
            cpus_per_task=8,
            stderr_to_stdout=True,
            partition="debug",
            account=args.slurm_acct,
        )
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not args.ssl:
            executor.update_parameters(setup=["export BIOBENCH_DISABLE_SSL=1"])
    else:
        executor = submitit.DebugExecutor(folder=args.log_to)
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not args.ssl:
            os.environ["BIOBENCH_DISABLE_SSL"] = "1"

    ages_args = args.update(args.ages_args)
    beluga_args = args.update(args.beluga_args)
    birds525_args = args.update(args.birds525_args)
    fishnet_args = args.update(args.fishnet_args)
    imagenet_args = args.update(args.imagenet_args)
    inat21_args = args.update(args.inat21_args)
    iwildcam_args = args.update(args.iwildcam_args)
    kabr_args = args.update(args.kabr_args)
    leopard_args = args.update(args.leopard_args)
    newt_args = args.update(args.newt_args)
    plankton_args = args.update(args.plankton_args)
    plantnet_args = args.update(args.plantnet_args)
    rarespecies_args = args.update(args.rarespecies_args)

    # 2. Run benchmarks.
    jobs = []
    for model_args in args.models_cvml:
        if args.ages_run_cvml:
            job = executor.submit(ages.benchmark_cvml, ages_args, model_args)
            jobs.append(job)
        if args.beluga_run_cvml:
            job = executor.submit(beluga.benchmark_cvml, beluga_args, model_args)
            jobs.append(job)
        if args.birds525_run_cvml:
            job = executor.submit(birds525.benchmark_cvml, birds525_args, model_args)
            jobs.append(job)
        if args.fishnet_run_cvml:
            job = executor.submit(fishnet.benchmark_cvml, fishnet_args, model_args)
            jobs.append(job)
        if args.imagenet_run_cvml:
            job = executor.submit(imagenet.benchmark_cvml, imagenet_args, model_args)
            jobs.append(job)
        if args.inat21_run_cvml:
            job = executor.submit(inat21.benchmark_cvml, inat21_args, model_args)
            jobs.append(job)
        if args.iwildcam_run_cvml:
            job = executor.submit(iwildcam.benchmark_cvml, iwildcam_args, model_args)
            jobs.append(job)
        if args.kabr_run_cvml:
            job = executor.submit(kabr.benchmark_cvml, kabr_args, model_args)
            jobs.append()
        if args.leopard_run_cvml:
            job = executor.submit(leopard.benchmark_cvml, leopard_args, model_args)
            jobs.append(job)
        if args.newt_run_cvml:
            jobs.append(executor.submit(newt.benchmark_cvml, newt_args, model_args))
        if args.plankton_run_cvml:
            job = executor.submit(plankton.benchmark, plankton_args, model_args)
            jobs.append(job)
        if args.plantnet_run_cvml:
            job = executor.submit(plantnet.benchmark, plantnet_args, model_args)
            jobs.append(job)
        if args.rarespecies_run_cvml:
            job = executor.submit(rarespecies.benchmark, rarespecies_args, model_args)
            jobs.append(job)

    for model_args in args.models_vlm:
        if args.ages_run_vlm:
            job = executor.submit(ages.benchmark_vlm, ages_args, model_args)
            jobs.append(job)
        if args.beluga_run_vlm:
            job = executor.submit(beluga.benchmark_vlm, beluga_args, model_args)
            jobs.append(job)
        if args.birds525_run_vlm:
            job = executor.submit(birds525.benchmark_vlm, birds525_args, model_args)
            jobs.append(job)
        if args.fishnet_run_vlm:
            job = executor.submit(fishnet.benchmark_vlm, fishnet_args, model_args)
            jobs.append(job)
        if args.imagenet_run_vlm:
            job = executor.submit(imagenet.benchmark_vlm, imagenet_args, model_args)
            jobs.append(job)
        if args.inat21_run_vlm:
            job = executor.submit(inat21.benchmark_vlm, inat21_args, model_args)
            jobs.append(job)
        if args.iwildcam_run_vlm:
            job = executor.submit(iwildcam.benchmark_vlm, iwildcam_args, model_args)
            jobs.append(job)
        if args.kabr_run_vlm:
            jobs.append(executor.submit(kabr.benchmark_vlm, kabr_args, model_args))
        if args.leopard_run_vlm:
            job = executor.submit(leopard.benchmark_vlm, leopard_args, model_args)
            jobs.append(job)
        if args.newt_run_vlm:
            job = executor.submit(newt.benchmark_vlm, newt_args, model_args)
            jobs.append()
        if args.plankton_run_vlm:
            job = executor.submit(plankton.benchmark, plankton_args, model_args)
            jobs.append(job)
        if args.plantnet_run_vlm:
            job = executor.submit(plantnet.benchmark, plantnet_args, model_args)
            jobs.append(job)
        if args.rarespecies_run_vlm:
            job = executor.submit(rarespecies.benchmark, rarespecies_args, model_args)
            jobs.append(job)

    logger.info("Submitted %d jobs.", len(jobs))

    # 3. Display results.
    os.makedirs(args.report_to, exist_ok=True)
    for i, future in enumerate(submitit.helpers.as_completed(jobs)):
        err = future.exception()
        if err:
            logger.warning("Error running job: %s: %s", err, err.__cause__)
            continue

        model_args, report = future.result()
        save(args, model_args, report)
        logger.info("Finished %d/%d jobs.", i + 1, len(jobs))

    logger.info("Finished.")


if __name__ == "__main__":
    args = tyro.cli(Args)

    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
    min_nofile = 1024 * 8
    if soft < min_nofile:
        resource.setrlimit(resource.RLIMIT_NOFILE, (min_nofile, hard))

    main(args)

```

# Biology Benchmark (`biobench`)

This library is an easy-to-read benchmark for biology-related computer vision tasks.

It aims to make it easy to:

1. Evaluate new models.
2. Add new tasks.
3. Understand meaningful (or not) differences in model performance.

Check out the [docs](https://samuelstevens.me/biobench/) for an interactive leaderboard.

## Getting Started

I use [uv](https://docs.astral.sh/uv/) for Python which makes it easy to manage Python versions, dependencies, virtual environments, etc.

To install uv, run `curl -LsSf https://astral.sh/uv/install.sh | sh`.

Then download at least one of the dataset.
NeWT is really easy to download.

```sh
uv run biobench/newt/download.py --dir ./newt
```

Download it wherever you want on your own filesystem.

Then run just the NeWT benchmark on all the default models.

```sh
CUDA_VISIBLE_DEVICES=0 uv run benchmark.py \
  --newt-run --newt-args.datadir ./newt
```

## Why?

**For computational biologists:** biobench gives you an overview of how different models perform on different tasks. If you have a concrete task that you need to solve, you can easily write a script that matches other, existing tasks and then evaluate many different models on your task. If you have an idea of a task, you can find the most similar existing task(s) on the leaderboard and compare model performance.

**For computer vision researchers:** biobench is a realistic set of benchmarks that more accurately reflect how your model will be used by downstream users. If you aim to train a new foundation vision model, be aware that downstream users will likely not fine-tune it, and will instead use the image embeddings to do all sorts of weird things. Your foundation model should output representations that are universally useful; biobench lets you measure to what degree this is true.

## Concrete Goals

*Easy*, *fast*, *reproducible*, *understandable* evaluation of PyTorch computer vision models across a suite of realistic biology-related vision tasks.

- *Easy*: one launch script, with all options documented in the code and in auto-generated web documentation.
- *Fast*: Each evaluation takes at most 1 hour of A100 or A6000 time. There might be $n$ evaluations, so $n$ hours of A100, but it is embarrassingly parallel and the launch script supports easy parallel running and reporting.
- *Reproducible*: the results include instructions to regenerate these results from scratch, assuming access to the `biobench` Git repo and that web dependencies have not changed.[^web-deps]
- *Understandable*: results are in a machine-readable format, but include a simple human-readable notebook for reading. Common analyses (mean score across all tasks) are included in the notebook and take under one second to run.

[^web-deps]: Web dependencies include things like datasets being available from their original source, Huggingface datasets can be re-downloaded, model checkpoints do not change, etc.


We at [Imageomics](https://imageomics.osu.edu) use this library for testing [BioCLIP](https://imageomics.github.io/bioclip) and other internal models  during development.
Because of this, there are two main classes of tasks:

1. Downstream applications. These are tasks like [KABR](https://samuelstevens.me/biobench/biobench/kabr/index.html) or [Beluga whale re-ID](https://samuelstevens.me/biobench/biobench/beluga/index.html). These tasks represent real problems that computer vision systems fail to solve today.
2. Benchmarks. These are made-up tasks like [Birds525](https://samuelstevens.me/biobench/biobench/birds525/index.html) or [RareSpecies](https://samuelstevens.me/biobench/biobench/rarespecies/index.html) that are artificial tasks, created to help us understand how useful a model might be in the real world for similar tasks.


## Road Map

1. Add contributing guide.
2. Add example images for each task to the docs.
3. Add 5-shot RareSpecies with simpleshot (like in BioCLIP paper). This is blocked because the Huggingface dataset doesn't work ([see this issue](https://huggingface.co/datasets/imageomics/rare-species/discussions/8)).
4. Add FishVista for localized trait prediction. This is another non-classification task, and we are specifically interested in traits. But it will take more work because we have to match bounding boxes and patch-level features which is challenging after resizes.

## Additional Tasks

[Counting insects on sticky insect traps](https://github.com/md-121/yellow-sticky-traps-dataset)
[Predicting plant stem angle](https://plantvision.unl.edu/datasets/download-panicoid-phenomap-1-dataset/)
