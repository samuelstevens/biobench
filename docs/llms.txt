>>>> CONVENTIONS.md
# Code Style

- Keep code simple, explicit, typed, test-driven, and ready for automation.
- Source files are UTF-8 but must contain only ASCII characters. Do not use smart quotes, ellipses, em-dashes, emoji, or other non-ASCII glyphs.
- Docstrings are a single unwrapped paragraph. Rely on your editor's soft-wrap.
- Prefer explicit over implicit constructs. No wildcard imports.

```python
import numpy as np
import polars as pl  # use Polars instead of Pandas
```

Always reference modules by their alias. Never use `from X import *`.

- Decorate every public function or class with `@beartype.beartype`.
- For tensors, add `@jaxtyped(typechecker=beartype.beartype)` and use `jaxtyping` shapes.
- Use frozen `@dataclasses.dataclass` for data containers such as `Config` or `Args`.
- Classes use `CamelCase`, for example `Dataset` or `FeatureExtractor`.
- Functions and variables use `snake_case`, for example `download_split` or `md5_of_file`.
- Constants are `UPPER_SNAKE`, defined at module top, for example `URLS = {...}`.
- File descriptors end in `fd`, for example `log_fd`.
- File paths end in `_fpath`; directories end in `_dpath`.
- Constructors follow verb prefixes:
  - `make_...` returns an object.
  - `get_...` returns a primitive value such as a string or path.
  - `setup_...` performs side effects and returns nothing.

## Shape-suffix notation

Attach suffixes to tensor variables to clarify shape:

- B – batch size
- W – patch grid width
- H – patch grid height
- D – feature dimension
- L – number of latents
- C – number of classes

Example: `acts_BWHD` has shape (batch, width, height, d).

## Logging and progress bars

```python
logger = logging.getLogger(__name__)
logger.info("message")

for x in helpers.progress(dataset):
    ...
```

Use `helpers.progress` instead of `tqdm` so that logging is useful in non-interactive contexts (log files, batch jobs, etc).

# Testing

- Use pytest with fixtures and parameterization.
- Use Hypothesis for property-based tests, especially in helpers.
- Mark slow integration tests with `@pytest.mark.slow`.

# Project layout

Each task lives in its own folder, for example `biobench/herbarium19/`.  
Inside a task folder:

- `download.py` fetches the dataset.
- `__init__.py` exposes the task API, including a `benchmark(cfg)` entry point.

## Download scripts

- Start each downloader with a header line `/// script` and a `dependencies = [...]` list.
- Stream with `requests.get(..., stream=True)` and wrap in `tqdm` for progress.
- Verify checksums before extraction.

>>>> README.md
# Biology Benchmark (`biobench`)

![Coverage](docs/coverage.svg)

This library is an easy-to-read benchmark for biology-related computer vision tasks.

It aims to make it easy to:

1. Evaluate new models.
2. Add new tasks.
3. Understand meaningful (or not) differences in model performance.

Check out the [docs](https://samuelstevens.me/biobench/) for an introduction.

## Getting Started

I use [uv](https://docs.astral.sh/uv/) for Python which makes it easy to manage Python versions, dependencies, virtual environments, etc.

To install uv, run `curl -LsSf https://astral.sh/uv/install.sh | sh`.

Then download at least one of the dataset.
NeWT is really easy to download.

```sh
uv run biobench/newt/download.py --dir ./newt
```

Download it wherever you want on your own filesystem.

## Why?

**For computational biologists:** biobench gives you an overview of how different models perform on different tasks. If you have a concrete task that you need to solve, you can easily write a script that matches other, existing tasks and then evaluate many different models on your task. If you have an idea of a task, you can find the most similar existing task(s) on the leaderboard and compare model performance.

**For computer vision researchers:** biobench is a realistic set of benchmarks that more accurately reflect how your model will be used by downstream users. If you aim to train a new foundation vision model, be aware that downstream users will likely not fine-tune it, and will instead use the image embeddings to do all sorts of weird things. Your foundation model should output representations that are universally useful; biobench lets you measure to what degree this is true.

## Concrete Goals

*Easy*, *fast*, *reproducible*, *understandable* evaluation of PyTorch computer vision models across a suite of realistic biology-related vision tasks.

- *Easy*: one launch script, with all options documented in the code and in auto-generated web documentation.
- *Fast*: Each evaluation takes at most 1 hour of A100 or A6000 time. There might be $n$ evaluations, so $n$ hours of A100, but it is embarrassingly parallel and the launch script supports easy parallel running and reporting.
- *Reproducible*: the results include instructions to regenerate these results from scratch, assuming access to the `biobench` Git repo and that web dependencies have not changed.[^web-deps]
- *Understandable*: results are in a machine-readable format, but include a simple human-readable notebook for reading. Common analyses (mean score across all tasks) are included in the notebook and take under one second to run.

[^web-deps]: Web dependencies include things like datasets being available from their original source, Huggingface datasets can be re-downloaded, model checkpoints do not change, etc.


We at [Imageomics](https://imageomics.osu.edu) use this library for testing [BioCLIP](https://imageomics.github.io/bioclip) and other internal models  during development.
Because of this, there are two main classes of tasks:

1. Downstream applications. These are tasks like [KABR](https://samuelstevens.me/biobench/biobench/kabr/index.html) or [Beluga whale re-ID](https://samuelstevens.me/biobench/biobench/beluga/index.html). These tasks represent real problems that computer vision systems fail to solve today.
2. Benchmarks. These are made-up tasks like [RareSpecies](https://samuelstevens.me/biobench/biobench/rarespecies/index.html) that are artificial tasks, created to help us understand how useful a model might be in the real world for similar tasks.


## Road Map

1. Add contributing guide.
2. Add example images for each task to the docs.
3. Add 5-shot RareSpecies with simpleshot (like in BioCLIP paper). This is blocked because the Huggingface dataset doesn't work ([see this issue](https://huggingface.co/datasets/imageomics/rare-species/discussions/8)).
4. Add FishVista for localized trait prediction. This is another non-classification task, and we are specifically interested in traits. But it will take more work because we have to match bounding boxes and patch-level features which is challenging after resizes.

## Additional Tasks

[Counting insects on sticky insect traps](https://github.com/md-121/yellow-sticky-traps-dataset)
[Predicting plant stem angle](https://plantvision.unl.edu/datasets/download-panicoid-phenomap-1-dataset/)

# Installing

>>>> REGRESSIONS.md
# Regressions

Last checked: 2025-05-01

# 1 failing test(s)

- biobench/test_auto_batch_size.py::test_schedule_not_materialised
# Coverage

Coverage: 2950/4394 lines (67.1%)

>>>> __init__.py
""" """

import typing

import tyro

from . import aimv2, third_party_models, vjepa
from .registry import list_vision_backbones, register_vision_backbone

register_vision_backbone("timm", third_party_models.Timm)
register_vision_backbone("open-clip", third_party_models.OpenClip)
register_vision_backbone("dinov2", third_party_models.DinoV2)
register_vision_backbone("sam2", third_party_models.SAM2)
register_vision_backbone("aimv2", aimv2.AIMv2)
register_vision_backbone("vjepa", vjepa.VJEPA)

# Some helpful types
if typing.TYPE_CHECKING:
    # Static type seen by language servers, type checkers, etc.
    ModelOrg = str
else:
    # Runtime type used by tyro.
    ModelOrg = tyro.extras.literal_type_from_choices(list_vision_backbones())

>>>> aimv2.py
import dataclasses
import json
import os
import re

import beartype
import requests
import safetensors.torch
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import helpers, registry


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """This is the configuration class to store the configuration of an `AIMv2Model`.

    Instantiating a configuration with the defaults will yield a similar configuration to that of the [apple/aimv2-large-patch14-224](https://huggingface.co/apple/aimv2-large-patch14-224).

    Args:
        hidden_size: Dimension of the hidden representations.
        intermediate_size: Dimension of the SwiGLU representations.
        num_hidden_layers: Number of hidden layers in the Transformer.
        num_attention_heads: Number of attention heads for each attention layer in the Transformer.
        num_channels: Number of input channels.
        image_size: Image size.
        patch_size: Patch size.
        rms_norm_eps: Epsilon value used for the RMS normalization layer.
        attention_dropout: Dropout ratio for attention probabilities.
        projection_dropout: Dropout ratio for the projection layer after the attention.
        torch_dtype: Data type.
        qkv_bias: Whether to add a bias to the queries, keys and values.
        use_bias: Whether to add a bias in the feed-forward and projection layers.
    """

    hidden_size: int
    intermediate_size: int
    num_hidden_layers: int
    num_attention_heads: int
    num_channels: int
    image_size: int
    patch_size: int
    rms_norm_eps: float
    attention_dropout: float
    projection_dropout: float
    torch_dtype: str
    qkv_bias: bool
    use_bias: bool


@beartype.beartype
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

    def extra_repr(self) -> str:
        return f"{tuple(self.weight.shape)}, eps={self.eps}"

    def _norm(self, x: torch.Tensor) -> torch.Tensor:
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)


@jaxtyped(typechecker=beartype.beartype)
class SwiGLUFFN(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()

        self.fc1 = torch.nn.Linear(
            cfg.hidden_size, cfg.intermediate_size, bias=cfg.use_bias
        )
        self.fc2 = torch.nn.Linear(
            cfg.intermediate_size, cfg.hidden_size, bias=cfg.use_bias
        )
        self.fc3 = torch.nn.Linear(
            cfg.hidden_size, cfg.intermediate_size, bias=cfg.use_bias
        )

    def forward(self, x: Float[Tensor, "*batch d"]) -> Float[Tensor, "*batch d"]:
        x = torch.nn.functional.silu(self.fc1(x)) * self.fc3(x)
        x = self.fc2(x)
        return x


@jaxtyped(typechecker=beartype.beartype)
class PatchEmbed(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.proj = torch.nn.Conv2d(
            cfg.num_channels,
            cfg.hidden_size,
            kernel_size=(cfg.patch_size, cfg.patch_size),
            stride=(cfg.patch_size, cfg.patch_size),
        )
        self.norm = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.proj(x).flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x


@jaxtyped(typechecker=beartype.beartype)
class ViTPreprocessor(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        num_patches = (cfg.image_size // cfg.patch_size) ** 2

        self.patchifier = PatchEmbed(cfg)
        self.pos_embed = torch.nn.Parameter(
            torch.zeros((1, num_patches, cfg.hidden_size))
        )

    def forward(
        self, x: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches d"]:
        tokens = self.patchifier(x)
        _, N, _ = tokens.shape
        pos_embed = self.pos_embed.to(tokens.device)
        tokens = tokens + pos_embed[:, :N]
        return tokens


@jaxtyped(typechecker=beartype.beartype)
class Attention(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        dim = cfg.hidden_size

        self.num_heads = cfg.num_attention_heads
        self.qkv = torch.nn.Linear(dim, dim * 3, bias=cfg.qkv_bias)
        self.attn_drop = torch.nn.Dropout(cfg.attention_dropout)
        self.proj = torch.nn.Linear(dim, dim, bias=cfg.use_bias)
        self.proj_drop = torch.nn.Dropout(cfg.projection_dropout)

    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        B, N, C = x.shape
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, C // self.num_heads)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = qkv.unbind(0)

        x = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask)
        x = x.transpose(1, 2).contiguous().reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


@jaxtyped(typechecker=beartype.beartype)
class Block(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.attn = Attention(cfg)
        self.norm_1 = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)
        self.mlp = SwiGLUFFN(cfg)
        self.norm_2 = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)

    def forward(
        self, x: torch.Tensor, mask: torch.Tensor | None = None
    ) -> torch.Tensor:
        x = x + self.attn(self.norm_1(x), mask)
        x = x + self.mlp(self.norm_2(x))
        return x


@jaxtyped(typechecker=beartype.beartype)
class Transformer(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.blocks = torch.nn.ModuleList([
            Block(cfg) for _ in range(cfg.num_hidden_layers)
        ])
        self.post_trunk_norm = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)

    def forward(self, tokens: Float[Tensor, "..."]) -> Float[Tensor, "..."]:
        for block in self.blocks:
            tokens = block(tokens)
        tokens = self.post_trunk_norm(tokens)
        return tokens


@jaxtyped(typechecker=beartype.beartype)
class AIMv2(registry.VisionBackbone):
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()

        # Config
        with open(download_hf_file(ckpt, "config.json"), "r") as fd:
            cfg_dct = json.load(fd)
        for key in ("architectures", "auto_map", "transformers_version"):
            cfg_dct.pop(key)
        assert cfg_dct.pop("model_type") == "aimv2"
        cfg = Config(**cfg_dct)

        # Model
        self.preprocessor = ViTPreprocessor(cfg)
        self.trunk = Transformer(cfg)

        # Pre-trained weights
        ckpt_fpath = download_hf_file(ckpt, "model.safetensors")
        state_dict = safetensors.torch.load_file(ckpt_fpath)
        self.load_state_dict(state_dict)

        # Extract image size from checkpoint name using regex

        match = re.search(r"patch\d+-(\d+)", ckpt)
        self.size = int(match.group(1)) if match else 224  # Default to 224 if not found

    def forward(self, x: Float[Tensor, "..."]) -> Float[Tensor, "..."]:
        x = self.preprocessor(x)
        x = self.trunk(x)
        return x

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        x = self.forward(batch)
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

    def make_img_transform(self):
        import torch
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=self.size),
            v2.CenterCrop(size=(self.size, self.size)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711],
            ),
        ])


@beartype.beartype
def download_hf_file(ckpt: str, filepath: str, *, force: bool = False) -> str:
    """
    Download a file from a Hugging Face model repository.

    Args:
        ckpt: The model checkpoint identifier (e.g., 'apple/aimv2-large-patch14-224')
        filepath: The path to the file within the repo (e.g., 'config.json')
        force: Whether to force download even if the file exists locally

    Returns:
        The path to the downloaded file on the local filesystem
    """

    # Construct the URL
    url = f"https://huggingface.co/{ckpt}/resolve/main/{filepath}"

    # Create the local path
    cache_dir = helpers.get_cache_dir()
    local_dir = os.path.join(cache_dir, "hf", ckpt)
    local_path = os.path.join(local_dir, filepath)

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(local_path), exist_ok=True)

    # Check if the file exists
    if os.path.exists(local_path) and not force:
        return local_path

    # Download the file
    response = requests.get(url, stream=True)
    response.raise_for_status()

    with open(local_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

    return local_path

>>>> beluga/__init__.py
"""
Individual re-identification of Beluga whales (*Delphinapterus leucas*) using [this LILA BC dataset](https://lila.science/datasets/beluga-id-2022/).

We use a very simple method:

1. Embed all images using a vision backbone.
2. For each image, treat it as a test image and find its nearest neighbor (k=1).
3. Give a score of 1.0 if the nearest neighbor is the same individual, otherwise 0.0.

You could improve this with nearest centroid classification, k>1, or any number of fine-tuning techniques.
But we are simply interested in seeing if models embed images of the same individual closer together in representation space.

If you use this task, please cite the original dataset paper and the paper that proposed this evaluation method:

```
@article{algasov2024understanding,
  title={Understanding the Impact of Training Set Size on Animal Re-identification},
  author={Algasov, Aleksandr and Nepovinnykh, Ekaterina and Eerola, Tuomas and K{\"a}lvi{\"a}inen, Heikki and Stewart, Charles V and Otarashvili, Lasha and Holmberg, Jason A},
  journal={arXiv preprint arXiv:2405.15976},
  year={2024}
}

@inproceedings{vcermak2024wildlifedatasets,
  title={WildlifeDatasets: An open-source toolkit for animal re-identification},
  author={{\v{C}}erm{\'a}k, Vojt{\v{e}}ch and Picek, Lukas and Adam, Luk{\'a}{\v{s}} and Papafitsoros, Kostas},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5953--5963},
  year={2024}
}
```
"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import sklearn.neighbors
import torch
import torchvision.datasets
from jaxtyping import Float, Shaped, jaxtyped
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("beluga")


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """Run the BelugaID benchmark."""
    backbone = registry.load_vision_backbone(cfg.model)

    # Embed all images.
    features = get_features(cfg, backbone)
    # Convert string names into integer labels.
    encoder = sklearn.preprocessing.OrdinalEncoder(dtype=int)
    y = encoder.fit_transform(features.labels.reshape(-1, 1)).reshape(-1)

    clf = sklearn.neighbors.NearestNeighbors(n_neighbors=1)
    clf.fit(features.x, y)
    preds = clf.kneighbors(return_distance=False)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(features.ids, preds, y)
    ]

    return reporting.Report("beluga", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    """A block of features."""

    x: Float[Tensor, "n dim"]
    """Input features; from a `biobench.registry.VisionBackbone`."""
    labels: Shaped[np.ndarray, " n"]
    """Individual name."""
    ids: Shaped[np.ndarray, " n"]
    """Array of image ids."""

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)

    @property
    def n(self) -> int:
        return len(self.ids)


@beartype.beartype
def collate_fn(batch):
    imgs = torch.stack([img for img, _ in batch])
    metadata = [meta for _, meta in batch]
    return imgs, metadata


@beartype.beartype
@torch.no_grad()
def get_features(cfg: config.Experiment, backbone: registry.VisionBackbone) -> Features:
    """
    Get a block of features from a vision backbone.

    Args:
        args: BelugaID arguments.
        backbone: visual backbone.
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    if not os.path.isdir(cfg.data.beluga):
        msg = f"Path '{cfg.data.beluga}' doesn't exist. Did you download the Beluga dataset?"
        raise ValueError(msg)

    dataset = torchvision.datasets.CocoDetection(
        os.path.join(cfg.data.beluga, "beluga.coco", "images", "train2022"),
        os.path.join(
            cfg.data.beluga, "beluga.coco", "annotations", "instances_train2022.json"
        ),
        img_transform,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        collate_fn=collate_fn,
    )

    all_features, all_labels, all_ids = [], [], []

    def probe(batch):
        imgs, _ = batch
        imgs = imgs.to(cfg.device, non_blocking=True)

        with torch.amp.autocast("cuda"):
            backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc="beluga"):
            imgs, metadata = next(it)
            imgs = imgs.to(cfg.device, non_blocking=True)

            with torch.amp.autocast("cuda"):
                features = backbone.img_encode(imgs).img_features

            assert all(len(meta) == 1 for meta in metadata)
            labels = [meta[0]["name"] for meta in metadata]
            ids = [str(meta[0]["image_id"]) for meta in metadata]

            all_features.append(features.cpu())
            all_labels.extend(labels)
            all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = np.array(all_labels)

    return Features(all_features, all_labels, all_ids)

>>>> beluga/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Downloads the Begula whale dataset from lila.science.
"""

import dataclasses
import os
import tarfile

import requests
import tqdm
import tyro

URL = "http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/wild-me/beluga.coco.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configuration."""

    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images."""
    expand: bool = True
    """whether to expand tarfiles into a folder."""


def main(args: Args):
    """Download and unzip the data."""
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_tar_path = os.path.join(args.dir, "beluga.coco.tar.gz")

    if args.download:
        # Download images.
        r = requests.get(URL, stream=True)
        r.raise_for_status()
        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_tar_path}.")

    if args.expand:
        with tarfile.open(images_tar_path, "r") as tar:
            for member in tqdm.tqdm(
                tar, desc="Extracting images", total=len(tar.getnames())
            ):
                tar.extract(member, path=args.dir, filter="data")

        print(f"Extracted images: {args.dir}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> benchmark.py
"""
Entrypoint for running all tasks in `biobench`.

Most of this script is self documenting.
Run `python benchmark.py --help` to see all the options.

Note that you will have to download all the datasets, but each dataset includes its own download script with instructions.
For example, see `biobench.newt.download` for an example.

.. include:: ./design.md
"""

import collections
import importlib
import logging
import os

import beartype
import submitit
import tyro

from biobench import config, helpers, jobkit, reporting


@beartype.beartype
def main(
    cfgs: list[str] = [os.path.join("configs", "neurips.toml")],
    dry_run: bool = True,
    max_pending: int = 8,
):
    """
    Launch all jobs, using either a local GPU or a Slurm cluster. Then report results and save to disk.

    Args:
        cfgs: List of paths to TOML config files.
        dry_run: If --no-dry-run, actually run experiment.
        max_pending: Number of jobs that can be claimed by any one launcher process.
    """

    # Load all configs from the provided paths.
    cfgs = [cfg for path in cfgs for cfg in config.load(path)]

    if not cfgs:
        print("No configurations loaded.")
        return

    # ------------------------------------------------------
    # Verify all configs have consistent execution settings.
    # ------------------------------------------------------
    first = cfgs[0]
    for cfg in cfgs[1:]:
        if cfg.slurm_acct != first.slurm_acct:
            raise ValueError("All configs must have the same slurm_acct")
        if cfg.log_to != first.log_to:
            raise ValueError("All configs must have the same log_to directory")
        if cfg.ssl != first.ssl:
            raise ValueError("All configs must have the same ssl setting")

    # --------------
    # Setup logging.
    # --------------
    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    level = logging.DEBUG if cfg.verbose else logging.INFO
    logging.basicConfig(level=level, format=log_format)
    logger = logging.getLogger("benchmark.py")

    # ---------------
    # Setup executor.
    # ---------------
    if first.slurm_acct:
        executor = submitit.SlurmExecutor(folder=first.log_to)
        executor.update_parameters(
            time=30,
            gpus_per_node=1,
            cpus_per_task=8,
            stderr_to_stdout=True,
            partition="debug",
            account=first.slurm_acct,
        )
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not first.ssl:
            executor.update_parameters(setup=["export BIOBENCH_DISABLE_SSL=1"])
    else:
        executor = submitit.DebugExecutor(folder=first.log_to)
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not first.ssl:
            os.environ["BIOBENCH_DISABLE_SSL"] = "1"

    db = reporting.get_db(first)

    # Clear old (5 days+) runs.
    cleared = reporting.clear_stale_claims(db, max_age_hours=24 * 5)
    logger.info("Cleared %d stale jobs from 'runs' table.", cleared)

    job_stats = collections.defaultdict(int)
    model_stats = collections.defaultdict(int)
    fq = jobkit.FutureQueue(max_size=max_pending)
    exit_hook = jobkit.ExitHook(
        lambda args: reporting.release_run(db, *args)
    ).register()

    def flush_one():
        """
        Get the next finished job from queue, blocking if necessary, write the report and relinquish the claim.
        """
        job, cfg, task = fq.pop()
        try:
            report: reporting.Report = job.result()
            report.write()
            logger.info("%s+%s/%s done", task, cfg.model.org, cfg.model.ckpt)
        except Exception as err:
            logger.info("%s+%s/%s failed: %s", task, cfg.model.org, cfg.model.ckpt, err)
        finally:
            exit_hook.discard((cfg, task))

    for cfg in cfgs:
        for task, data_root in cfg.data.to_dict().items():
            reason = get_skip_reason(db, cfg, task, data_root, dry_run)
            if reason:
                job_stats[reason] += 1
                continue

            if dry_run:
                job_stats["todo"] += 1
                model_stats[cfg.model.ckpt] += 1
                continue  # no side-effect

            if not reporting.claim_run(db, cfg, task):
                job_stats["queued"] += 1  # someone else just grabbed it
                continue

            exit_hook.add((cfg, task))  # for signal/atexit handler
            job = executor.submit(worker, task, cfg)
            fq.submit((job, cfg, task))
            job_stats["submitted"] += 1

            while fq.full():
                flush_one()

    if dry_run:
        logger.info("Job Summary:")
        logger.info("%-20s | %-5s", "Reason", "Count")
        logger.info("-" * 31)
        for reason, count in sorted(job_stats.items()):
            logger.info("%-20s | %5d", reason, count)
        logger.info("-" * 31)

        logger.info("Model Summary:")
        logger.info("%-50s | %-5s", "Model", "Count")
        logger.info("-" * 61)
        for model, count in sorted(model_stats.items()):
            logger.info("%-50s | %5d", model, count)
        logger.info("-" * 61)
        return

    while fq:
        flush_one()

    logger.info("Finished.")


@beartype.beartype
def worker(task_name: str, cfg: config.Experiment) -> reporting.Report:
    helpers.bump_nofile(512)

    module = importlib.import_module(f"biobench.{task_name}")
    return module.benchmark(cfg)


@beartype.beartype
def get_skip_reason(
    db, cfg: config.Experiment, task: str, data_root: str, dry_run: bool
) -> str | None:
    """Return a short reason string if we should skip (None -> keep)."""
    try:
        importlib.import_module(f"biobench.{task}")
    except ModuleNotFoundError:
        return "no code"

    if not data_root:
        return "no data"

    if reporting.already_ran(db, cfg, task):
        return "done"

    if reporting.is_claimed(db, cfg, task):
        return "queued"

    return None


if __name__ == "__main__":
    tyro.cli(main)

>>>> confidence-intervals.md
# Confidence Intervals

Recommended Reading:

* [Sebastian Raschka's blogpost on confidence intervals for ML](https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html)

## BioBench Confidence Intervals

We resample from the test predictions $N$ times.
Then we re-calculate the statistic of interest (typically just mean accuracy) for each resampling.
Over $N$ re-samples, we collect a distribution of possible mean accuracies.
Then, if we repeated this process 100 times, we could assume that in 95 of the cases, the true mean accuracy is in the sampled confidence interval.

> For me personally, this is pretty convoluted logic.
> I just assume that my true mean is 95% likely to be in my confidence interval.[^wrong]

In practice, we use $N = 500$.

[^wrong]: I know this is wrong. I know. You don't need to tell me. I know it's wrong. But it's way freakin' easier to reason about and it's only subtley wrong and it's still better than just a mean. I know it's wrong. I know.

## Note on Confidence Interval Types

For different tasks, we *could* use different kinds of bootstrapping.
Non-parametric methods like nearest centroid classifiers or KNN are particularly easy to re-evaluate and so we could bootstrap the entire process.
Parametric methods like linear classifiers or SVMs are slow to train, so we don't want to repeat the training step 200+ times.

In practice, however, we simply bootstrap test set predictions because it can be cheaply applied to all benchmarks regardless of training or inference cost.
It requires only that we have a $N$-dimensional vector of scores, where $N$ is the number of test examples, which is always under 10M, so at most a 40MB vector.

>>>> config.py
import dataclasses
import os
import tomllib
import typing

import beartype


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Model:
    """Configuration for a model to be evaluated.

    This class defines the essential parameters needed to identify and load a specific model for evaluation in the benchmark.

    Attributes:
        org: Organization or source of the model (e.g., "open-clip").
        ckpt: Checkpoint or specific model identifier (e.g., "ViT-B-16/openai").
    """

    org: str
    ckpt: str

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Data:
    beluga: str = ""
    """Data pathfor the Beluga whale re-ID benchmark."""
    fishnet: str = ""
    """Data path for the FishNet benchmark."""
    fungiclef: str = ""
    """Data path for the FungiCLEF benchmark."""
    imagenet1k: str = ""
    """Data path for the ImageNet-1K benchmark. You can put anything (like 'huggingface') because it is downloaded from HF."""
    newt: str = ""
    """Data path for the NeWT benchmark."""
    herbarium19: str = ""
    """Data path for the Herbarium19 benchmark."""
    inat21: str = ""
    """Data path for the iNat2021 benchmark."""
    kabr: str = ""
    """Data path for the KABR benchmark."""
    mammalnet: str = ""
    """Data path for the MammalNet benchmark."""
    plantnet: str = ""
    """Data path for the Pl@ntNet benchmark."""
    plankton: str = ""
    """Data path for the planktok classification benchmark."""
    iwildcam: str = ""
    """Data path for the iWildCam benchmark."""

    def to_dict(self) -> dict[str, str]:
        return dataclasses.asdict(self)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Experiment:
    """Configuration to run one or more benchmarks in a parallel setting."""

    model: Model

    slurm_acct: str = ""
    """Slurm account. A non-empty string means using Slurm."""
    cfg: str = os.path.join("configs", "neurips.toml")
    """Path to TOML config file."""
    device: typing.Literal["cpu", "mps", "cuda"] = "cuda"
    """which kind of accelerator to use."""
    debug: bool = False
    """whether to run in debug mode."""
    n_train: int = -1
    """Number of maximum training samples. Negative number means use all of them."""
    ssl: bool = True
    """Use SSL when connecting to remote servers to download checkpoints; use --no-ssl if your machine has certificate issues. See `biobench.third_party_models.get_ssl()` for a discussion of how this works."""

    n_workers: int = 4
    """Number of dataloader workers."""
    batch_size: int = 16
    """Initial batch size to start with for tuning."""

    data: Data = dataclasses.field(default_factory=Data)

    report_to: str = os.path.join(".", "results")
    """where to save reports to."""
    log_to: str = os.path.join(".", "logs")
    """where to save logs to."""
    seed: int = 17
    """Random seed."""
    verbose: bool = False
    """DEBUG logging or not."""

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)

    def update(self, other):
        return dataclasses.replace(
            other,
            device=self.device,
            debug=self.debug,
            n_train=self.n_train,
            parallel=self.parallel,
        )


def load(path: str) -> list[Experiment]:
    """Load experiments from a TOML file.

    None of the fields in Experiment are lists, so anytime we find a list in the TOML, we add another dimension to our grid search over all possible experiments.
    """
    with open(path, "rb") as f:
        raw = tomllib.load(f)

    if not isinstance(raw, dict):
        raise ValueError(
            f"TOML file {path} must contain a dictionary at the root level"
        )

    # Extract models list
    models = raw.pop("models", [])
    if not isinstance(models, list):
        raise ValueError("models must be a list of tables in TOML")

    # Start with models as base experiments
    experiments = [{"model": Model(**model)} for model in models]

    # Handle data config specially
    data = raw.pop("data", {})

    # For each remaining field in the TOML
    for key, value in raw.items():
        new_experiments = []

        # Convert single values to lists
        if not isinstance(value, list):
            value = [value]

        # For each existing partial experiment
        for exp in experiments:
            # Add every value for this field
            for v in value:
                new_exp = exp.copy()
                new_exp[key] = v
                new_experiments.append(new_exp)

        experiments = new_experiments

    # Now add the NeWT config to all experiments
    for exp in experiments:
        exp["data"] = Data(**data)

    # Convert dictionaries to Experiment objects
    return [Experiment(**exp) for exp in experiments]

>>>> fishnet/__init__.py
"""
# FishNet: Fish Recognition, Detection, and Functional Traits Prediction

FishNet ([paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_FishNet_A_Large-scale_Dataset_and_Benchmark_for_Fish_Recognition_Detection_ICCV_2023_paper.pdf), [code](https://github.com/faixan-khan/FishNet)) is a large-scale diverse dataset containing 94,532 images from 17,357 aquatic species.
It contains three benchmarks: fish classification, fish detection, and functional traits prediction.

We mainly focus on the third task.
We train an two-layer MLP on the visual features extracted by different model backbones to predict the presence or absence of 9 different traits.

If you use this evaluation, be sure to cite the original work:

```
@inproceedings{fishnet,
    author    = {Khan, Faizan Farooq and Li, Xiang and Temple, Andrew J. and Elhoseiny, Mohamed},
    title     = {FishNet: A Large-scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {20496-20506}
}
```

This task was contributed by [Jianyang Gu](https://vimar-gu.github.io/).
"""

import logging
import os.path

import beartype
import numpy as np
import polars as pl
import sklearn.metrics
import torch
from jaxtyping import Float, Int, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("fishnet")

batch_size = 1024
learning_rate = 3e-4
n_steps = 30_000
threshold = 0.5


@jaxtyped(typechecker=beartype.beartype)
class Features(torch.utils.data.Dataset):
    """
    A dataset of learned features (dense vectors).
    """

    x: Float[Tensor, " n dim"]
    """Dense feature vectors from a vision backbone."""
    y: Int[Tensor, " n 9"]
    """0/1 labels of absence/presence of 9 different traits."""
    ids: list[str]
    """Image ids."""

    def __init__(
        self,
        x: Float[Tensor, " n dim"],
        y: Int[Tensor, " n n_classes"],
        ids: list[str],
    ):
        self.x = x
        self.y = y
        self.ids = ids

    @property
    def dim(self) -> int:
        """Dimension of the dense feature vectors."""
        _, dim = self.x.shape
        return dim

    def __len__(self) -> int:
        return len(self.x)

    def __getitem__(
        self, index
    ) -> tuple[Float[Tensor, " dim"], Int[Tensor, " n_classes"], str]:
        return self.x[index], self.y[index], self.ids[index]


@beartype.beartype
def init_clf(input_dim: int) -> torch.nn.Module:
    """A simple MLP classifier consistent with the design in FishNet."""
    return torch.nn.Sequential(
        torch.nn.Linear(input_dim, 512),
        torch.nn.Dropout(0.5),
        torch.nn.Linear(512, 9),
    )


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    """
    Calculate the macro-averaged F1 score across all fish trait predictions.

    For each fish image, we predict 9 binary traits:

    1. Feeding Path (benthic/pelagic)
    2. Tropical habitat (yes/no)
    3. Temperate habitat (yes/no)
    4. Subtropical habitat (yes/no)
    5. Boreal habitat (yes/no)
    6. Polar habitat (yes/no)
    7. Freshwater habitat (yes/no)
    8. Saltwater habitat (yes/no)
    9. Brackish water habitat (yes/no)

    The macro-averaging:

    1. Calculates an F1 score for each trait independently
    2. Takes the unweighted mean of these 9 F1 scores

    This ensures each trait contributes equally to the final score, regardless of class imbalance in the dataset (e.g., if there are many more tropical fish than brackish water fish).

    Args:
        preds: List of predictions, each containing:
            - info["y_pred"]: List of 9 binary predictions
            - info["y_true"]: List of 9 binary ground truth values

    Returns:
        The macro-averaged F1 score across all 9 traits
    """
    y_pred = np.array([pred.info["y_pred"] for pred in preds])
    y_true = np.array([pred.info["y_true"] for pred in preds])
    return sklearn.metrics.f1_score(
        y_true, y_pred, average="macro", labels=np.unique(y_true)
    )


def infinite(dataloader):
    """Creates an infinite iterator from a dataloader by creating a new iterator each time the previous one is exhausted.

    Args:
        dataloader: A PyTorch dataloader or similar iterable

    Yields:
        Batches from the dataloader, indefinitely
    """
    while True:
        # Create a fresh iterator from the dataloader
        it = iter(dataloader)
        for batch in it:
            yield batch


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    The FishNet benchmark.
    """
    # 1. Load model.
    backbone = registry.load_vision_backbone(cfg.model)

    # 2. Get features.
    train_dataset = get_features(cfg, backbone, is_train=True)
    test_dataset = get_features(cfg, backbone, is_train=False)

    # 3. Set up classifier.
    classifier = init_clf(train_dataset.dim).to(cfg.device)

    # 4. Load datasets for classifier.
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False
    )
    optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)
    criterion = torch.nn.BCEWithLogitsLoss()
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

    # 5. Fit the classifier.
    it = infinite(train_loader)
    for step in range(n_steps):
        features, labels, _ = next(it)
        features = features.to(cfg.device)
        labels = labels.to(cfg.device, dtype=torch.float)
        output = classifier(features)
        loss = criterion(output, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        # Evaluate the classifier.
        if (step + 1) % 1_000 == 0:
            preds = predict(cfg, classifier, test_loader)
            logger.info(
                "Step %d/%d (%.1f%%): %.3f (macro F1)",
                step + 1,
                n_steps,
                (step + 1) / n_steps * 100,
                score(preds),
            )
    preds = predict(cfg, classifier, test_loader)

    return reporting.Report("fishnet", preds, cfg)


@beartype.beartype
def predict(
    cfg: config.Experiment, classifier: torch.nn.Module, dataloader
) -> list[reporting.Prediction]:
    """
    Evaluates the trained classifier on a test split.

    Returns:
        List of `reporting.Prediction`s
    """
    total = 2 if cfg.debug else len(dataloader)
    it = iter(dataloader)
    preds = []
    for b in range(total):
        features, labels, ids = next(it)
        features = features.to(cfg.device)
        labels = labels.numpy()
        with torch.no_grad():
            pred_logits = classifier(features)
        pred_logits = (pred_logits > threshold).cpu().numpy()
        for id, pred, true in zip(ids, pred_logits, labels):
            info = {"y_pred": pred.tolist(), "y_true": true.tolist()}
            preds.append(reporting.Prediction(id, (pred == true).mean().item(), info))

    return preds


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    """Extract visual features."""
    if not os.path.isdir(cfg.data.fishnet):
        msg = f"Path '{cfg.data.fishnet}' doesn't exist. Did you download the FishNet dataset? See the docstring at the top of this file for instructions."
        raise ValueError(msg)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    file = "train.csv" if is_train else "test.csv"
    dataset = ImageDataset(cfg.data.fishnet, file, transform=img_transform)
    if is_train and cfg.n_train > 0:
        i = np.random.default_rng(seed=cfg.seed).choice(
            len(dataset), cfg.n_train, replace=False, shuffle=False
        )
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        shuffle=False,
    )

    def probe(batch):
        imgs, labels, ids = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features  # forward only

    all_features, all_labels, all_ids = [], [], []

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"fish/{file}"):
            images, labels, ids = next(it)
            images = images.to(cfg.device)

            features = backbone.img_encode(images).img_features
            all_features.append(features.cpu())
            all_labels.append(labels)

            all_ids.extend(ids)

    # Keep the Tensor data type for subsequent training
    all_features = torch.cat(all_features, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    assert len(all_ids) == len(dataset)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
class ImageDataset(torch.utils.data.Dataset):
    """
    A dataset for CV+ML that loads the required attribute labels.
    """

    def __init__(self, root_dir: str, csv_file: str, transform):
        self.root_dir = root_dir
        self.csv_file = os.path.join(self.root_dir, csv_file)
        self.df = pl.read_csv(self.csv_file).with_row_index()
        self.all_columns = [
            "FeedingPath",
            "Tropical",
            "Temperate",
            "Subtropical",
            "Boreal",
            "Polar",
            "freshwater",
            "saltwater",
            "brackish",
        ]
        for col in self.all_columns:
            self.df = self.df.filter(self.df[col].is_not_null())
        self.transform = transform

        # Corresponding column indices
        self.image_col = 4
        self.folder_col = 13
        self.label_cols = [15, 16, 17, 18, 19, 20, 21, 22, 23]
        logger.info("csv file: %s has %d item.", csv_file, len(self.df))

    def __getitem__(
        self, index
    ) -> tuple[Float[Tensor, "3 width height"], Int[Tensor, "9"], str]:
        row_data = self.df.row(index)
        image_name = row_data[self.image_col]
        image_name = image_name.split("/")[-1]
        folder = row_data[self.folder_col]
        image_path = os.path.join(self.root_dir, "Image_Library", folder, image_name)
        image = Image.open(image_path)

        # Extract the required attribute labels.
        label = []
        for col in self.label_cols:
            value = row_data[col]
            if col == 15:
                if value == "pelagic":
                    value = 1
                elif value == "benthic":
                    value = 0
                else:
                    raise ValueError("FeedingPath can only be pelagic or benthic.")
            label.append(value)
        label = torch.tensor(label)

        if self.transform:
            image = self.transform(image)

        return image, label, image_path

    def __len__(self) -> int:
        return len(self.df)

>>>> fishnet/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "gdown",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the FishNet dataset

Run with:

1. `python biobench/fishnet/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.fishnet.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import zipfile

import gdown
import requests
import tqdm
import tyro

dataset_url = "https://drive.google.com/uc?id=1mqLoap9QIVGYaPJ7T_KSBfLxJOg2yFY3"

labels_urls = [
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/train_full_meta_new.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/train.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/test.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/spec_gen_map.csv",
]


@dataclasses.dataclass()
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    images: bool = True
    """Whether to download the images zip file [5.4GB]."""
    labels: bool = True
    """Whether to download the labels."""
    extract: bool = True
    """Whether to extract the zip file."""


def main(args: Args):
    """Download FishNet."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    output_name = "fishnet.zip"
    zipfile_path = os.path.join(args.dir, output_name)

    # Download the zip file.
    if args.images:
        gdown.download(dataset_url, zipfile_path, quiet=False)
        print(f"Downloaded zip file: {zipfile_path}.")

    if args.labels:
        for labels_url in labels_urls:
            r = requests.get(labels_url, stream=True)
            r.raise_for_status()

            labels_path = os.path.join(args.dir, labels_url.split("/")[-1])
            with open(labels_path, "wb") as fd:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    fd.write(chunk)
            print(f"Downloaded labels: {labels_path}.")

    # Extract the zip file.
    if args.extract:
        with zipfile.ZipFile(zipfile_path, "r") as zip:
            for member in tqdm.tqdm(zip.infolist(), desc="Extracting images"):
                zip.extract(member, args.dir)
        print(f"Extracted images: {args.dir}/Image_Library.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> fungiclef/__init__.py
"""
FungiCLEF2023: classify fungal species using Danish Fungi preprocessed images.

Citations:
```
@inproceedings{BohemianVRA2023,
  title={FungiCLEF 2023 challenge evaluation},
  author={BohemianVRA},
  booktitle={ImageCLEF},
  year={2023}
}
```
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image

from .. import config, helpers, openset, registry, reporting, simpleshot
from .metrics import user_loss_score

logger = logging.getLogger("fungiclef")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]
    """image IDs for validation, observation IDs for training."""


@beartype.beartype
class FungiDataset(torch.utils.data.Dataset):
    def __init__(self, csv_fpath: str, img_dpath: str, transform):
        if not os.path.isdir(img_dpath):
            raise RuntimeError(f"Image directory not found: {img_dpath}")

        df = pl.read_csv(csv_fpath)
        self.img_names = df.get_column("image_path").to_numpy()
        self.labels = df.get_column("class_id").to_numpy().astype(int)
        self.obs_ids = df.get_column("observationID").to_numpy()

        self.img_dpath = img_dpath
        self.transform = transform

    def __len__(self) -> int:
        return len(self.img_names)

    def __getitem__(self, idx) -> dict[str, object]:
        img_name = self.img_names[idx]
        label = self.labels[idx].item()
        # try exact case, then lowercase
        p1 = os.path.join(self.img_dpath, img_name)
        if os.path.exists(p1):
            path = p1
        else:
            p2 = os.path.join(self.img_dpath, img_name.lower())
            if os.path.exists(p2):
                path = p2
                logger.debug("Using lowercase image path for %s", img_name)
            else:
                raise FileNotFoundError(
                    f"Image '{img_name}' not found in {self.img_dpath} (tried '{p1}', '{p2}')"
                )
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)

        return {
            "img_id": img_name,
            "img": img,
            "label": label,
            "obs_id": self.obs_ids[idx].item(),
        }


@beartype.beartype
def get_features(
    cfg: config.Experiment,
    backbone: registry.VisionBackbone,
    *,
    split: typing.Literal["train", "val"],
    pool: bool,
) -> Features:
    transform = backbone.make_img_transform()

    csv_fpath = os.path.join(
        cfg.data.fungiclef, f"FungiCLEF2023_{split}_metadata_PRODUCTION.csv"
    )

    img_dpath = os.path.join(
        cfg.data.fungiclef,
        "DF20_300" if split == "train" else "DF21_300",
    )
    dataset = FungiDataset(csv_fpath, img_dpath, transform)

    # subsample for train
    if split == "train" and cfg.n_train > 0:
        idxs = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        dataset = torch.utils.data.Subset(dataset, idxs)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.n_workers,
        pin_memory=True,
    )

    backbone = torch.compile(backbone.to(cfg.device))
    feats_list, labs_list, img_ids_list, obs_ids_list = [], [], [], []

    total = len(dataloader) if not cfg.debug else 2
    it = iter(dataloader)
    for b in helpers.progress(range(total), every=10, desc=f"fungi/{split}"):
        batch = next(it)
        imgs = batch["img"].to(cfg.device)
        with torch.amp.autocast("cuda"):
            out = backbone.img_encode(imgs).img_features.cpu().numpy()
        feats_list.append(out)
        labs_list.append(np.array(batch["label"], dtype=int))
        img_ids_list.extend(batch["img_id"])
        obs_ids_list.extend(batch["obs_id"])

    x = np.concatenate(feats_list, axis=0)
    y = np.concatenate(labs_list, axis=0)
    img_ids = np.array(img_ids_list)
    obs_ids = np.array(obs_ids_list)

    if pool:
        # for each unique obs take mean of its image features
        uniq, inv = np.unique(obs_ids, return_inverse=True)
        pooled = np.empty((len(uniq), x.shape[1]), dtype=x.dtype)
        for k, u in enumerate(helpers.progress(uniq, every=10_000, desc="groupby")):
            pooled[k] = x[inv == k].mean(axis=0)
        # labels should be identical within an observation
        pooled_y = np.array([y[inv == k][0] for k in range(len(uniq))], dtype=int)
        return Features(pooled, pooled_y, uniq)

    # image-level output
    return Features(x, y, img_ids)


@beartype.beartype
@torch.no_grad()
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)
    train_feats = get_features(cfg, backbone, split="train", pool=False)
    val_feats = get_features(cfg, backbone, split="val", pool=True)

    clf = init_clf(cfg)
    clf = openset.MahalanobisOpenSetClassifier(clf)
    clf.fit(train_feats.x, train_feats.y)

    preds = clf.predict(val_feats.x)

    # Identify train and test classes
    train_classes = set(np.unique(train_feats.y))

    examples = [
        reporting.Prediction(
            img_id,
            float(p == t),
            {"y_pred": int(p), "y_true": int(t), "ood": t not in train_classes},
        )
        for img_id, p, t in zip(val_feats.ids, preds, val_feats.y)
    ]
    return reporting.Report("fungiclef", examples, cfg)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    return openset.MahalanobisOpenSetClassifier(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            simpleshot.SimpleShotClassifier(device="cuda:0"),
        ),
    )


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    """
    Return the **User-Focused Loss** used in FungiCLEF:
        user_loss = classification_error + PSC/ESC cost

    Notes
    -----
    * `info['y_true']` and `info['y_pred']` are ints; unknown is -1.
    * The helper in metrics.py already combines CE and PSC/ESC.
    """
    y_true = np.fromiter((p.info["y_true"] for p in preds), dtype=int)
    y_pred = np.fromiter((p.info["y_pred"] for p in preds), dtype=int)
    return user_loss_score(y_true, y_pred)

>>>> fungiclef/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the FungiCLEF 2023 challenge dataset based on the Danish Fungi 2020 and 2021 preprocessed images and metadata.

Downloads:
 - Training images (max side size 300px; DF20) [~6.5GB]
 - Validation + Public Test images (max side size 300px; DF21) [~2.5GB]
 - Training, Validation, and Public Test metadata CSVs
"""

import dataclasses
import os
import tarfile

import requests
import tqdm
import tyro

URLS = {
    "train_imgs": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/DF20-300px.tar.gz",
    "val_imgs": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/DF21_300px.tar.gz",
    "train_metadata": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/FungiCLEF2023_train_metadata_PRODUCTION.csv",
    "val_metadata": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/FungiCLEF2023_val_metadata_PRODUCTION.csv",
}


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save downloaded archives and extract them."""
    chunk_size_kb: int = 1024
    """Download chunk size in KB."""
    download_train_imgs: bool = True
    """Whether to download training images (DF20) [~6.5GB]."""
    download_val_imgs: bool = True
    """Whether to download validation and public test images (DF21) [~2.5GB]."""
    download_train_metadata: bool = True
    """Whether to download training metadata CSV."""
    download_val_metadata: bool = True
    """Whether to download validation metadata CSV."""
    unzip: bool = True
    """Whether to extract downloaded archives."""


def download_file(name: str, url: str, dest_dir: str, chunk_size: int) -> str:
    os.makedirs(dest_dir, exist_ok=True)
    filename = os.path.basename(url)
    dest_path = os.path.join(dest_dir, filename)
    if os.path.exists(dest_path):
        print(f"{filename} already exists, skipping download")
        return dest_path
    response = requests.get(url, stream=True)
    response.raise_for_status()
    total = int(response.headers.get("content-length", 0))
    with (
        open(dest_path, "wb") as f,
        tqdm.tqdm(
            total=total, unit="B", unit_scale=True, desc=f"Downloading {filename}"
        ) as bar,
    ):
        for chunk in response.iter_content(chunk_size=chunk_size):
            f.write(chunk)
            bar.update(len(chunk))
    return dest_path


def extract_file(archive_path: str, dest_dir: str):
    with tarfile.open(archive_path, "r:gz") as tar:
        for member in tqdm.tqdm(
            tar, desc=f"Extracting {os.path.basename(archive_path)}"
        ):
            tar.extract(member, path=dest_dir)


def main(args: Args):
    base_dir = args.dir
    chunk_size = args.chunk_size_kb * 1024
    for key, url in URLS.items():
        flag = getattr(args, f"download_{key}")
        if not flag:
            continue
        path = download_file(key, url, base_dir, chunk_size)
        if args.unzip and key.endswith("_imgs"):
            extract_file(path, base_dir)
            print(f"Extracted {key} into {base_dir}")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> fungiclef/metrics.py
"""
FungiCLEF2023 custom metrics implementation.
Adapted from BohemianVRA's evaluate.py:
https://github.com/BohemianVRA/FGVC-Competitions/blob/main/FungiCLEF2023/evaluate.py
"""

import beartype
import numpy as np
import polars as pl
import sklearn.metrics

# Load poison status via Polars
_poison_df = pl.read_csv(
    "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/poison_status_list.csv"
)
POISONOUS_SPECIES = (
    _poison_df.filter(pl.col("poisonous") == 1)
    .select("class_id")
    .unique()
    .to_series()
    .to_numpy()
)


@beartype.beartype
def classification_error_with_unknown(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    cost_unknown_mis: float = 10.0,
    cost_mis_as_unknown: float = 0.1,
) -> float:
    """
    Classification error allowing for an "unknown" class (encoded as -1).

    Args:
        y_true: ground-truth labels (with -1 for unknown)
        y_pred: predicted labels (with -1 for unknown)
        cost_unknown_mis: cost of misclassifying a true unknown as known
        cost_mis_as_unknown: cost of misclassifying a true known as unknown

    Returns:
        normalized error rate
    """
    is_true_unknown = y_true == -1
    is_pred_unknown = y_pred == -1

    num_mis_unknown = np.sum(is_true_unknown & ~is_pred_unknown)
    num_mis_as_unknown = np.sum(~is_true_unknown & is_pred_unknown)
    num_other = np.sum((y_true != y_pred) & ~is_true_unknown & ~is_pred_unknown)
    total = y_true.size
    return (
        num_other
        + cost_unknown_mis * num_mis_unknown
        + cost_mis_as_unknown * num_mis_as_unknown
    ) / total


@beartype.beartype
def classification_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Standard classification error (unknown treated as error equally).
    """
    return classification_error_with_unknown(
        y_true, y_pred, cost_unknown_mis=1.0, cost_mis_as_unknown=1.0
    )


@beartype.beartype
def num_psc_decisions(y_true: np.ndarray, y_pred: np.ndarray) -> int:
    """
    Number of poisonous species incorrectly predicted as non-poisonous.
    """
    is_true_poison = np.isin(y_true, POISONOUS_SPECIES)
    is_pred_poison = np.isin(y_pred, POISONOUS_SPECIES)
    return int(np.sum(is_true_poison & ~is_pred_poison))


@beartype.beartype
def num_esc_decisions(y_true: np.ndarray, y_pred: np.ndarray) -> int:
    """
    Number of non-poisonous species incorrectly predicted as poisonous.
    """
    is_true_poison = np.isin(y_true, POISONOUS_SPECIES)
    is_pred_poison = np.isin(y_pred, POISONOUS_SPECIES)
    return int(np.sum(~is_true_poison & is_pred_poison))


@beartype.beartype
def psc_esc_cost_score(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    cost_psc: float = 100.0,
    cost_esc: float = 1.0,
) -> float:
    """
    Weighted cost for poisonousness confusion per sample.
    """
    total = y_true.size
    psc = num_psc_decisions(y_true, y_pred)
    esc = num_esc_decisions(y_true, y_pred)
    return (cost_psc * psc + cost_esc * esc) / total


@beartype.beartype
def user_loss_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    ce = classification_error_with_unknown(y_true, y_pred)
    psc = psc_esc_cost_score(y_true, y_pred)
    return ce + psc


@beartype.beartype
def evaluate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict[str, float]:
    """
    Compute all four FungiCLEF custom metrics plus macro F1.

    Returns:
        dict with keys:
            - F1_macro
            - Classification_Error
            - PSC_ESC_Cost
            - User_Focused_Loss
            - Classification_Error_with_Unknown
    """
    f1 = sklearn.metrics.f1_score(y_true, y_pred, average="macro") * 100.0
    ce = classification_error(y_true, y_pred)
    psc = psc_esc_cost_score(y_true, y_pred)
    ce_unk = classification_error_with_unknown(y_true, y_pred)
    user_loss = ce_unk + psc
    return {
        "f1_macro": f1,
        "ce": ce,
        "psc_esc_cost": psc,
        "user_loss": user_loss,
        "ce_unk": ce_unk,
    }

>>>> fungiclef/test_metrics.py
import numpy as np
import pytest
from hypothesis import given
from hypothesis import strategies as st

from . import metrics


# Override poisonous species for deterministic testing
@pytest.fixture(autouse=True)
def stub_poison(monkeypatch):
    monkeypatch.setattr(metrics, "POISONOUS_SPECIES", np.array([1, 3]), raising=True)


def test_classification_error_all_correct():
    y = np.array([0, 1, 2, 3])
    y_pred = y.copy()
    assert metrics.classification_error(y, y_pred) == 0.0
    # with unknown costs both = 1 => same
    assert metrics.classification_error_with_unknown(y, y_pred) == 0.0


def test_classification_error_all_wrong():
    y = np.array([0, 1, 2, 3])
    y_pred = np.array([4, 5, 6, 7])
    # all mismatches, cost_unknown_mis=1, cost_mis_as_unknown=1
    assert metrics.classification_error(y, y_pred) == 1.0
    # no unknowns => same
    assert metrics.classification_error_with_unknown(y, y_pred) == 1.0


def test_classification_error_with_unknown_edge():
    y_true = np.array([-1, 10])
    y_pred = np.array([5, -1])
    # one true unknown mispredicted (cost 10), one true known predicted unknown (cost 0.1)
    ce_unk = metrics.classification_error_with_unknown(
        y_true, y_pred, cost_unknown_mis=10.0, cost_mis_as_unknown=0.1
    )
    # (10 + 0.1) / 2 = 5.05
    assert pytest.approx(ce_unk, abs=1e-6) == 5.05
    # classification_error uses cost=1,1 => (1 + 1)/2 = 1
    assert metrics.classification_error(y_true, y_pred) == 1.0


def test_num_psc_decisions_and_num_esc_decisions():
    # with stubbed POISONOUS_SPECIES = [1,3]
    y_true = np.array([1, 2, 3, 4])
    y_pred = np.array([2, 3, 4, 1])
    # true poison at positions 0 and 2; pred poison when pred in [1,3]
    # pos 0: true poison, pred=2 not poison => PSC
    # pos 2: true poison, pred=4 not poison => PSC
    assert metrics.num_psc_decisions(y_true, y_pred) == 2
    # ESC: non-poison predicted as poison
    # pos1: true=2 not poison, pred=3 poison => ESC
    # pos3: true=4 not poison, pred=1 poison => ESC
    assert metrics.num_esc_decisions(y_true, y_pred) == 2


def test_psc_esc_cost_score_default_costs():
    y_true = np.array([1, 2, 3, 4])
    y_pred = np.array([2, 3, 4, 1])
    # PSC=2, ESC=2, cost_psc=100, cost_esc=1 => (200 + 2)/4 = 50.5
    cost = metrics.psc_esc_cost_score(y_true, y_pred)
    assert pytest.approx(cost, abs=1e-6) == 50.5


def test_evaluate_metrics_basic():
    # simple balanced case
    y_true = np.array([0, 1, 2, 3])
    y_pred = y_true.copy()
    res = metrics.evaluate_metrics(y_true, y_pred)
    assert res["f1_macro"] == 100.0
    assert res["ce"] == 0.0
    assert res["psc_esc_cost"] == 0.0
    assert res["user_loss"] == 0.0
    assert res["ce_unk"] == 0.0


def test_all_correct():
    y = np.arange(5)
    y_pred = y.copy()
    assert metrics.classification_error(y, y_pred) == 0.0


def test_all_wrong():
    y = np.arange(5)
    y_pred = np.arange(5) + 10
    assert metrics.classification_error(y, y_pred) == 1.0


@given(
    y_true=st.lists(st.integers(min_value=-1, max_value=10), min_size=1, max_size=20),
    y_pred=st.lists(st.integers(min_value=-1, max_value=10), min_size=1, max_size=20),
)
def test_fuzz_raises_or_returns(y_true, y_pred):
    y_t = np.array(y_true)
    y_p = np.array(y_pred)
    try:
        out = metrics.evaluate_metrics(y_t, y_p)
        # Expect dict keys and numeric values
        assert isinstance(out, dict)
        for k, v in out.items():
            assert isinstance(k, str)
            assert isinstance(v, float)
    except ValueError:
        # mismatch lengths should raise
        assert y_t.shape != y_p.shape

>>>> helpers.py
"""
Useful helpers for more than two tasks that don't fit anywhere else.
"""

import collections
import collections.abc
import contextlib
import dataclasses
import gc
import itertools
import logging
import os
import os.path
import pathlib
import resource
import subprocess
import sys
import time
import warnings

import beartype
import numpy as np
import torch
from jaxtyping import Int, jaxtyped


@beartype.beartype
def get_cache_dir() -> str:
    cache_dir = ""
    for var in ("BIOBENCH_CACHE", "HF_HOME", "HF_HUB_CACHE"):
        cache_dir = cache_dir or os.environ.get(var, "")
    return cache_dir or "."


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress"):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)

    def __iter__(self):
        start = time.time()
        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if isinstance(self.it, collections.abc.Sized):
                    pred_min = (len(self) - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        len(self),
                        (i + 1) / len(self) * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        return len(self.it)


@beartype.beartype
def fs_safe(string: str) -> str:
    """Makes a string safe for filesystems by removing typical special characters."""
    return string.replace(":", "_").replace("/", "_")


@beartype.beartype
def write_hparam_sweep_plot(
    task: str,
    model: str,
    clf,
    x: str = "param_ridgeclassifier__alpha",
    y: str = "mean_test_score",
) -> str:
    import matplotlib.pyplot as plt
    import polars as pl

    if not hasattr(clf, "cv_results_"):
        return ""

    df = pl.DataFrame(clf.cv_results_)

    fig, ax = plt.subplots()

    if "n_resources" in df.columns:
        for n_resources in df.get_column("n_resources").unique().sort():
            ax.scatter(
                x=df.filter(pl.col("n_resources") == n_resources)[x],
                y=df.filter(pl.col("n_resources") == n_resources)[y],
                label=f"{n_resources} ex.",
            )
        fig.legend()
    else:
        ax.scatter(x=df[x], y=df[y])

    ax.set_xlabel(x)
    ax.set_ylabel(y)
    ax.set_xscale("log")
    ax.set_title(model)

    fig.tight_layout()
    filepath = os.path.join("logs", f"{task}_{fs_safe(model)}_hparam.png")
    fig.savefig(filepath)
    return filepath


@jaxtyped(typechecker=beartype.beartype)
def balanced_random_sample(
    labels: Int[np.ndarray, " n_labels"], n: int
) -> Int[np.ndarray, " n"]:
    """
    Select n random examples while balancing the number of examples per class.
    """
    # Count the occurrences of each class
    class_counts = collections.Counter(labels)
    unique_classes = list(class_counts.keys())
    n_classes = len(unique_classes)

    if not n_classes:
        return np.array([], dtype=int)

    # Calculate ideal number of samples per class
    samples_per_class = n // n_classes

    # Handle remainder by allocating extra samples to random classes
    remainder = n % n_classes
    extra_samples = np.zeros(n_classes, dtype=int)
    if remainder > 0:
        extra_indices = np.random.choice(n_classes, remainder, replace=False)
        extra_samples[extra_indices] = 1

    # Calculate final samples per class
    final_samples = np.array([samples_per_class] * n_classes) + extra_samples

    # Initialize result array
    selected_indices = []

    # For each class, select random samples
    for i, class_label in enumerate(unique_classes):
        # Get all indices for this class
        class_indices = np.where(labels == class_label)[0]

        # Calculate how many to take (minimum of available samples and desired samples)
        n_to_take = min(len(class_indices), final_samples[i])

        # Randomly sample without replacement
        if n_to_take > 0:
            sampled_indices = np.random.choice(class_indices, n_to_take, replace=False)
            selected_indices.extend(sampled_indices)

    # If we still don't have enough samples (due to some classes having too few examples),
    # sample from the remaining examples across all classes
    if len(selected_indices) < n:
        # Create a mask of already selected indices
        mask = np.ones(len(labels), dtype=bool)
        mask[selected_indices] = False
        remaining_indices = np.where(mask)[0]

        # How many more do we need?
        needed = n - len(selected_indices)

        # Sample without replacement from remaining indices
        if needed > 0 and len(remaining_indices) > 0:
            additional_indices = np.random.choice(
                remaining_indices, min(needed, len(remaining_indices)), replace=False
            )
            selected_indices.extend(additional_indices)

    return np.array(selected_indices, dtype=int)


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@beartype.beartype
def bump_nofile(margin: int = 512) -> None:
    """
    Make RLIMIT_NOFILE.soft = RLIMIT_NOFILE.hard - margin (if that is higher than the current soft limit).  No change if margin would push soft < 1. Raises RuntimeError if hard <= margin.
    """
    if margin < 0:
        raise ValueError("margin must be non-negative")

    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)

    if hard <= margin:
        raise RuntimeError(
            f"hard limit ({hard}) is <= margin ({margin}); ask an admin to raise the hard limit."
        )

    target_soft = hard - margin
    if soft < target_soft:
        resource.setrlimit(resource.RLIMIT_NOFILE, (target_soft, hard))


@beartype.beartype
def _default_batchsize_schedule(start: int = 2) -> collections.abc.Iterable[int]:
    """
    2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 196, 256, 384, 512, 768, 1024, ...
    """

    x = start
    for m in itertools.cycle((3 / 2, 4 / 3)):  # 3/2, 4/3, 3/2, 4/3, ...
        yield int(x)
        x *= m


@beartype.beartype
def infer_batch_size(batch: object) -> int | None:
    """
    Return the leading dimension of the *first* tensor found inside `batch`. Works for arbitrary nested structures.
    """
    if isinstance(batch, torch.Tensor):
        return batch.shape[0]

    if isinstance(batch, (list, tuple)):
        for item in batch:
            bs = infer_batch_size(item)
            if bs is not None:
                return bs

    if isinstance(batch, dict):
        for item in batch.values():
            bs = infer_batch_size(item)
            if bs is not None:
                return bs

    if dataclasses.is_dataclass(batch):
        return infer_batch_size(dataclasses.asdict(batch))

    # Fallback: inspect attributes (namedtuple, SimpleNamespace, custom)
    if hasattr(batch, "__dict__"):
        return infer_batch_size(vars(batch))

    return None


@contextlib.contextmanager
@beartype.beartype
def auto_batch_size(
    dataloader: torch.utils.data.DataLoader,
    *,
    probe: collections.abc.Callable[[torch.Tensor], torch.Tensor],
    schedule: collections.abc.Iterable[int] | None = None,
    upper: int = 4096,
    backoff: int = 0,
):
    """Context manager that mutates `dataloader.batch_size` in-place to use the largest batch that fits GPU RAM.

    This function tests progressively larger batch sizes until it finds the maximum that can be processed without running out of memory.

    Args:
        dataloader: The already constructed loader you use in your loop. Its `batch_sampler.batch_size` attribute is patched on the fly.
        probe: A 1-argument callable used to test memory. Typical usage: `lambda x: backbone.img_encode(x).img_features`.
        schedule: An iterator of candidate batch sizes. If None, use the canonical schedule.
        schedule: An iterator of strictly increasing candidate batch sizes (2, 4, 8, ...). A *ValueError* is raised when a non-increasing value is encountered. If None, use the canonical schedule.
        upper: Maximum batch size to try, regardless of available memory.
        backoff: int, default = 0. How far to step **back** in the candidate schedule from the largest batch-size that completes without OOM  (clamped to the smallest candidate if ``n`` is too big).
        * `backoff = 0`  -> use the **largest** successful size
        * `backoff = 1`  -> use the **second-largest** successful size
        * `backoff = n`  -> use the *n*-th size below the largest success

    Yields:
        int: The selected batch size.
    """

    @beartype.beartype
    class OrderedSet[T]:
        def __init__(self):
            self._lst = []

        def __bool__(self) -> bool:
            return bool(self._lst)

        @property
        def last(self) -> T:
            return self._lst[-1]

        def append(self, t: T) -> None:
            if t in self._lst:
                return

            self._lst.append(t)

        def pop(self) -> T:
            return self._lst.pop()

        def __repr__(self) -> str:
            return f"{self.__class__.__name__}({self._items!r})"

        def __str__(self) -> str:
            return str(self._items)

    if dataloader.batch_sampler is None:
        raise ValueError("dataloader must have a batch_sampler")

    if backoff < 0:
        raise ValueError(f"backoff '{backoff}' < 0; must be >= 0")

    logger = logging.getLogger("auto-bsz")

    oom_signatures = (
        "out of memory",
        "cuda error: invalid configuration argument",  # SPM-efficient-attn OOM
        "expected canuse32bitindexmath(input) && canuse32bitindexmath(output) to be true, but got false.",  # Conv layers with big batch sizes.
    )

    dataloader.batch_sampler.batch_size = min(
        dataloader.batch_sampler.batch_size, upper
    )

    orig_bsz = int(dataloader.batch_sampler.batch_size)
    good_bszs = OrderedSet()
    schedule_iter = schedule or _default_batchsize_schedule(orig_bsz)

    torch.cuda.empty_cache()  # be nice

    t_start = time.perf_counter()

    for tried_bsz in schedule_iter:
        if good_bszs and tried_bsz <= good_bszs.last:
            raise ValueError(
                f"Schedule not monotonically increasing: {tried_bsz} <= {good_bszs.last}"
            )

        if tried_bsz > upper:
            tried_bsz = upper

        # patch sampler attr
        dataloader.batch_sampler.batch_size = tried_bsz
        logger.info("Trying batch_size=%d", tried_bsz)

        # pull ONE mini-batch, send through probe
        try:
            batch = next(iter(dataloader))
            probe(batch)  # forward only; discard output

            # If the loader produced fewer items than we asked for, we've reached the dataset size -- any larger batch will give the same tensor, so stop growing.
            effective_bsz = infer_batch_size(batch)
            if effective_bsz is None:
                raise RuntimeError(
                    "Unable to deduce batch size from probe batch; ensure it contains at least one torch.Tensor."
                )

            if effective_bsz < tried_bsz:
                logger.info(
                    "Dataset exhausted at %d examples (asked for %d); capping batch size.",
                    effective_bsz,
                    tried_bsz,
                )
                ok_bsz = effective_bsz
                good_bszs.append(ok_bsz)
                break

            ok_bsz = tried_bsz
            good_bszs.append(ok_bsz)
            logger.info("batch_size=%d succeeded", ok_bsz)

            # honor explicit ceiling
            if upper is not None and ok_bsz >= upper:
                logger.info("Reached ok_bsz (%d) >= upper (%d)", ok_bsz, upper)
                ok_bsz = upper
                break

        except RuntimeError as err:
            msg = str(err).lower()
            if any(sig in msg for sig in oom_signatures):
                logger.info("OOM at batch_size=%d; reverting to %d", tried_bsz, ok_bsz)
                torch.cuda.empty_cache()
                break
            else:
                raise

    # (re-)verify ok_bs once more in a clean context
    while good_bszs:
        ok_bsz = good_bszs.pop()
        dataloader.batch_sampler.batch_size = ok_bsz
        try:
            batch = next(iter(dataloader))
            probe(batch)
            break  # we know ok_bsz is actually good.

        except RuntimeError as err:
            if any(sig in str(err).lower() for sig in oom_signatures):
                logger.info("Still OOM at %d; trying previous candidate", ok_bsz)
            else:
                raise

    while good_bszs and backoff:
        backoff -= 1
        ok_bsz = good_bszs.pop()
        dataloader.batch_sampler.batch_size = ok_bsz

    elapsed = time.perf_counter() - t_start
    logger.info("Selected batch_size %d after %.2f s", ok_bsz, elapsed)

    # Final tidy-up to avoid residual OOMs
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()  # frees cached blocks from other procs
    gc.collect()  # clears Python refs / fragments

    try:
        yield ok_bsz  # user code runs here with patched batch_size
    finally:
        # always restore original value
        dataloader.batch_sampler.batch_size = orig_bsz


NFS_TYPES = {"nfs", "nfs4", "nfsd", "autofs"}  # extend if you wish


@beartype.beartype
def warn_if_nfs(path: str | os.PathLike):
    """
    If *path* is on an NFS mount, emit a RuntimeWarning.

    Works on Linux (/proc/mounts) and macOS/BSD (`mount` CLI); silently returns on other OSes or if detection fails.
    """
    p = pathlib.Path(path).resolve()

    # Linux: /proc/self/mountinfo
    if sys.platform.startswith("linux"):
        try:
            with open("/proc/self/mountinfo") as fd:
                entries = [line.split() for line in fd]
            # fields: 4= mount point, - separator -, fstype
            mounts = {fields[4]: fields[-3] for fields in entries}
        except Exception:
            return

    # macOS / BSD: `mount`
    elif sys.platform in {"darwin", "freebsd"}:
        try:
            out = subprocess.check_output(["mount", "-p"], text=True)
            mounts = {}
            for line in out.splitlines():
                mp, _dev, fstype, *_ = line.split()  # mount-point, ...
                mounts[mp] = fstype
        except Exception:
            return
    else:
        return  # unsupported OS

    # find longest mount-point prefix of *p*
    mount_point = max(
        (mp for mp in mounts if p.is_relative_to(mp) or mp == "/"),
        key=len,
        default="/",
    )
    if mounts.get(mount_point) in NFS_TYPES:
        warnings.warn(
            f"SQLite database '{path}' appears to be on an NFS mount (fs type: {mounts[mount_point]}). Concurrent writers over NFS can corrupt the journal; consider using a local SSD or tmpfs instead.",
            RuntimeWarning,
            stacklevel=2,
        )

>>>> herbarium19/__init__.py
"""
Herbarium19: classify specimens into species based on the 2019 FGVC6 competition.

```
@article{tan2019herbarium,
  title={The herbarium challenge 2019 dataset},
  author={Tan, Kiat Chuan and Liu, Yulong and Ambrose, Barbara and Tulig, Melissa and Belongie, Serge},
  journal={arXiv preprint arXiv:1906.05372},
  year={2019}
}
```
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
import torchvision.datasets
from jaxtyping import Float, Int, Shaped, jaxtyped
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("herbarium19")


@jaxtyped(typechecker=beartype.beartype)
class Sample(typing.TypedDict):
    """A dictionary representing a single image sample with its metadata.

    Attributes:
        img_id: Unique identifier for the image.
        img: The image tensor with shape [3, width, height] (RGB channels first).
        label: Binary class label (0 or 1) for the image.
    """

    img_id: str
    img: Float[Tensor, "3 width height"]
    label: Int[Tensor, ""]


@beartype.beartype
class Dataset(torchvision.datasets.ImageFolder):
    """ImageFolder but returns Sample."""

    def __getitem__(self, index) -> Sample:
        path, label = self.samples[index]
        img = self.loader(path)
        if self.transform:
            img = self.transform(img)
        return {"img_id": path, "img": img, "label": label}

    @property
    def labels(self) -> Int[np.ndarray, " n"]:
        return np.array([label for _, label in self.samples])


@beartype.beartype
@torch.no_grad()
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)

    train_feats = get_features(cfg, backbone, is_train=True)
    test_feats = get_features(cfg, backbone, is_train=False)

    clf = init_clf(cfg)
    clf.fit(train_feats.x, train_feats.y)

    if hasattr(clf, "best_params_"):
        helpers.write_hparam_sweep_plot("herbarium19", cfg.model.ckpt, clf)

    preds = clf.predict(test_feats.x)
    examples = [
        reporting.Prediction(
            img_id, float(p == t), {"y_pred": p.item(), "y_true": t.item()}
        )
        for img_id, p, t in zip(test_feats.ids, preds, test_feats.y)
    ]
    return reporting.Report("herbarium19", examples, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    split = "train" if is_train else "validation"
    images_dir_path = os.path.join(cfg.data.herbarium19, split)

    img_transform = backbone.make_img_transform()
    dataset = Dataset(images_dir_path, img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    backbone = torch.compile(backbone.to(cfg.device))

    def probe(batch):
        imgs = batch["img"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features  # forward only

    all_ids, all_features, all_labels = [], [], []
    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"hb19/{split}"):
            batch = next(it)
            imgs = batch["img"].to(cfg.device)

            with torch.amp.autocast("cuda"):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_ids.extend(batch["img_id"])
            all_labels.extend(batch["label"])

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)
    assert len(all_ids) == len(dataset)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    if 0 < cfg.n_train <= 300:
        return sklearn.linear_model.RidgeClassifier()

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        factor=3,
        random_state=cfg.seed,
        scoring="f1_macro",
    )

>>>> herbarium19/download.py
# biobench/herbarium19/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the Herbarium 2019 FGVC6 dataset directly from storage.googleapis.com.

Splits:
 - train (34,225 images; 38 GB)
 - validation (2,679 images; 3 GB)
 - test (9,565 images; 11 GB)

Verifies MD5 checksums before extraction.
"""

import dataclasses
import hashlib
import os
import tarfile

import requests
import tqdm
import tyro

URLS = {
    "train": {
        "url": "https://storage.googleapis.com/nybg/herbarium-2019-fgvc6/train.tar.gz",
        "md5": "53c6b9ee2f831f5101dbe00958091dc8",
    },
    "validation": {
        "url": "https://storage.googleapis.com/nybg/herbarium-2019-fgvc6/validation.tar.gz",
        "md5": "2f854d580949e54f114993a74adc3d4b",
    },
    "test": {
        "url": "https://storage.googleapis.com/nybg/herbarium-2019-fgvc6/test.tar.gz",
        "md5": "297648fb76eed1b1c6f0ca1fd8188de0",
    },
}


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """Where to save the downloaded archives and extract them."""
    chunk_size_kb: int = 1024
    """Download chunk size in KB."""
    download_train: bool = True
    download_validation: bool = True
    download_test: bool = True
    unzip: bool = True


def md5_of_file(path: str, chunk_size: int = 8192) -> str:
    h = hashlib.md5()
    file_size = os.path.getsize(path)
    with open(path, "rb") as f:
        desc = f"MD5 for {os.path.basename(path)}"
        with tqdm.tqdm(total=file_size, unit="B", unit_scale=True, desc=desc) as bar:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                h.update(chunk)
                bar.update(len(chunk))
    return h.hexdigest()


def download_split(name: str, dest_dir: str, chunk_size: int) -> str:
    info = URLS[name]
    url, expected_md5 = info["url"], info["md5"]
    os.makedirs(dest_dir, exist_ok=True)
    archive_path = os.path.join(dest_dir, f"{name}.tar.gz")

    # Skip download if present and checksum matches
    if os.path.exists(archive_path):
        if md5_of_file(archive_path) == expected_md5:
            print(f"{name}.tar.gz already exists and MD5 matches; skipping download")
            return archive_path
        else:
            print(f"MD5 mismatch for existing {name}.tar.gz; re-downloading")

    # Stream-download
    r = requests.get(url, stream=True)
    r.raise_for_status()
    total = int(r.headers.get("content-length", 0))
    with (
        open(archive_path, "wb") as f,
        tqdm.tqdm(
            total=total, unit="B", unit_scale=True, desc=f"download {name}"
        ) as bar,
    ):
        for chunk in r.iter_content(chunk_size=chunk_size):
            f.write(chunk)
            bar.update(len(chunk))

    # verify
    actual_md5 = md5_of_file(archive_path)
    if actual_md5 != expected_md5:
        raise ValueError(
            f"MD5 mismatch for {name}.tar.gz: expected {expected_md5}, got {actual_md5}"
        )

    return archive_path


def extract_split(archive_path: str, dest_dir: str):
    with tarfile.open(archive_path, "r:gz") as tar:
        for m in tqdm.tqdm(tar, desc=f"Extracting {os.path.basename(archive_path)}"):
            tar.extract(m, path=dest_dir)


def main(args: Args):
    splits = {
        "train": args.download_train,
        "validation": args.download_validation,
        "test": args.download_test,
    }
    for name, do_dl in splits.items():
        if not do_dl:
            continue
        archive = download_split(name, args.dir, args.chunk_size_kb * 1024)
        if args.unzip:
            extract_split(archive, args.dir)
            print(f"Extracted `{name}` into {args.dir}/{name}/")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> imagenet1k/__init__.py
""" """

import dataclasses
import logging
import math
import warnings

import beartype
import datasets
import datasets.formatting.torch_formatter
import numpy as np
import sklearn.experimental.enable_halving_search_cv
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Float16, Int, Shaped, jaxtyped

from biobench import config, helpers, registry, reporting

logger = logging.getLogger("imagenet1k")

warnings.filterwarnings(
    "ignore",
    message="To copy construct from a tensor",
    category=UserWarning,
    module=datasets.formatting.torch_formatter.__name__,
)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float16[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)
    test_features = get_features(cfg, backbone, is_train=False)
    train_features = get_features(cfg, backbone, is_train=True)

    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)
    logger.info("Trained a classifier on %d examples.", len(train_features.y))

    if hasattr(clf, "best_params_"):
        helpers.write_hparam_sweep_plot("imagenet1k", cfg.model.ckpt, clf)
        alpha = clf.best_params_["ridgeclassifier__alpha"].item()
        logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("imagenet1k", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.micro_acc(preds)


class Transform:
    def __init__(self, img_transform):
        self._img_transform = img_transform

    def __call__(self, example):
        example["image"] = example["image"].convert("RGB")
        example["image"] = self._img_transform(example["image"])
        return example


@beartype.beartype
@torch.no_grad
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))
    split = "train" if is_train else "validation"

    dataset = datasets.load_dataset(
        "ILSVRC/imagenet-1k", split=split, cache_dir=helpers.get_cache_dir()
    )

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(np.array(dataset["label"]), cfg.n_train)
        assert len(i) == cfg.n_train
    else:
        i = np.arange(dataset.num_rows)

    n_workers = min(len(i), cfg.n_workers)

    # Map
    dataset = (
        dataset.map(lambda ex, idx: {"id": str(idx)}, with_indices=True)
        .select(i)
        .to_iterable_dataset(num_shards=n_workers)
        .map(Transform(img_transform))
        .with_format("torch")
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.batch_size,
        num_workers=n_workers,
        drop_last=False,
        shuffle=False,
    )

    all_features, all_labels, all_ids = [], [], []

    def probe(batch):
        imgs = batch["image"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(dataloader, probe=probe) as batch_size:
        total = max(n_workers, math.ceil(len(i) / batch_size))
        it = iter(dataloader)
        logger.debug("Need to embed %d batches of %d images.", total, batch_size)
        for b in helpers.progress(range(total), every=10, desc=f"in1k/{split}"):
            batch = next(it)

            images = batch["image"].to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(images).img_features

            all_features.append(features.cpu())
            all_labels.extend(batch["label"])
            all_ids.extend(batch["id"])

    all_features = torch.cat(all_features, dim=0).cpu().to(torch.float16).numpy()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels).numpy()
    assert len(all_ids) == len(i) or cfg.n_train < 0
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    clf = sklearn.pipeline.make_pipeline(
        sklearn.preprocessing.StandardScaler(),
        sklearn.linear_model.RidgeClassifier(),
    )
    if 0 < cfg.n_train < 2_000:
        return clf

    return sklearn.model_selection.HalvingGridSearchCV(
        clf,
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        factor=3,
        random_state=cfg.seed,
    )

>>>> inat21/__init__.py
"""
Trains a simple ridge regression classifier on visual representations for the iNat21 challenge.
In the challenge, there are 10K different species (classes).
We use the mini training set with 50 images per species, and test on the validation set, which has 10 images per species.

This task is a benchmark: it should help you understand how general a vision backbone's representations are.
This is not a true, real-world task.

If you use this task, be sure to cite the original iNat21 dataset paper:

```
@misc{inat2021,
  author={Van Horn, Grant and Mac Aodha, Oisin},
  title={iNat Challenge 2021 - FGVC8},
  publisher={Kaggle},
  year={2021},
  url={https://kaggle.com/competitions/inaturalist-2021}
}
```
"""

import dataclasses
import logging
import os

import beartype
import numpy as np
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
import torchvision.datasets
from jaxtyping import Float16, Int, Shaped, jaxtyped

from biobench import config, helpers, registry, reporting

logger = logging.getLogger("inat21")

n_classes = 10_000


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float16[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using validation data.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Get features
    val_features = get_features(cfg, backbone, is_train=False)
    train_features = get_features(cfg, backbone, is_train=True)

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    if hasattr(clf, "best_params_"):
        helpers.write_hparam_sweep_plot("imagenet1k", cfg.model.ckpt, clf)
        alpha = clf.best_params_["ridgeclassifier__alpha"].item()
        logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = val_features.y
    pred_labels = clf.predict(val_features.x)

    preds = [
        reporting.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(val_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("inat21", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.micro_acc(preds)


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torchvision.datasets.ImageFolder):
    """
    Subclasses ImageFolder so that `__getitem__` includes the path, which we use as the ID.
    """

    def __getitem__(self, index: int) -> tuple[str, object, object]:
        """
        Args:
            index (int): Index

        Returns:
            tuple: (path, sample, target) where target is class_index of the target class.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return path, sample, target

    @property
    def labels(self) -> Int[np.ndarray, " n"]:
        return np.array([label for _, label in self.samples])


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))
    split = "train_mini" if is_train else "val"

    root = os.path.join(cfg.data.inat21, split)

    if not os.path.isdir(root):
        msg = f"Path '{root}' doesn't exist. Did you download the iNat21 dataset?"
        raise ValueError(msg)

    dataset = Dataset(root, img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=True,
    )

    all_ids, all_features, all_labels = [], [], []

    def probe(batch):
        ids, imgs, labels = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc=f"inat21/{split}"):
            ids, images, labels = next(it)
            images = images.to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(images).img_features

            all_features.append(features.cpu())
            all_labels.extend(labels)
            all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu().to(torch.float16).numpy()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels).numpy()
    if is_train and cfg.n_train > 0:
        assert len(all_ids) == cfg.n_train

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    clf = sklearn.pipeline.make_pipeline(
        sklearn.preprocessing.StandardScaler(),
        sklearn.linear_model.RidgeClassifier(),
    )
    if 0 < cfg.n_train < 20_000:
        clf

    return sklearn.model_selection.HalvingGridSearchCV(
        clf,
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        factor=3,
    )

>>>> inat21/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the iNat21 (mini) dataset.

Run with:

1. `python biobench/inat21/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.inat21.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import tarfile

import beartype
import requests
import tqdm
import tyro

val_images_url = "https://ml-inat-competition-datasets.s3.amazonaws.com/2021/val.tar.gz"
train_mini_images_url = (
    "https://ml-inat-competition-datasets.s3.amazonaws.com/2021/train_mini.tar.gz"
)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    val: bool = True
    """Whether to download validation images [8.4GB]."""
    train: bool = True
    """Whether to download (mini) train images [42GB]."""


@beartype.beartype
def download_tar(url: str, tar_path: str, chunk_size: int):
    r = requests.get(url, stream=True)
    r.raise_for_status()

    n_bytes = int(r.headers["content-length"])

    with open(tar_path, "wb") as fd:
        for chunk in tqdm.tqdm(
            r.iter_content(chunk_size=chunk_size),
            total=n_bytes / chunk_size,
            unit="b",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading",
        ):
            fd.write(chunk)


def extract_tar(tar_path: str, n_images: int, dir: str):
    with tarfile.open(tar_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=n_images + 10_000):
            tar.extract(member, path=dir, filter="data")


@beartype.beartype
def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    train_tar_path = os.path.join(args.dir, "train_mini.tar.gz")
    val_tar_path = os.path.join(args.dir, "val.tar.gz")

    if args.val:
        download_tar(val_images_url, val_tar_path, chunk_size)
        print(f"Downloaded validation images: {val_tar_path}.")

    extract_tar(val_tar_path, 100_000, args.dir)
    print("Extracted validation images.")

    if args.train:
        download_tar(train_mini_images_url, train_tar_path, chunk_size)
        print(f"Downloaded train (mini) images: {train_tar_path}.")

    extract_tar(train_tar_path, 500_000, args.dir)
    print("Extracted training images.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> iwildcam/__init__.py
"""
Fits a linear classifier that is trained using cross-entropy on the training set of iWildCam 2020.


"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
import wilds
import wilds.common.data_loaders
from jaxtyping import Float, Int, Shaped, jaxtyped

from biobench import config, helpers, registry, reporting

logger = logging.getLogger("iwildcam")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Load dataloaders.
    test_features = get_features(cfg, backbone, is_train=False)
    logger.info("Got test features.")

    train_features = get_features(cfg, backbone, is_train=True)
    logger.info("Got train features.")

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    if hasattr(clf, "best_params_"):
        helpers.write_hparam_sweep_plot("iwildcam", cfg.model.ckpt, clf)
        alpha = clf.best_params_["ridgeclassifier__alpha"].item()
        logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("iwildcam", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    if not os.path.exists(cfg.data.iwildcam) or not os.path.isdir(cfg.data.iwildcam):
        msg = f"Path '{cfg.data.iwildcam}' doesn't exist. Did you download the iWildCam dataset? See the docstring at the top of this file for instructions."
        raise RuntimeError(msg)

    dataset = wilds.get_dataset(
        dataset="iwildcam", download=False, root_dir=cfg.data.iwildcam
    )

    transform = backbone.make_img_transform()
    if is_train:
        subset = "train"
        dataset = dataset.get_subset(subset, transform=transform)
        if cfg.n_train > 0:
            i = helpers.balanced_random_sample(dataset.y_array.numpy(), cfg.n_train)
            assert len(i) == cfg.n_train
            dataset = torch.utils.data.Subset(dataset, i)
            # When we create a Subset, it doesn't inherit the collate method from the original dataset. The WILDS dataloader expects this attribute to be present as it uses it for the collate_fn parameter. We need to copy it from the original dataset to avoid AttributeError.
            dataset.collate = dataset.dataset.collate
        dataloader = wilds.common.data_loaders.get_train_loader(
            "standard",
            dataset,
            batch_size=cfg.batch_size,
            num_workers=cfg.n_workers,
        )
    else:
        subset = "test"
        dataset = dataset.get_subset(subset, transform=transform)
        dataloader = wilds.common.data_loaders.get_eval_loader(
            "standard",
            dataset,
            batch_size=cfg.batch_size,
            num_workers=cfg.n_workers,
        )

    backbone = torch.compile(backbone.to(cfg.device))
    all_features, all_labels, all_ids = [], [], []

    def probe(batch):
        imgs, labels, _ = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc=f"iwildcam/{subset}"):
            imgs, labels, _ = next(it)
            imgs = imgs.to(cfg.device)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_labels.extend(labels)

            ids = [str(i + len(all_ids)) for i in range(len(labels))]
            all_ids.extend(ids)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = torch.tensor(all_labels).numpy()
    all_ids = np.array(all_ids)

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    if 0 < cfg.n_train <= 300:
        return sklearn.linear_model.RidgeClassifier()

    return sklearn.model_selection.GridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        # This uses sklearn.metrics.f1_score with average="macro", just like our final score calculator.
        scoring="f1_macro",
    )

>>>> iwildcam/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "wilds",
#     "tyro",
# ]
# ///
import dataclasses

import tyro
import wilds


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """where to save data."""
    download: bool = True
    """whether to download the data."""


def main(args: Args):
    wilds.get_dataset(dataset="iwildcam", download=args.download, root_dir=args.dir)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> jobkit/__init__.py
from .futures import FutureQueue
from .hooks import ExitHook

__all__ = ["ExitHook", "FutureQueue"]

>>>> jobkit/futures.py
import time
import typing

import beartype

T = typing.TypeVar("T")


@beartype.beartype
class FutureQueue(typing.Generic[T]):
    def __init__(self, max_size: int):
        """Create queue. max_size >= 0; 0 => always full."""
        self._max_size = max_size
        self._items = []  # FIFO queue of items

    def submit(self, item: T) -> None:
        """RuntimeError if full()."""
        if self.full():
            raise RuntimeError(f"Queue is full (max_size={self._max_size})")
        self._items.append(item)

    def pop(self) -> T:
        """Block until *some* contained Job is done, remove and return its payload."""
        if not self._items:
            return None

        # First, check if any job is already done (non-blocking)
        for i, item in enumerate(self._items):
            if self._is_done(item):
                return self._items.pop(i)

        # If no job is done, wait for the first one to complete
        while self._items:
            for i, item in enumerate(self._items):
                if self._is_done(item):
                    return self._items.pop(i)
            # No job is done yet, sleep briefly before checking again
            time.sleep(0.1)

        return None

    def _is_done(self, obj) -> bool:
        """Check if an object or any of its nested items is done."""
        # Direct check for objects with done() method
        if hasattr(obj, "done") and callable(obj.done) and obj.done():
            return True

        # Check first level of nesting (tuples, lists, dicts)
        if isinstance(obj, (tuple, list)):
            for item in obj:
                if hasattr(item, "done") and callable(item.done) and item.done():
                    return True
                # Check second level of nesting
                if isinstance(item, (tuple, list)):
                    for subitem in item:
                        if (
                            hasattr(subitem, "done")
                            and callable(subitem.done)
                            and subitem.done()
                        ):
                            return True
                elif isinstance(item, dict):
                    for subitem in item.values():
                        if (
                            hasattr(subitem, "done")
                            and callable(subitem.done)
                            and subitem.done()
                        ):
                            return True
        elif isinstance(obj, dict):
            for item in obj.values():
                if hasattr(item, "done") and callable(item.done) and item.done():
                    return True
                # Check second level of nesting
                if isinstance(item, (tuple, list)):
                    for subitem in item:
                        if (
                            hasattr(subitem, "done")
                            and callable(subitem.done)
                            and subitem.done()
                        ):
                            return True
                elif isinstance(item, dict):
                    for subitem in item.values():
                        if (
                            hasattr(subitem, "done")
                            and callable(subitem.done)
                            and subitem.done()
                        ):
                            return True

        return False

    def full(self) -> bool:
        """Return True if the queue is at capacity."""
        if self._max_size == 0:
            return True
        return len(self._items) >= self._max_size

    def __len__(self) -> int:
        """Return the number of items in the queue."""
        return len(self._items)

    def __bool__(self) -> bool:
        """Return True if the queue is not empty."""
        return bool(self._items)

    def __iter__(self):
        """Iterate over items in FIFO order."""
        return iter(self._items)

>>>> jobkit/hooks.py
import atexit
import collections.abc
import signal
import sys
import threading
import typing
import weakref

import beartype

HashableT = typing.TypeVar("HashableT", bound=collections.abc.Hashable)


@beartype.beartype
class ExitHook(typing.Generic[HashableT]):
    """
    Keep track of outstanding *claims* and make sure each one is released
    on SIGINT/SIGTERM or normal interpreter shutdown.

    Parameters
    ----------
    release_fn
        Callback that frees a single claim (e.g. lambda c: reporting.release_run(db, *c)).
    lock_factory
        Optional constructor--defaults to `threading.Lock` but can be swapped
        for a stub in tests.

    Typical usage
    -------------
    >>> hook = ExitHook(release_fn).register()
    >>> if claim():              # user-defined "claim" operation
    ...     hook.add(payload)
    ...     try:
    ...         run_job()
    ...     finally:
    ...         hook.discard(payload)
    """

    # ---------------------------------------------------------------------
    # Class-level state shared by all ExitHook instances
    # ---------------------------------------------------------------------

    # WeakSet lets us track "all currently alive hooks" without accidentally keeping them alive.
    _live: "weakref.WeakSet[ExitHook]" = weakref.WeakSet()
    _installed: bool = False
    _install_lock: threading.Lock = threading.Lock()

    def __init__(
        self,
        release_fn: collections.abc.Callable[[HashableT], None],
        *,
        lock_factory: collections.abc.Callable[[], threading.Lock] = threading.Lock,
    ):
        self._release_fn = release_fn
        self._claims = set()
        self._lock = lock_factory()
        self._registered = False

    def register(self) -> "ExitHook[HashableT]":
        """Install signal & atexit hooks to ensure claims are released."""
        # Register this instance and, once per process, hook up the signals.
        ExitHook._live.add(self)
        with ExitHook._install_lock:
            if not ExitHook._installed:
                signal.signal(signal.SIGINT, ExitHook._shared_handler)
                signal.signal(signal.SIGTERM, ExitHook._shared_handler)
                ExitHook._installed = True
        # Even if we installed signals earlier, tests expect atexit.register to be invoked for *each* new `register()` call after they monkey-patch it.
        atexit.register(ExitHook._shared_exit_handler)
        return self

    def add(self, claim: HashableT) -> None:
        """Add a claim to be tracked and released on exit."""
        with self._lock:
            self._claims.add(claim)

    def discard(self, claim: HashableT) -> None:
        """Remove a claim from tracking."""
        with self._lock:
            self._claims.discard(claim)

    def release_run(self, claim: HashableT) -> None:
        """Thin wrapper around the release_fn."""
        self._release_fn(claim)

    # ------------------------------------------------------------------
    # Shared callbacks
    # ------------------------------------------------------------------

    @staticmethod
    def _shared_handler(signum, frame):
        """Flush claims for *all* live hooks."""
        for hook in list(ExitHook._live):
            hook._release_all_claims()

        # propagate the original intent
        if signum == signal.SIGINT:
            raise KeyboardInterrupt
        elif signum == signal.SIGTERM:
            sys.exit(128 + signum)

    @staticmethod
    def _shared_exit_handler():
        for hook in list(ExitHook._live):
            hook._release_all_claims()

    def _release_all_claims(self):
        """Release all tracked claims and clear the set."""
        with self._lock:
            for claim in self._claims:
                self._release_fn(claim)
            self._claims.clear()

>>>> jobkit/test_futures.py
"""
Unit tests for the `JobQueue` helper described in Coding-spec: ClaimReaper & JobQueue.

We use a minimal `FakeJob` stub that mimics the `submitit.Job` API (`done()` -> bool) so the tests remain independent of SubmitIt. Each test includes a docstring explaining the contract being verified.
"""

import queue
import threading
import time

import pytest

from .futures import FutureQueue


class FakeJob:
    """Simple stub that reports completion via `done()`."""

    def __init__(self, done: bool = False):
        self._done = done

    def done(self) -> bool:
        return self._done

    def mark_done(self):
        self._done = True

    # Make debugging nicer.
    def __repr__(self) -> str:
        return f"<FakeJob done={self._done}>"


class DelayedJob(FakeJob):
    """`done()` flips to True after `delay` wall-seconds."""

    def __init__(self, delay: float):
        super().__init__(done=False)
        self._ready_at = time.time() + delay

    def done(self) -> bool:
        if not self._done and time.time() >= self._ready_at:
            self._done = True
        return self._done


def test_submit_and_len():
    """
    `submit()` appends items and `len(queue)` tracks the count.

    A queue with `max_size=3` should grow from 0 -> 1 -> 2 as items are added.
    """
    q = FutureQueue(max_size=3)
    assert len(q) == 0

    q.submit(FakeJob())
    assert len(q) == 1

    q.submit(FakeJob())
    assert len(q) == 2


def test_full_and_submit_raises():
    """
    `submit()` must raise `RuntimeError` when the queue is full.

    This covers both:
    * normal capacity exhaustion (`max_size=1`)
    * the "always full" sentinel (`max_size=0`)
    """
    single = FutureQueue(max_size=1)
    single.submit(FakeJob())  # fills the queue
    with pytest.raises(RuntimeError):
        single.submit(FakeJob())  # should fail

    always_full = FutureQueue(max_size=0)
    assert always_full.full() is True
    with pytest.raises(RuntimeError):
        always_full.submit(FakeJob())  # should fail immediately


def test_bool_dunder_and_full():
    """
    `__bool__` mirrors "non-empty" and `full()` reflects capacity status.

    * Empty queue -> `bool(q)` is `False`.
    * After one insert -> `bool(q)` is `True`.
    * When at capacity -> `full()` becomes `True`.
    """
    q = FutureQueue(max_size=2)
    assert not q and len(q) == 0

    q.submit(FakeJob())
    assert q and not q.full()

    q.submit(FakeJob())
    assert q.full()


@pytest.mark.timeout(5)
def test_pop_returns_first_finished_payload():
    """
    `pop()` must return the *earliest finished* payload, not simply FIFO order.

    We enqueue two jobs:
    * `slow` -> not yet done
    * `fast` -> already done

    Even though `slow` was submitted first, `pop()` should unblock on `fast`.
    """
    slow, fast = FakeJob(done=False), FakeJob(done=True)
    q = FutureQueue(max_size=3)
    q.submit(slow)  # inserted first
    q.submit(fast)  # inserted second

    popped = q.pop()
    assert popped is fast
    assert len(q) == 1 and q.full() is False


@pytest.mark.timeout(5)
def test_pop_recognises_nested_jobs():
    """
    The queue must scan *nested* payloads (<= 2 levels, depth-first).

    We wrap a finished `FakeJob` inside a tuple along with arbitrary metadata
    and verify that `pop()` still detects completion.
    """
    done_job = FakeJob(done=True)
    nested_payload = (done_job, {"cfg": "dummy"})
    q = FutureQueue(max_size=2)
    q.submit(nested_payload)

    popped = q.pop()
    assert popped is nested_payload


@pytest.mark.timeout(5)
def test_pop_frees_capacity_for_further_submissions():
    """
    After `pop()` removes a finished payload the queue should no longer be full.

    * Fill a queue of size 1 with a *finished* job.
    * `pop()` should return immediately and leave the queue empty.
    * A subsequent `submit()` must succeed without raising.
    """
    q = FutureQueue(max_size=1)
    q.submit(FakeJob(done=True))  # queue is full

    q.pop()  # frees the slot
    assert len(q) == 0 and not q.full()

    # Should accept a new item now.
    try:
        q.submit(FakeJob())
    except RuntimeError:
        pytest.fail("Queue did not free capacity after pop()")


@pytest.mark.timeout(5)
def test_depth_two_nesting_is_detected():
    """
    The spec promises a *depth-first scan <= 2* levels deep.

    We wrap a finished job two layers down:  [ ( job ) ].  `pop()` must still
    recognise completion and unblock.
    """
    deep = [[FakeJob(done=True)]]
    q = FutureQueue(max_size=2)
    q.submit(deep)

    popped = q.pop()
    assert popped is deep  # exact payload returned


@pytest.mark.timeout(5)
def test_payload_with_multiple_jobs_mixed_status():
    """
    If any *member* job is finished the *whole* payload counts as finished.

    We craft a container with (unfinished, finished).  Even though the first
    element is *not* done, the second is -- so `pop()` must still dequeue it.
    """
    first, second = FakeJob(done=False), FakeJob(done=True)
    mixed = (first, second)
    q = FutureQueue(max_size=1)
    q.submit(mixed)

    popped = q.pop()
    assert popped is mixed
    assert len(q) == 0  # queue now empty


@pytest.mark.timeout(5)
def test_non_job_payload_left_in_queue_if_no_done_job_inside():
    """
    Submitting a payload *without* any `done()` method must *not* starve later
    finished jobs.

    We enqueue a plain dict (no job) first, followed by a finished `FakeJob`.
    `pop()` should skip over the dict, return the finished job, and leave the
    dict in the queue.
    """
    sentinel = {"msg": "not a job"}
    finished = FakeJob(done=True)

    q = FutureQueue(max_size=3)
    q.submit(sentinel)
    q.submit(finished)

    assert q.pop() is finished
    assert list(q) == [sentinel]  # dict still waiting (never satisfies pop)


@pytest.mark.timeout(5)
def test_iter_preserves_fifo_after_interleaved_pops():
    """
    `__iter__` must always reflect current FIFO order of *remaining* items.

    Scenario:
    1. enqueue A(done=False), B(done=True), C(done=False)
    2. `pop()` removes B
    3. Order should now be [A, C] -- verify with `list(q)`.
    """
    a, b, c = FakeJob(), FakeJob(done=True), FakeJob()
    q = FutureQueue(max_size=5)
    q.submit(a)
    q.submit(b)
    q.submit(c)

    q.pop()  # removes B
    assert [a, c] == list(q)


def _call_pop(fq: FutureQueue, out: queue.Queue):
    """Run fq.pop() in a separate thread and capture result / exception."""
    try:
        out.put(fq.pop())
    except Exception as exc:  # pragma: no cover
        out.put(exc)


def test_pop_returns_first_runtime_finish_not_fifo():
    """
    The job that *completes first in wall time* must be returned, regardless
    of insertion order.

    Queue order:  slow(2 s)  ->  fast(0.1 s)

    Expected: `pop()` blocks ~0.1 s, returns *fast*.
    """
    slow = DelayedJob(2.0)
    fast = DelayedJob(0.1)

    fq = FutureQueue(max_size=2)
    fq.submit(slow)  # FIFO head
    fq.submit(fast)  # finishes first

    out = queue.Queue()
    t = threading.Thread(target=_call_pop, args=(fq, out), daemon=True)
    t.start()
    t.join(timeout=5)

    assert not t.is_alive(), "pop() blocked too long"
    popped = out.get_nowait()
    assert popped is fast
    assert list(fq) == [slow]  # slow still pending


def test_blocking_pop_unblocks_when_only_job_finishes_later():
    """
    When **all** payloads are unfinished, `pop()` must wait until one finishes.

    We enqueue a single `DelayedJob(0.2)`, start `pop()` in a thread, and
    verify it returns within the safety window.
    """
    delayed = DelayedJob(0.2)
    fq = FutureQueue(max_size=1)
    fq.submit(delayed)

    out = queue.Queue()
    t = threading.Thread(target=_call_pop, args=(fq, out), daemon=True)
    t.start()
    t.join(timeout=5)

    assert not t.is_alive(), "pop() blocked >5 s"
    assert out.get_nowait() is delayed
    assert len(fq) == 0


def test_pop_skips_non_job_then_unblocks_on_future_finish():
    """
    Interleaving:  [dict, DelayedJob].

    `pop()` must *skip* the non-job payload, block until the job finishes,
    then return **the job** while leaving the dict untouched.
    """
    sentinel = {"cfg": "noop"}
    job = DelayedJob(0.1)

    fq = FutureQueue(3)
    fq.submit(sentinel)
    fq.submit(job)

    out = queue.Queue()
    t = threading.Thread(target=_call_pop, args=(fq, out), daemon=True)
    t.start()
    t.join(timeout=5)

    assert out.get_nowait() is job
    assert list(fq) == [sentinel]


def test_multiple_done_jobs_in_one_payload_returns_first_depth_first():
    """
    If a single payload contains *several* finished jobs, the queue should treat the first depth-first hit as the trigger.

    We build:  ( doneA , [ doneB ] )  -- `pop()` may choose either, but spec allows returning the *payload* not the individual job; ensure dequeueing occurs immediately.
    """
    a, b = FakeJob(done=True), FakeJob(done=True)
    nested = (a, [b])

    fq = FutureQueue(2)
    fq.submit(nested)

    assert fq.pop() is nested
    assert len(fq) == 0

>>>> jobkit/test_hooks.py
import atexit
import signal
import threading
import time

import beartype
import pytest

from .hooks import ExitHook


def _invoke_handler(handler, signum):
    """
    Call the given *handler* as the signal machinery would:
    - pass (signum, current-frame)   (frame may be None for our purposes)
    - swallow KeyboardInterrupt / SystemExit so the suite keeps running.
    """
    try:
        handler(signum, None)
    except (KeyboardInterrupt, SystemExit):
        pass


def test_register_returns_self_and_is_idempotent():
    """
    *Spec:* `register()` **must** return the same object so callers can write
    `reaper = ClaimReaper(...).register()`.

    It must also be safe to call repeatedly (harmless no-ops) because some launchers may defensively register twice.
    """

    sink = []
    hook = ExitHook(lambda claim: sink.append(claim))

    assert hook.register() is hook  # first call
    assert hook.register() is hook  # second call - still fine


def test_add_then_discard_accepts_generic_payload():
    """
    With the new signature `add(claim)`, the class must accept *any* hashable
    payload the caller chooses.  The easiest non-trivial smoke-test is a tuple.
    """

    hook = ExitHook(release_fn=lambda c: None).register()

    claim = ("cfg-xyz", "task-abc")
    hook.add(claim)  # should not raise
    hook.discard(claim)  # should not raise


def test_multiple_outstanding_claims_do_not_interfere():
    """
    Adding several distinct claims before discarding them should not trigger
    internal errors such as "set changed size during iteration".
    """

    hook = ExitHook(lambda c: None).register()
    claims = [f"job-{i}" for i in range(5)]

    for c in claims:
        hook.add(c)

    for c in claims:
        hook.discard(c)


def test_release_run_calls_injected_release_fn():
    """
    `release_run()` is documented as a *thin wrapper* that forwards to the
    injected `release_fn`.  Verify that the exact same claim object reaches the
    callback once--and only once.
    """

    hits = []
    hook = ExitHook(lambda claim: hits.append(claim))

    claim = ("id", 7)
    hook.release_run(claim)

    assert hits == [claim], "release_fn should have been invoked exactly once"


def test_add_requires_hashable_claim():
    """
    Non-hashable objects cannot be stored in the internal set that tracks
    outstanding claims.  A correct implementation therefore raises TypeError.
    """

    hook = ExitHook(lambda c: None)

    # list is non-hashable -> should blow up
    with pytest.raises((TypeError, beartype.roar.BeartypeCallHintParamViolation)):
        hook.add(["unhashable"])


def test_release_run_invokes_callback_exactly_each_time():
    """
    *Behavioural guarantee:* every explicit release_run() call must translate
    into one--and only one--invocation of the injected release_fn, regardless of
    whether the claim was previously added/discarded.
    """

    hits = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    claim = ("cfg-id-123", "task-foo")

    # Claim is active
    hook.add(claim)
    hook.release_run(claim)
    assert hits == [claim]  # called once

    # Claim is no longer tracked
    hook.discard(claim)
    hook.release_run(claim)
    assert hits == [claim, claim]  # called again, no extras


def test_sigint_releases_all_claims():
    hits = []
    claims = [f"claim-{i}" for i in range(3)]
    hook = ExitHook(lambda c: hits.append(c)).register()

    for c in claims:
        hook.add(c)

    old_handler = signal.getsignal(signal.SIGINT)
    try:
        with pytest.raises(KeyboardInterrupt):
            signal.raise_signal(signal.SIGINT)
    finally:
        signal.signal(signal.SIGINT, old_handler)  # restore for other tests

    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(claims)


def test_sigint_handler_releases_all_claims():
    """
    Core guarantee: when the SIGINT handler is invoked, every *current* claim
    is released exactly once.

    We call the handler directly rather than raising a real signal so the
    suite survives even if ClaimReaper forgot to install it.
    """

    hits, claims = [], [f"claim-{i}" for i in range(3)]
    hook = ExitHook(lambda c: hits.append(c)).register()
    for c in claims:
        hook.add(c)

    handler = signal.getsignal(signal.SIGINT)
    assert callable(handler) and handler not in (signal.SIG_DFL, signal.SIG_IGN), (
        "ClaimReaper failed to install a SIGINT handler"
    )

    _invoke_handler(handler, signal.SIGINT)

    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(claims)


def test_duplicate_add_is_idempotent_under_sigint():
    """
    Adding the same claim twice must not produce duplicate releases when the
    SIGINT handler fires.
    """

    hits, claim = [], ("cfg-42", "task-alpha")
    hook = ExitHook(lambda c: hits.append(c)).register()
    hook.add(claim)
    hook.add(claim)  # duplicate

    handler = signal.getsignal(signal.SIGINT)
    assert callable(handler), "missing SIGINT handler"
    _invoke_handler(handler, signal.SIGINT)

    assert hits == [claim], "duplicate add produced duplicate release"


def test_sigterm_handler_releases_only_current_claims():
    """
    Discarded claims should not be released by the SIGTERM handler.
    """

    hits = []
    live1, live2, discarded = "stay-1", "stay-2", "gone-x"
    hook = ExitHook(lambda c: hits.append(c)).register()

    for c in (live1, live2, discarded):
        hook.add(c)
    hook.discard(discarded)  # no longer live

    handler = signal.getsignal(signal.SIGTERM)
    assert callable(handler), "missing SIGTERM handler"
    _invoke_handler(handler, signal.SIGTERM)

    assert sorted(hits) == sorted([live1, live2])


def test_discard_unknown_claim_is_noop():
    """
    Discarding a claim that was never added should *not* raise.  This keeps
    launcher code simple because it can unconditionally discard in finally-
    blocks without first checking membership.
    """

    hook = ExitHook(lambda c: None).register()

    # Should silently ignore
    hook.discard("non-existent")


def test_register_calls_atexit(monkeypatch):
    """
    *Contract*: `register()` must install an atexit hook so that claims are released on normal interpreter shutdown.

    We patch `atexit.register` to capture the callback and assert that:
    1) it was invoked exactly once; 2) the registered object is callable.
    """

    captured: list[callable] = []

    def _fake_register(fn, *args, **kwargs):
        captured.append(fn)

    monkeypatch.setattr(atexit, "register", _fake_register)

    # construction shouldn't trigger the hook -- only .register()
    hook = ExitHook(lambda _: None)
    assert not captured

    hook.register()
    assert len(captured) == 1
    assert callable(captured[0])


def test_atexit_handler_releases_all_live_claims(monkeypatch):
    """
    Verify that the function registered with `atexit` releases *every* claim
    that is still active when the interpreter would exit.

    Strategy
    --------
    * Capture the cleanup callback via a patched `atexit.register`.
    * Add several claims.
    * Manually invoke the captured callback.
    * Confirm `release_fn` was called once per tracked claim (order irrelevant).
    """

    cleanup_fns = []

    def _capture(fn, *a, **kw):
        cleanup_fns.append(fn)

    monkeypatch.setattr(atexit, "register", _capture)

    hits = []
    hook = ExitHook(lambda c: hits.append(c)).register()
    claims = [("cfg-0", "task-0"), ("cfg-1", "task-1"), ("cfg-2", "task-2")]
    for claim in claims:
        hook.add(claim)

    # Simulate interpreter shutdown
    assert cleanup_fns, "No atexit hook registered"
    cleanup_fns[0]()  # invoke captured function

    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(claims)


def test_lock_prevents_set_mutation_during_massive_adds():
    """
    A classic failure mode is "set changed size during iteration" when the
    signal-handler walks `_claims` while another thread is adding claims.

    Strategy
    --------
    * Worker thread continuously adds new claims.
    * Main thread waits a short moment, then calls the SIGINT handler.
    * If locking is absent we'll almost certainly trigger the RuntimeError.
    * We also check that each claim is released **at most once**.
    """

    hits: list[str] = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    stop = threading.Event()

    def _producer():
        i = 0
        while not stop.is_set():
            hook.add(f"claim-{i}")
            i += 1

    t = threading.Thread(target=_producer)
    t.start()

    # Give producer a head-start so the set is being modified.
    time.sleep(0.05)

    handler = signal.getsignal(signal.SIGINT)
    _invoke_handler(handler, signal.SIGINT)

    stop.set()
    t.join()

    # No duplicates => each claim released only once (lock prevented races)
    assert len(hits) == len(set(hits))


def test_lock_prevents_set_mutation_during_discards():
    """
    Another race: thread discarding while handler iterates.
    """

    hits: list[str] = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    # Pre-populate many claims
    claims = [f"c{i}" for i in range(250)]
    for c in claims:
        hook.add(c)

    def _consumer():
        for c in claims:
            hook.discard(c)
            time.sleep(0.0005)  # keep the race window open

    t = threading.Thread(target=_consumer)
    t.start()

    # Let the consumer start discarding, then fire the handler
    time.sleep(0.02)
    handler = signal.getsignal(signal.SIGINT)
    _invoke_handler(handler, signal.SIGINT)

    t.join()

    # All *remaining* live claims were released once; any discarded before
    # the handler shouldn't re-appear, so no duplicates.
    assert len(hits) == len(set(hits))


def test_lock_serialises_multiple_concurrent_handlers():
    """
    If two threads invoke the handler almost simultaneously, the internal lock
    must guarantee:
      * no crashes,
      * each claim released <= 1 time,
      * after both finish `_claims` is empty (second call sees nothing).

    We mimic this by launching a second thread that calls the SIGTERM handler
    while the main thread does the same.
    """

    hits: list[tuple[str, str]] = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    claims = [(f"cfg{i}", f"t{i}") for i in range(100)]
    for c in claims:
        hook.add(c)

    handler = signal.getsignal(signal.SIGTERM)

    barrier = threading.Barrier(2)

    def _invoke():
        barrier.wait()
        _invoke_handler(handler, signal.SIGTERM)

    t = threading.Thread(target=_invoke)
    t.start()

    barrier.wait()  # release both threads
    _invoke_handler(handler, signal.SIGTERM)
    t.join()

    # Every claim released exactly once
    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(set(hits))


def test_handler_identity_shared_across_instances():
    """
    Registering two separate ClaimReaper objects must **not** replace the
    process-wide signal handler with two different callables.  Both `.register()`
    calls should leave exactly *one* shared handler installed.
    """

    old = signal.getsignal(signal.SIGINT)
    try:
        ExitHook(lambda _: None).register()
        handler1 = signal.getsignal(signal.SIGINT)

        ExitHook(lambda _: None).register()
        handler2 = signal.getsignal(signal.SIGINT)

        assert handler1 is handler2 is not signal.SIG_DFL
    finally:
        signal.signal(signal.SIGINT, old)


def test_multiple_hooks_each_release_their_own_claims():
    """
    When the shared handler runs, *every* live claim from *every* registered
    hook must be released exactly once.
    """

    hits1, hits2 = [], []
    r1 = ExitHook(lambda c: hits1.append(c)).register()
    r2 = ExitHook(lambda c: hits2.append(c)).register()

    claim1, claim2 = ("A", 1), ("B", 2)
    r1.add(claim1)
    r2.add(claim2)

    _invoke_handler(signal.getsignal(signal.SIGINT), signal.SIGINT)

    assert hits1 == [claim1]
    assert hits2 == [claim2]


def test_discarded_claims_on_one_hook_do_not_affect_others():
    """
    If hook-1 discards a claim before the signal arrives, only hook-2's
    live claim should be released.
    """

    h1, h2 = [], []
    r1 = ExitHook(lambda c: h1.append(c)).register()
    r2 = ExitHook(lambda c: h2.append(c)).register()

    gone = ("gone", 0)
    stay = ("stay", 1)

    r1.add(gone)
    r1.discard(gone)  # already finished
    r2.add(stay)

    _invoke_handler(signal.getsignal(signal.SIGINT), signal.SIGINT)

    assert h1 == []  # no spurious releases
    assert h2 == [stay]


def test_unregistered_hook_claims_are_not_released():
    """
    Claims tracked by a *non-registered* ClaimReaper instance must *not* be
    released when some other hook's handler fires.
    """

    ghost_hits, live_hits = [], []
    ghost = ExitHook(lambda c: ghost_hits.append(c))  # NOT registered
    live = ExitHook(lambda c: live_hits.append(c)).register()

    ghost.add(("ghost", 9))
    live.add(("live", 10))

    _invoke_handler(signal.getsignal(signal.SIGINT), signal.SIGINT)

    assert ghost_hits == []  # untouched
    assert live_hits == [("live", 10)]


def test_handler_is_reentrant_no_duplicate_releases():
    """
    After the first invocation empties the claim set, a *second* call should
    release nothing new.  This proves internal state was cleared.
    """

    hits = []
    r = ExitHook(lambda c: hits.append(c)).register()
    r.add(("cfg", "task"))

    handler = signal.getsignal(signal.SIGTERM)
    _invoke_handler(handler, signal.SIGTERM)  # first pass
    first_count = len(hits)

    _invoke_handler(handler, signal.SIGTERM)  # second pass
    assert len(hits) == first_count, "duplicate releases detected"


def test_can_add_new_claims_after_previous_release():
    """
    A hook should remain usable: after its claims are flushed by a signal,
    callers may add new claims and expect those to be released on the *next*
    signal.
    """

    hits = []
    r = ExitHook(lambda c: hits.append(c)).register()

    r.add("first")
    h = signal.getsignal(signal.SIGTERM)
    _invoke_handler(h, signal.SIGTERM)

    r.add("second")
    _invoke_handler(h, signal.SIGTERM)

    assert hits == ["first", "second"]


def test_second_signal_after_empty_claims_is_noop():
    """
    If no claims are outstanding, invoking the handler should simply return and
    **not** raise or append anything.
    """

    hits = []
    ExitHook(lambda c: hits.append(c)).register()

    handler = signal.getsignal(signal.SIGINT)
    _invoke_handler(handler, signal.SIGINT)  # nothing to release

    assert hits == []


def test_atexit_cleanup_is_idempotent():
    """
    The function registered with `atexit` must clear the claim set so that a second manual call is harmless (re-entrant) and releases nothing new.
    """

    captured = []

    def _capture(fn, *a, **kw):
        captured.append(fn)

    with pytest.MonkeyPatch().context() as m:
        m.setattr(atexit, "register", _capture)

        hits = []
        r = ExitHook(lambda c: hits.append(c)).register()
        r.add("x")

    # Simulate normal shutdown twice
    captured[0]()  # first call
    captured[0]()  # second call - should be a no-op

    assert hits == ["x"]

>>>> kabr/__init__.py
"""
# Kenyan Animal Behavior Recognition (KABR)

KABR is a video recognition task ([paper](https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/papers/Kholiavchenko_KABR_In-Situ_Dataset_for_Kenyan_Animal_Behavior_Recognition_From_Drone_WACVW_2024_paper.pdf), [website](https://kabrdata.xyz/), [Huggingface](https://huggingface.co/datasets/imageomics/KABR)) where the model predicts Kenyan animal behavior in short video segments.

This can be framed as a classification task: given a short video segment of a single animal, which behavior is most common within the segment?

While specialized architectures exist, we train a simple nearest-centroid classifier [which works well with few-shot tasks](https://arxiv.org/abs/1911.04623) over video representations.
We get video representations by embedding each frame of the video and taking the mean over the batch dimension.

## Data

To download the data, you need to use the dataset download script:

1. Copy-paste the [download script](https://huggingface.co/datasets/imageomics/KABR/raw/main/download.py) to your data directory, like `/scratch/KABR/download.py`.
2. Run `python download.py`. It doesn't have any requirements beyond the Python standard library.
"""

import csv
import dataclasses
import logging
import os

import beartype
import numpy as np
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting, simpleshot

logger = logging.getLogger(__name__)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Video:
    """A single video instance as a sequence of frames."""

    video_id: int
    """Video ID."""
    frames: list[str]
    """Paths to actual frame images."""
    labels: list[int]
    """Frame-level labels."""

    def __post_init__(self):
        err_msg = f"Video {self.video_id} has a different number of frames ({len(self.frames)} and labels ({len(self.labels)})."
        assert len(self.frames) == len(self.labels), err_msg


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """
    Clips of at most 90 frames in Charades format with each frame stored as an image.
    """

    def __init__(self, path, split: str, transform=None, seed: int = 42):
        self.path = path
        self.split = split
        self.transform = transform
        self.seed = seed

        self.rng = np.random.default_rng(seed=seed)

        self.n_frames = 16
        self.n_every = 5

        # Load videos
        #############

        frames: dict[int, list[str]] = {}
        labels: dict[int, list[int]] = {}

        if not os.path.exists(self.path) or not os.path.isdir(self.path):
            msg = f"Path '{self.path}' doesn't exist. Did you download the KABR dataset? See the docstring at the top of this file for instructions."
            raise RuntimeError(msg)

        with open(os.path.join(self.path, "annotation", f"{split}.csv")) as fd:
            reader = csv.reader(fd, delimiter=" ")
            next(reader)  # skip headers
            for _, video_id, frame_id, path, label in reader:
                video_id = int(video_id)
                frame_id = int(frame_id)
                label = int(label)

                if video_id not in frames:
                    frames[video_id] = []
                if video_id not in labels:
                    labels[video_id] = []

                if frame_id > len(frames[video_id]) + 1:
                    raise ValueError(f"Video {video_id} is missing a frame.")

                path = os.path.join(self.path, "dataset", "image", path)
                frames[video_id].append(path)
                labels[video_id].append(label)

        self.videos = [
            Video(video_id, frames[video_id], labels[video_id])
            for video_id in frames.keys()
            if len(frames[video_id]) >= self.n_frames
        ]

    def __getitem__(
        self, i: int
    ) -> tuple[list[Float[Tensor, "3 width height"]], list[int], str]:
        """
        Returns 16 frames and their labels sampled every 5 frames from a clip. The start of the clip is uniformly sampled. If there are fewer
        """
        n_every = self.n_every

        video = self.videos[i]

        while len(video.frames) < ((self.n_frames - 1) * n_every + 1):
            n_every -= 1

        if n_every <= 0:
            print(n_every, len(video.frames), ((self.n_frames - 1) * n_every + 1))
        assert n_every >= 1

        # margin is the number of extra frames on either size of the 16x5 sampled frames.
        margin = len(video.frames) - ((self.n_frames - 1) * n_every + 1)

        # Pick a random start, then pick n_frames frames every n_every frames.
        # (sam) This is likely not clear and there are probably better ways to express this in Python that is more clear to other video ML devs. Please open a PR if you know a better way!
        start = self.rng.integers(0, margin + 1)
        frames = video.frames[start:None:n_every][: self.n_frames]
        labels = video.labels[start:None:n_every][: self.n_frames]

        images = [Image.open(frame) for frame in frames]

        if self.transform is not None:
            images = [self.transform(image) for image in images]

        return images, labels, str(i)

    def __len__(self) -> int:
        return len(self.videos)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """Runs KABR benchmark."""
    # 1. Load model
    backbone = registry.load_vision_backbone(cfg.model)
    backbone = backbone.to(cfg.device)

    # 2. Load data.
    test_features = get_features(cfg, backbone, is_train=False)
    train_features = get_features(cfg, backbone, is_train=True)

    # 4. Do simpleshot.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    # Return benchmark report.
    preds = [
        reporting.Prediction(
            str(video_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for video_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]
    return reporting.Report("kabr", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    """
    Gets all model features and true labels for all frames and all examples in the dataloader.

    Returns it as a pair of big tensors; other tasks use a dedicated class for this, but here it's just a tuple.

    Args:
        args: KABR task arguments.
        backbone: Vision backbone.
        dataloader: Dataloader for whatever data you want to get features for.

    Returns:
        tuple of model features and true labels. See signature for shape.
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone)
    split = "train" if is_train else "val"

    dataset = Dataset(cfg.data.kabr, split, transform=img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=True,
    )

    all_feats, all_labels, all_ids = [], [], []

    def probe(batch):
        frames, _, _ = batch
        frames = torch.stack(frames, dim=0)
        frames = frames.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
            n_frames, bsz, c, h, w = frames.shape
            frames = frames.view(bsz * n_frames, c, h, w)
            outputs = backbone.img_encode(frames)
            features = outputs.img_features.view(n_frames, bsz, -1)
            features = aggregate_frames(features)

    with helpers.auto_batch_size(dataloader, probe=probe, backoff=1):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc=f"kabr/{split}"):
            frames, labels, ids = next(it)
            frames = torch.stack(frames, dim=0)
            labels = torch.stack(labels, dim=0)
            frames = frames.to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
                n_frames, bsz, c, h, w = frames.shape
                frames = frames.view(bsz * n_frames, c, h, w)
                outputs = backbone.img_encode(frames)
                features = outputs.img_features.view(n_frames, bsz, -1)

                features = aggregate_frames(features)
                all_feats.append(features.cpu())

            labels = aggregate_labels(labels)
            all_labels.append(labels.cpu())

            logger.debug("Embedded batch %d/%d", b + 1, total)
            all_ids.extend(ids)

    all_feats = torch.cat(all_feats, dim=0).cpu().numpy()
    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()
    all_ids = np.array(all_ids)

    return Features(all_feats, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    return simpleshot.SimpleShotClassifier(device="cuda:0")


@jaxtyped(typechecker=beartype.beartype)
def aggregate_labels(
    labels: Int[Tensor, "n_frames n_examples"],
) -> Int[Tensor, " n_examples"]:
    """Aggregate per-frame labels to a per-video label. Uses the most common label (mode)."""
    return torch.mode(labels, dim=0).values


@jaxtyped(typechecker=beartype.beartype)
def aggregate_frames(
    features: Float[Tensor, "n_frames n_examples dim"],
) -> Float[Tensor, "n_examples dim"]:
    return torch.max(features, dim=0).values

>>>> kabr/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the Kenyan Animal Behavior Recognition (KABR) dataset.

Examples
--------
# bare-bones (all animals, default path):
python -m biobench.kabr.download

# custom output directory, keep zip archives:
python -m biobench.kabr.download --dir /scratch/KABR --keep-archives
"""

import collections.abc
import dataclasses
import glob
import hashlib
import os.path
import pathlib
import zipfile

import beartype
import requests
import tqdm
import tyro

# --------- #
# Constants #
# --------- #

BASE_URL = "https://huggingface.co/datasets/imageomics/KABR/resolve/main/KABR"
DATASET_PREFIX = "dataset/image/"

ANIMAL_PART_RANGE: dict[str, tuple[str, str]] = {
    "giraffes": ("aa", "ad"),
    "zebras_grevys": ("aa", "am"),
    "zebras_plains": ("aa", "al"),
}

STATIC_FILES: list[str] = [
    "README.txt",
    "annotation/classes.json",
    "annotation/distribution.xlsx",
    "annotation/train.csv",
    "annotation/val.csv",
    "configs/I3D.yaml",
    "configs/SLOWFAST.yaml",
    "configs/X3D.yaml",
    "dataset/image2video.py",
    "dataset/image2visual.py",
]

# ------- #
# Helpers #
# ------- #


@beartype.beartype
def generate_part_files(animal: str, start: str, end: str) -> list[str]:
    """Generate `dataset/image/{animal}_part_??` blocks inclusive of start/end."""
    start_a, start_b = map(ord, start)
    end_a, end_b = map(ord, end)
    return [
        f"{DATASET_PREFIX}{animal}_part_{chr(a)}{chr(b)}"
        for a in range(start_a, end_a + 1)
        for b in range(start_b, end_b + 1)
    ]


@beartype.beartype
def all_files_for_animals(animals: collections.abc.Iterable[str]) -> list[str]:
    files: list[str] = STATIC_FILES.copy()
    for animal in animals:
        files.append(f"{DATASET_PREFIX}{animal}_md5.txt")
        start, end = ANIMAL_PART_RANGE[animal]
        files.extend(generate_part_files(animal, start, end))
    return files


@beartype.beartype
def stream_download(url: str, dst: pathlib.Path, chunk_bytes: int) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    with requests.get(url, stream=True, timeout=30) as r:
        r.raise_for_status()
        total = int(r.headers.get("content-length", 0))
        with (
            open(dst, "wb") as fd,
            tqdm.tqdm(
                total=total,
                unit="B",
                unit_scale=True,
                unit_divisor=1024,
                desc=dst.name,
                leave=False,
            ) as pbar,
        ):
            for chunk in r.iter_content(chunk_size=chunk_bytes):
                fd.write(chunk)
                pbar.update(len(chunk))


@beartype.beartype
def md5_file(path: pathlib.Path, chunk_bytes: int = 8 * 1024 * 1024) -> str:
    h = hashlib.md5()
    with open(path, "rb") as fd:
        for block in iter(lambda: fd.read(chunk_bytes), b""):
            h.update(block)
    return h.hexdigest()


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    dir: pathlib.Path = pathlib.Path("KABR_files")
    """Where to place downloaded data."""
    animals: tuple[str, ...] = tuple(ANIMAL_PART_RANGE.keys())
    """Subset of animals to download."""
    chunk_size_kb: int = 1024
    """Stream chunk size in KB."""
    keep_archives: bool = False
    """Keep concatenated *.zip files & md5 after extraction."""
    skip_existing: bool = True
    """Skip download if file already present on disk."""


@beartype.beartype
def main(args: Args) -> None:
    files = all_files_for_animals(args.animals)
    chunk = args.chunk_size_kb * 1024

    print("Downloading KABR ...")
    for rel in tqdm.tqdm(files, unit="file"):
        dst = args.dir / rel
        if args.skip_existing and dst.exists():
            continue
        url = f"{BASE_URL}/{rel}"
        stream_download(url, dst, chunk)

    print("Concatenating split archives ...")
    for animal in args.animals:
        out_zip = args.dir / f"{DATASET_PREFIX}{animal}.zip"
        parts = sorted(
            glob.glob(str(args.dir / f"{DATASET_PREFIX}{animal}_part_*")),
            key=lambda p: pathlib.Path(p).name,
        )
        if out_zip.exists() or not parts:
            continue
        total_bytes = sum(os.path.getsize(p) for p in parts)
        with (
            open(out_zip, "wb") as dst,
            tqdm.tqdm(
                total=total_bytes,
                unit="B",
                unit_scale=True,
                unit_divisor=1024,
                desc=f"Concat {animal}",
                leave=False,
            ) as bar,
        ):
            for part in parts:
                with open(part, "rb") as src:
                    for chunk in iter(lambda: src.read(8 * 1024 * 1024), b""):
                        dst.write(chunk)
                        bar.update(len(chunk))
                pathlib.Path(part).unlink()

    print("Validating & extracting ...")
    for animal in tqdm.tqdm(args.animals, unit="animal"):
        md5_txt = args.dir / f"{DATASET_PREFIX}{animal}_md5.txt"
        zip_path = args.dir / f"{DATASET_PREFIX}{animal}.zip"
        if not md5_txt.exists() or not zip_path.exists():
            print(f"Skipping {animal} (missing files).")
            continue

        expected = md5_txt.read_text().strip().split()[0]
        got = md5_file(zip_path)
        if got != expected:
            raise RuntimeError(
                f"MD5 mismatch for {zip_path.name}: {got} (expected {expected})"
            )

        with zipfile.ZipFile(zip_path) as zf:
            zf.extractall(zip_path.parent)

        if not args.keep_archives:
            zip_path.unlink(missing_ok=True)
            md5_txt.unlink(missing_ok=True)

    print(f"Done. Data at: {args.dir}")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> kabr/test_download.py
import hashlib

from . import download as kd


def test_generate_part_files():
    out = kd.generate_part_files("giraffes", "aa", "ab")
    assert out == [
        "dataset/image/giraffes_part_aa",
        "dataset/image/giraffes_part_ab",
    ]


def test_all_files_for_animals_includes_static_and_md5():
    animals = ("giraffes",)
    files = kd.all_files_for_animals(animals)
    # static + md5 + first part file sanity
    assert "README.txt" in files
    assert "dataset/image/giraffes_md5.txt" in files
    assert "dataset/image/giraffes_part_aa" in files
    # no duplicates
    assert len(files) == len(set(files))


def test_md5_file(tmp_path):
    p = tmp_path / "blob.bin"
    data = b"abc" * 123
    p.write_bytes(data)
    expect = hashlib.md5(data).hexdigest()
    assert kd.md5_file(p) == expect


def test_stream_download_stub(tmp_path, monkeypatch):
    # prepare fake response
    payload = b"hello world"

    class FakeResp:
        headers = {"content-length": str(len(payload))}

        def iter_content(self, chunk_size):
            yield payload

        def raise_for_status(self): ...

        # context-manager methods
        def __enter__(self):
            return self

        def __exit__(self, *exc): ...

    monkeypatch.setattr(kd.requests, "get", lambda *a, **kw: FakeResp())
    # run
    dst = tmp_path / "file.bin"
    kd.stream_download("http://foo.bar", dst, 16)
    assert dst.read_bytes() == payload

>>>> kabr/test_kabr.py
import pytest

from . import Video


def test_video_length_check():
    Video(0, ["f1"], [0])
    with pytest.raises(AssertionError):
        Video(1, ["f1"], [0, 0])

>>>> logbook.md
# Logbook

This provides a set of notes, step-by-step, of my process developing this paper.
In combination with the preprint, code comments, and git blame, it's probably the best way to understand *why* a decision was made, rather than *what* decision was made.

*Open science y'all*

# 02/10/2025

What are the core experiments?

We need to choose different number of samples, and run experiments multiple times.
I think I have a good set of tasks.
NeWT is probably the best first task.
Then we sample 0, 1, 3, 10, 30, 100, 300, ... MAX samples per task.
For each task and number of samples, we need to run at least 5 trials.
I should double check and see how much it would cost me to run these trials with OpenAI's GPT-4o mini.

GPT-4o mini:
Affordable small model for fast, everyday tasks | 128k context length
* Input: $0.150 / 1M tokens
* Cached input: $0.075 / 1M tokens
* Output: $0.600 / 1M tokens

Plus I can halve these costs with the Batched API.

Sonnet 3.5 is $3M / 1M input tokens, which is waaaay more expensive.
Even Haiku is $0.80 / 1M tokens, which is like 6 times more expensive.
Sorry Anthropic :(

Gemini 2.0 is $0.10 / 1M input tokens and $0.40 / 1M output tokens, which is 1/3 cheaper than 4o mini.
Then I could try Llama 3.2 11B vision which I know is garbage, and Pixtral 12B.
I think that four models is enough.
If moondream is available, that would be great.

I can do CLIP, BioCLIP, DINOv2 and ResNet50 for vision models.

How would I get this result as soon as possible?

# 02/11/2025

Some paper motivtion ideas from Claude:

Goal: Enable informed data collection strategies for ecological ML systems
Problem: Ecologists are willing to invest in data labeling, but lack clear guidance on whether additional labeling effort will translate to meaningful performance improvements
Solution: Empirical analysis of performance scaling curves for VLMs vs traditional ML approaches, revealing the point at which additional data collection yields diminishing returns

Goal: Optimize resource allocation in ecological ML projects
Problem: Projects must decide upfront whether to invest in (a) extensive data collection and traditional ML or (b) minimal data collection and VLMs, but lack empirical guidance for this decision
Solution: Characterization of performance vs sample size curves across multiple ecological tasks, enabling data-driven decisions about collection strategies

Goal: Bridge the "medium-data" gap in ecological ML
Problem: Current literature focuses on few-shot (1-5 samples) or large-data (10000+ samples) regimes, while many ecological projects operate in the 100-1000 sample range
Solution: Targeted analysis of the critical "medium-data" regime where VLM and traditional ML approaches may cross over in performance

Goal: Create sustainable ecological ML pipelines
Problem: Projects often start with few samples but accumulate more over time, making it unclear whether to invest in VLMs (good initial performance) or traditional ML (better scaling)
Solution: Framework for understanding when and how to transition between approaches as more data becomes available

Goal: Align ML approaches with ecological data collection realities
Problem: Standard few-shot learning research assumes fixed, small sample sizes, while ecological projects often have gradually expanding datasets
Solution: Analysis of how different approaches scale with increasing data, matching real-world ecological data collection patterns

# 02/12/2025

Gemini Flash 1.5 8B spent ~13c on Ages with no examples.
Now I'm trying with 1 training example.

# 02/18/2025

When I provide examples in the user/assistant/user/assistant format (mutiple messages), then the model responds with a classification for each example in the history.

Here is the JSON:

```json
[
  {
    "role": "user",
    "content": [
      {
        "type": "image_url",
        "image_url": {
          "url": "<BASE64>"
        }
      },
      {
        "type": "text",
        "text": "What is this a picture of, western sandpiper, whimbrel, Cooper's hawk, black-bellied plover, sharp-shinned hawk, dunlin, sanderling, Swainson's hawk, semipalmated plover, rough-legged hawk, least sandpiper or bald eagle? Respond with your answer in bold."
      }
    ]
  },
  {
    "role": "assistant",
    "content": "**semipalmated plover**"
  },
  {
    "role": "user",
    "content": [
      {
        "type": "image_url",
        "image_url": {
          "url": "<BASE64>"
        }
      },
      {
        "type": "text",
        "text": "What is this a picture of, least sandpiper, sanderling, whimbrel, dunlin, semipalmated plover, Swainson's hawk, rough-legged hawk, bald eagle, western sandpiper, Cooper's hawk, black-bellied plover or sharp-shinned hawk? Respond with your answer in bold."
      }
    ]
  },
  {
    "role": "assistant",
    "content": "**Cooper's hawk**"
  },
  {
    "role": "user",
    "content": [
      {
        "type": "image_url",
        "image_url": {
          "url": "<BASE64>"
        }
      },
      {
        "type": "text",
        "text": "What is this a picture of, whimbrel, black-bellied plover, least sandpiper, bald eagle, Swainson's hawk, sharp-shinned hawk, dunlin, sanderling, Cooper's hawk, semipalmated plover, rough-legged hawk or western sandpiper? Respond with your answer in bold."
      }
    ]
  },
  {
    "role": "assistant",
    "content": "**bald eagle**"
  },
  {
    "role": "user",
    "content": [
      {
        "type": "image_url",
        "image_url": {
          "url": "<BASE64>"
        }
      },
      {
        "type": "text",
        "text": "What is this a picture of, Swainson's hawk, bald eagle, western sandpiper, whimbrel, black-bellied plover, least sandpiper, semipalmated plover, rough-legged hawk, sanderling, dunlin, sharp-shinned hawk or Cooper's hawk? Respond with your answer in bold."
      }
    ]
  }
]
```

And the model responds with

`"From top to bottom, left to right:\n\n1. **Whimbrel**\n2. **Cooper's hawk**\n3. **Bald eagle**\n4. **Sharp-shinned hawk**"`

# 02/19/2025

[pyinstrument](https://pyinstrument.readthedocs.io/en/latest/guide.html) worked really well for profiling.

Some thoughts on the research questions:

This is sort of a guide for how to apply MLLMs to ai4ecology tasks.
Some of the questions I've had:

* Should I label data? This is the overarching question.
* Do MLLMs do better with more data (few-shot prompting)?
  * Do MLLMs need to be a certain size to leverage more examples?
* Do ViT+kNNs do better?
* How should you include multiple images in MLLM prompts (multiple turns or one message)?

One thing is that I'm not sure how to frame this as a *research* paper.
This is a technical report or a tutorial or a blog post.
But what is the core research question being asked?
Such a question should be timeless to some degree.
Whether to put multiple images in the same message or across multiple turns depends on the current set of MLLMs.
This is likely to change as we continue making better models.
What's the core question?


Timeless Research Framing:

Instead of asking:

    "Do MLLMs outperform traditional ML?"
    → (Too dependent on current model architectures)
    "How do MLLMs scale with data in ecological tasks?"
    → (Still somewhat tied to specifics)

Consider asking:
"What are the fundamental trade-offs between MLLMs and traditional ML for ecological data collection?"

    This frames the problem around trade-offs rather than absolute comparisons, which are bound to change.
    It ensures relevance even as models evolve.
    It guides ecological practitioners on when to choose one approach over another, rather than just evaluating models.

Framing the Contributions as Lasting Principles

Rather than reporting on absolute performance metrics (which will become outdated), the paper could:

    Characterize key factors that determine performance scaling
        E.g., when does task complexity demand more supervision?
        How does domain shift impact MLLMs vs. traditional ML?
        What types of ecological tasks benefit from VLMs (e.g., general object recognition vs fine-grained species classification)?

    Propose a decision-making framework
        How should an ecologist decide between investing in more labeled data vs using an MLLM?
        What characteristics of a dataset (e.g., class imbalance, domain diversity, image quality) affect whether few-shot methods will work?

    Identify inflection points
        Rather than saying "MLLMs need X samples to work," identify the conditions under which adding more data stops improving performance significantly.

Example Hypothesis-Driven Approach

A lasting insight could be testing a hypothesis that generalizes beyond current models, such as:

📌 "Vision-language models rely on semantic generalization, while traditional ML relies on statistical optimization—how does this affect their sample efficiency in ecological tasks?"

    This abstracts away specific model architectures and instead frames the comparison in terms of two fundamental learning paradigms:
        MLLMs (leveraging pre-trained world knowledge, generalizing with few examples)
        Traditional ML (learning purely from the data distribution, scaling better with more labels)
    This would help researchers and practitioners understand the strengths and weaknesses of different approaches even as new models emerge.

Takeaways

If we focus too much on specifics (e.g., "Does GPT-4V outperform a ViT?"), the work will quickly become outdated. Instead:

    Focus on trade-offs rather than head-to-head comparisons.
    Analyze patterns in data efficiency, not just raw performance numbers.
    Frame findings in a way that applies to future models.

Would this kind of framing align with what you're aiming for?



# CVPR Experiment Implementation Checklist

## 1. Models
- [ ] **MLLMs (API-based inference):**
  - [ ] Gemini Flash 2.0
  - [ ] Gemini Flash 1.5 8B
  - [ ] GPT-4o-mini
  - [ ] GPT-4o
  - [ ] Sonnet 3.5
  - [ ] Qwen2-VL 7B
  - [ ] Haiku 3.5
  - [ ] Qwen2-VL 72B
  - [ ] Llama 3.2 11B Vision
  - [ ] Llama 3.2 90B  
  - [ ] **API Settings:** Temperature = 0, batched API calls where possible  

- [ ] **Vision Encoders (Feature Extraction):**
  - [ ] DINOv2 (ViT-B, ViT-L, ViT-H)
  - [ ] CLIP (ViT-B, ViT-L, ViT-H)
  - [ ] SigLIP (ViT-B, ViT-L, ViT-H)
  - [ ] ResNet50 (ImageNet Pretrained)
  - [ ] BioCLIP  

- [ ] **ML Classifiers:**
  - [ ] kNN ($k$ autotuned via GridSearchCV)
  - [ ] SVM (RBF kernel, default hyperparams)
  - [ ] Ridge Classifier (regularization $\alpha=1.0$)

## 2. Datasets & Tasks
- [ ] **Species Classification:**
  - [ ] Birds525
  - [ ] iNat2021
  - [ ] PlantNet
- [ ] **Domain Adaptation:**
  - [ ] Plankton (microscope imagery)
  - [ ] iWildcam (camera trap images)
- [ ] **Functional Trait Prediction:**
  - [ ] FishNet
- [ ] **Generalization:**
  - [ ] Ages (Adult train → Juvenile test)
- [ ] **Multi-task Generalization:**
  - [ ] NeWT Benchmark

## 3. Data Preprocessing
- [ ] Resize images: **Smaller side = 224 px, then center crop to 224×224**
- [ ] Normalize images using **ImageNet mean/std**
- [ ] No augmentations applied (e.g., cropping, flipping)
- [ ] No image modifications before MLLM inference

## 4. Few-shot Sampling & Data Regimes
- [ ] Train subsets: **1, 3, 10, 30, 100, 300, 1000, 3000, 10,000** samples
- [ ] Uniform random sampling from training data
- [ ] **PlantNet & Long-tail datasets:** Secondary experiment with class-balanced sampling
- [ ] Fixed random seed (\texttt{42}) for reproducibility

## 5. MLLM Prompting Strategies
- [ ] **Single-turn prompting:** All few-shot examples + query in a single message
- [ ] **Multi-turn prompting:** Few-shot examples provided sequentially before query
- [ ] **Minimal text prompts:** (e.g., "Classify this species")
- [ ] **CoT variant:** "Think step by step first" appended to prompt (not in few-shot examples)

## 6. MLLM Response Parsing
- [ ] Deterministic regex-based extraction
- [ ] Take the **first species mentioned** if multiple species are listed
- [ ] Track **successful parses** as a function of few-shot examples

## 7. Evaluation Metrics
- [ ] Accuracy@1, Accuracy@5 (micro/macro) for classification tasks
- [ ] Domain adaptation: Accuracy drop (iWildCam, Plankton)
- [ ] Generalization gap (Ages task)
- [ ] FishNet: Mean Squared Error (MSE)
- [ ] **Compute bootstrapped confidence intervals:**
  - [ ] Resample test set **500 times with replacement**
  - [ ] Compute mean metric for each resample
  - [ ] Report **95\% confidence interval**

## 8. Compute Infrastructure
- [ ] **Hardware:**
  - [ ] Traditional ML models trained on NVIDIA A6000 GPUs
  - [ ] ViT-based inference batched for efficiency
  - [ ] MLLM inference run on cloud-based APIs
- [ ] **Parallelization:**
  - [ ] Vision encoder inference parallelized across GPUs
  - [ ] API queries batched where possible

---

## **Final Checks Before Running Experiments**
- [ ] Confirm all models are accessible (API keys, dependencies, etc.)
- [ ] Ensure dataset splits and sampling methods are correctly implemented
- [ ] Validate MLLM output parsing pipeline
- [ ] Test bootstrapping method with a small dataset before full-scale evaluation
- [ ] Set up logging to track MLLM responses, successful parses, and failure cases

---

**Once all boxes are checked, you’re ready to launch experiments!**


I want to summarize the project goals, progress, and next steps.
I need to have a short (2-sentence) and a medium (2-paragraph) description.
A long description is available in this logbook.md and in the paper.

Hi all,

TL;DR: I'm comparing the sample efficiency of MLLMs and CV+ML methods using a suite of biology-related computer vision tasks (https://github.com/samuelstevens/biobench) and making recommendations to practioners in the field.

Why you're receiving this email: I have talked about this project a bunch to many people and I wanted to provide an update. If you aren't interested, just reply saying you don't want to hear about this anymore.

Details: I am evaluating the performance scaling of multimodal large language models (MLLMs) versus traditional vision encoders with machine learning classifiers across diverse ecological tasks, including species classification, domain adaptation, and functional trait prediction using BioBench (https://github.com/samuelstevens/biobench). I am comparing MLLMs (e.g., GPT-4o, Gemini, Qwen2-VL) and vision models (DINOv2, CLIP, SigLIP) under varying data regimes (1–10K samples), prompting strategies (single-turn, multi-turn, CoT), and sampling methods (uniform vs. class-balanced) to determine when additional labeled data improves performance, when MLLMs can compensate for data scarcity, and how prompting choices impact reliability. I expect to find that MLLM performance "saturates" with fewer samples compared to CV+ML methods, and will describe (hopefully) lasting insights based on these empirical findings.

Goals:

1. CV4Animals 4-page submission due March 28th
2. Public codebase and results shortly thereafter

Progress:

1. I have a codebase to evaluate CV+ML methods that is fairly battle-tested.
2. I am developing the MLLMs evaluation methods now.
3. After ICCV (March 7th) I am going to run many experiments in parallel, leveraging API-based MLLMs and local GPUs (OSU, OSC) for CV+ML methods.
4. I have an Overleaf draft with methodological details and a rough outline already. I will continue writing as I get experimental results.

How you can help (if you want):

* Doing analysis. Once I have some experimental results (hopefully March 14th at the latest) then I will want to ask why particular tasks are better/worse, why particular ML methods are better/worse, why particular MLLMs are better/worse, etc. This will need some time with notebooks/scripts to make graphs and tables.
* Writing. The core insight and message is not super clear, even though I feel their is high value in knowing the answer to these questions. Writing the introduction and conclusion would be great.
* Ideas. I would love to hear ideas on models, experiments, tasks or other parts that you are happy to share.

I'm explicitly not asking for help with implementation because I currently wrote the entire codebase  and can hold it all in my head. In combination with AI tool support, I feel that adding additional people to the experiments will slow me down.

I think analysis and meaningful writing will warrant authorship, and offhand ideas warrant acknowledgements. If you feel differently, I am happy to discuss further. Right now, I am first author and Jenna is second author.

Again, if you're not interested, feel free to let me know and I'll take you off this "mailing list"; I plan on sending an update every couple weeks until the CV4Animals deadline.

Best,
Sam

# 02/20/2025

I need to run some experiments with MLLMs and get some progress.

Ages: 1800 test samples (600 x 3)
Plankton: 151K
Newt: 16.4K
iWildcam: 42.7K

Let's just stick with --debug using only 100 random samples.
This is enough for me to notice meaningful differences in prompting strategies (and bugs).

Right now, both models are returning nothing.
(I fixed this by changing the 'role' parameter)

I would like to see one comparison so far:

With 100 randomly sampled examples from `ages`, compare:

* Gemini 1.5 Flash 8B
* Qwen2-VL-7B
* Llama3.2 11B
* GPT-4o-mini

With 0, 1, 3, and 10 samples, multi and single-turn prompts.
I want to know a couple things:

1. How many examples actually fit into their prompt
2. Accuracy (obviously)

Hopefully we will see

1. GPT-4o-mini > Gemini > Qwen > Llama
2. Performance goes up with samples
3. Single > multi

# 02/24/2025

Ages is such a dumb task, it explicitly is a train/test mismatch.
Without any training samples, obviously you do better.

What about FishNet?

FishNet worked! More samples is better.
Now I want to track

1. How many examples actually fit into the prompt.
2. I want to save a sample of the MLLM responses
3. Cost per response

And I need to try multi-turn prompts as well.
Then just show me the freaking graphs baby!

# 04/23/2025

Good outline:

Do we need benchmarks? -> Most AI/ML researchers agree yes. There are good studies on the history of bechmark, shared tasks, etc.

Are current benchmarks good enough? -> Not anymore. I show that this is true with the statistical graphs. I give an explanation why by showing images in ImageNet-1K, MS-COCO, ADE20K, iNat21 and the tasks in BioBench.

How should we build new benchmarks?
* Engineeering why: benchmarks should help us answer new questions about models, goldilocks zone, etc.
* Science why: benchmarks should drive progress outside of the benchmark task itself.

>>>> mammalnet/__init__.py
"""
# MammalNet

We frame behavior recognition as a classification task.
Given a short video segment, embed the video via some frame-sampling strategy and associate that embedding with a label.
We train a simple nearest-centroid classifier [which works well with few-shot tasks](https://arxiv.org/abs/1911.04623) over these representation-label pairs.

You must use torchcodec 0.2 with torch 2.6.
If you have torch 2.7, then use torchcodec 0.3.

If you use this benchmark, please cite the original work:

```
@inproceedings{chen2023mammalnet,
  title={Mammalnet: A large-scale video benchmark for mammal recognition and behavior understanding},
  author={Chen, Jun and Hu, Ming and Coker, Darren J and Berumen, Michael L and Costelloe, Blair and Beery, Sara and Rohrbach, Anna and Elhoseiny, Mohamed},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13052--13061},
  year={2023}
}
```
"""

import csv
import dataclasses
import logging
import os.path

import beartype
import numpy as np
import torch
import torchcodec.decoders
from jaxtyping import Float16, Int, Shaped, jaxtyped
from torch import Tensor

from .. import config, helpers, registry, reporting, simpleshot

logger = logging.getLogger(__name__)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> tuple[config.Model, reporting.Report]:
    # 1. Load model
    backbone = registry.load_vision_backbone(cfg.model)
    backbone = backbone.to(cfg.device)

    # 2. Load data.
    test_features = get_features(cfg, backbone, is_train=False)
    train_features = get_features(cfg, backbone, is_train=True)

    # 4. Do simpleshot.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    # Return benchmark report.
    preds = [
        reporting.Prediction(
            str(video_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for video_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]
    return reporting.Report("mammalnet", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    return simpleshot.SimpleShotClassifier(device="cuda:0")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float16[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    """
    Gets all model features and true labels for all frames and all examples in the dataloader.

    Returns it as a pair of big tensors; other tasks use a dedicated class for this, but here it's just a tuple.

    Args:
        args: KABR task arguments.
        backbone: Vision backbone.
        is_train: Whether it's training data or not.

    Returns:
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone)
    split = "train" if is_train else "val"

    dataset = Dataset(
        cfg.data.mammalnet, split=split, seed=cfg.seed, transform=img_transform
    )
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=True,
    )

    all_feats, all_labels, all_ids = [], [], []

    def probe(batch):
        with torch.amp.autocast(cfg.device):
            frames, _, _ = batch
            frames = frames.to(cfg.device, non_blocking=True)
            # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
            n_frames, bsz, c, h, w = frames.shape
            frames = frames.view(bsz * n_frames, c, h, w)
            outputs = backbone.img_encode(frames)
            features = outputs.img_features.view(n_frames, bsz, -1)
            features = aggregate_frames(features)

    with helpers.auto_batch_size(dataloader, probe=probe, backoff=1):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc=f"mammalnet/{split}"):
            with torch.amp.autocast(cfg.device):
                frames, labels, ids = next(it)
                frames = frames.to(cfg.device, non_blocking=True)
                # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
                bsz, n_frames, c, h, w = frames.shape
                frames = frames.view(bsz * n_frames, c, h, w)
                outputs = backbone.img_encode(frames)
                features = outputs.img_features.view(bsz, n_frames, -1)

                features = aggregate_frames(features)
                all_feats.append(features.cpu())

            all_labels.append(labels.cpu())
            all_ids.extend(ids)

    all_feats = torch.cat(all_feats, dim=0).cpu().numpy()
    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()
    all_ids = np.array(all_ids)

    return Features(all_feats, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
class Dataset:
    n_frames: int = 32
    composition: str = "composition"

    def __init__(self, root: str, *, split: str, seed: int, transform):
        import torchvision.transforms.v2.functional

        self.root = root
        self.split = split
        self.transform = transform
        self.to_pil_image = torchvision.transforms.v2.functional.to_pil_image

        if not os.path.exists(self.root) or not os.path.isdir(self.root):
            msg = f"Path '{self.root}' doesn't exist. Did you download the MammalNet dataset?"
            raise RuntimeError(msg)

        self.samples = []
        with open(
            os.path.join(self.root, "annotation", self.composition, f"{self.split}.csv")
        ) as fd:
            reader = csv.reader(fd, delimiter=" ")
            for rel_path, species_id, behavior_id in reader:
                # the CSV already prefixes "trimmed_videos/..."
                full_path = os.path.join(self.root, *rel_path.split("/"))
                if not os.path.isfile(full_path):
                    logger.warn("Missing clip '%s'; skipping", full_path)
                    continue

                vid_id, ext = os.path.splitext(os.path.basename(full_path))

                self.samples.append({
                    "path": full_path,
                    "id": vid_id,
                    "species": int(species_id),
                    "behavior": int(behavior_id),
                })

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, i) -> tuple[Float16[Tensor, "n 3 width height"], int, str]:
        item = self.samples[i]
        decoder = torchcodec.decoders.VideoDecoder(item["path"])

        frames = []
        fs = torch.linspace(0, len(decoder) - 1, self.n_frames, dtype=int).tolist()
        for frame in decoder.get_frames_at(indices=fs).data:
            frames.append(self.transform(self.to_pil_image(frame, "RGB")))

        return torch.stack(frames), item["behavior"], item["id"]


@jaxtyped(typechecker=beartype.beartype)
def aggregate_frames(
    features: Float16[Tensor, "n_frames n_examples dim"],
) -> Float16[Tensor, "n_examples dim"]:
    return torch.max(features, dim=0).values

>>>> mammalnet/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "beartype",
#     "ffmpeg-python",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the MammalNet benchmark and its annotations.
"""

import concurrent.futures
import dataclasses
import json
import logging
import pathlib
import statistics
import tarfile

import beartype
import ffmpeg
import requests
import tqdm
import tyro

VIDEOS_URL = "https://mammalnet.s3.amazonaws.com/full_video.tar.gz"
ANNOTATIONS_URL = "https://mammalnet.s3.amazonaws.com/annotation.tar"

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("mammalnet.download")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """Where to save the downloaded archives and (optionally) extract them."""
    chunk_size_kb: int = 1024
    """Download chunk size (KB). 1024 KB ~ 1 MB."""
    download_videos: bool = True
    """Whether to download the video archive."""
    download_annotations: bool = True
    """Whether to download the annotation archive."""
    trim_videos: bool = True
    """Whether to create trimmed video clips based on annotations."""
    check_stats: bool = True
    n_workers: int = 16
    """Number of parallel `ffmpeg`s to spawn."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Annotation:
    label: str
    """The class label for this annotation segment."""
    start_s: float
    end_s: float

    @property
    def duration_s(self) -> float:
        return self.end_s - self.start_s


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Detection:
    id: str
    """Unique identifier for the video clip."""
    taxonomy: list[dict[str, str]]
    """Taxonomic classification information for the detected animal."""
    annotations: list[Annotation]
    """List of time segments with behavior annotations."""
    duration_s: int
    """Total duration of the video in seconds."""
    resolution: tuple[int, int]
    """Video resolution in pixels"""
    fps: int
    """Frames per second of the video."""
    subset: str
    """Dataset split this video belongs to (e.g., 'train', 'val', 'test')."""
    url: str
    """Original source URL for the video."""

    @classmethod
    def from_json(cls, id, dct):
        annotations = [
            Annotation(
                label=ann["label"],
                start_s=float(ann["segment"][0]),
                end_s=float(ann["segment"][1]),
            )
            for ann in dct.pop("annotations")
        ]
        taxonomy = dct.pop("taxnomy")
        duration_s = dct.pop("duration")
        resolution = tuple(int(x) for x in dct.pop("resolution").split("x"))
        return cls(
            id=id,
            taxonomy=taxonomy,
            annotations=annotations,
            duration_s=duration_s,
            resolution=resolution,
            **dct,
        )


@beartype.beartype
def _download(url: str, dest: pathlib.Path, chunk_bytes: int) -> pathlib.Path:
    dest.parent.mkdir(parents=True, exist_ok=True)
    if dest.exists():
        print(f"{dest.name} already present, skipping download.")
        return dest

    r = requests.get(url, stream=True)
    r.raise_for_status()
    total = int(r.headers.get("content-length", 0))

    with (
        dest.open("wb") as f,
        tqdm.tqdm(
            total=total, unit="B", unit_scale=True, desc=f"Downloading {dest.name}"
        ) as bar,
    ):
        for chunk in r.iter_content(chunk_size=chunk_bytes):
            f.write(chunk)
            bar.update(len(chunk))
    return dest


@beartype.beartype
def _extract(archive: pathlib.Path, out_dir: pathlib.Path) -> None:
    with tarfile.open(archive, "r:*") as tar:
        for member in tqdm.tqdm(tar, desc=f"Extracting {archive.name}"):
            tar.extract(member, path=out_dir)


@beartype.beartype
def _probe(path: pathlib.Path) -> float:
    out = ffmpeg.probe(str(path))
    return float(out["format"]["duration"])


@beartype.beartype
def _load_detections(base: pathlib.Path) -> list[Detection]:
    with open(base / "annotation" / "detection_annotations.json") as fd:
        detections = [
            Detection.from_json(key, value) for key, value in json.load(fd).items()
        ]

    return detections


@beartype.beartype
def _stats(base: pathlib.Path, n_workers: int = 8):
    detections = _load_detections(base)

    ###############
    # Full videos #
    ###############

    from_json = [det.duration_s for det in detections]
    mean_s_from_json = statistics.mean(from_json)

    vids = list(
        p for p in (base / "full_videos").iterdir() if p.suffix.lower() == ".mp4"
    )
    durations = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_probe, vid) for vid in vids]
        for fut in tqdm.tqdm(
            concurrent.futures.as_completed(futs),
            total=len(futs),
            desc="full video durations",
        ):
            durations.append(fut.result())
    mean_s_from_disk = statistics.mean(durations)

    print("From paper:")
    print(f"  Mean (s) : {106:6.1f}")
    print("From detection_annotations.json:")
    print(f"  Mean (s) : {mean_s_from_json:6.1f}")
    print(f"From {base / 'full_videos'}:")
    print(f"  Mean (s) : {mean_s_from_disk:6.1f}")

    ##################
    # Trimmed videos #
    ##################

    # Calculate expected durations from annotations
    from_json = [ann.duration_s for det in detections for ann in det.annotations]
    mean_s_from_json = statistics.mean(from_json)

    vids = list(
        p for p in (base / "trimmed_videos").iterdir() if p.suffix.lower() == ".mp4"
    )
    durations = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_probe, vid) for vid in vids]
        for fut in tqdm.tqdm(
            concurrent.futures.as_completed(futs),
            total=len(futs),
            desc="trimmed video durations",
        ):
            if err := fut.exception():
                logger.warning("Exception: %s", err)
                continue
            durations.append(fut.result())
    mean_s_from_disk = statistics.mean(durations)

    print("From paper:")
    print(f"  Mean (s) : {77:6.1f}")
    print("From detection_annotations.json:")
    print(f"  Mean (s) : {mean_s_from_json:6.1f}")
    print(f"From {base / 'trimmed_videos'}:")
    print(f"  Mean (s) : {mean_s_from_disk:6.1f}")


@beartype.beartype
def _trim(src: pathlib.Path, dst: pathlib.Path, start_s: float, end_s: float):
    """Copy-trim [start_s, end_s] from *src* into *dst* without re-encoding."""
    if start_s >= end_s:
        raise ValueError("start_s must be < end_s")

    duration = end_s - start_s
    (
        ffmpeg
        # fast seek to ~start (input-side -ss is key-frame aligned, faster)
        .input(str(src), ss=start_s)
        # output-side -t gives exact length; libx264/AAC re-encodes safely
        .output(
            str(dst),
            t=duration,
            vcodec="libx264",
            crf=23,
            preset="veryfast",
            movflags="+faststart",  # web-friendly moov placement
            loglevel="error",  # silence ffmpeg spam unless errors
            **{"an": None},  # <- -an  (strip audio)
        )
        .overwrite_output()
        .run()
    )


@beartype.beartype
def _trim_all(base: pathlib.Path, n_workers: int):
    (base / "trimmed_videos").mkdir(exist_ok=True)
    jobs = []
    with open(base / "annotation" / "detection_annotations.json") as fd:
        for key, value in json.load(fd).items():
            det = Detection.from_json(key, value)

            src = base / "full_videos" / f"{det.id}.mp4"
            if len(det.annotations) > 1:
                for k, ann in enumerate(det.annotations):
                    dst = base / "trimmed_videos" / f"{det.id}_{k + 1}.mp4"
                    jobs.append((src, dst, ann.start_s, ann.end_s))
            else:
                ann = det.annotations[0]
                dst = base / "trimmed_videos" / f"{det.id}.mp4"
                jobs.append((src, dst, ann.start_s, ann.end_s))

    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_trim, *job) for job in jobs]
        for fut in tqdm.tqdm(concurrent.futures.as_completed(futs), total=len(futs)):
            fut.result()  # re-raise on failure


@beartype.beartype
def main(args: Args) -> None:
    base = pathlib.Path(args.dir).expanduser().resolve()
    chunk = args.chunk_size_kb * 1024

    if args.download_videos:
        target = base / pathlib.Path(VIDEOS_URL).name
        archive = _download(VIDEOS_URL, target, chunk)

        _extract(archive, base)
        print(f"Extracted videos into {base}")

    if args.download_annotations:
        target = base / pathlib.Path(ANNOTATIONS_URL).name
        archive = _download(ANNOTATIONS_URL, target, chunk)

        _extract(archive, base)
        print(f"Extracted annotations into {base}")

    if args.trim_videos:
        _trim_all(base, args.n_workers)

    if args.check_stats:
        _stats(base, args.n_workers)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> newt/__init__.py
"""
# NeWT: Natural World Tasks

NeWT is a collection of 164 binary classification tasks related to visual understanding of the natural world ([CVPR 2021 paper](https://arxiv.org/abs/2103.16483), [code](https://github.com/visipedia/newt/tree/main)).

We evaluate a vision model by extracting visual features for each image, fitting a linear SVM to the training examples, and evaluating on the test data.
We aggregate scores across all 164 tasks.

If you use this evaluation, be sure to cite the original work:

```
@inproceedings{van2021benchmarking,
  title={Benchmarking Representation Learning for Natural World Image Collections},
  author={Van Horn, Grant and Cole, Elijah and Beery, Sara and Wilber, Kimberly and Belongie, Serge and Mac Aodha, Oisin},
  booktitle={Computer Vision and Pattern Recognition},
  year={2021}
}
```
"""

import collections.abc
import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import scipy.stats
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.svm
import torch
from jaxtyping import Bool, Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("newt")


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    The NeWT benchmark.
    First, get features for all images.
    Second, select the subsets of features that correspond to different tasks and train an SVM.
    Third, evaluate the SVM and report results.
    """

    # Fit SVMs.
    all_preds = []
    for task in get_all_tasks(cfg):
        (x_train, y_train), (x_test, y_test) = task.splits

        x_mean = x_train.mean(axis=0, keepdims=True)

        x_train = x_train - x_mean
        x_train = l2_normalize(x_train)

        x_test = x_test - x_mean
        x_test = l2_normalize(x_test)

        svc = init_svc(cfg.n_train)

        svc.fit(x_train, y_train)
        y_pred = svc.predict(x_test)
        info = {
            "task": task.name,
            "cluster": task.cluster,
            "subcluster": task.subcluster,
        }
        preds = [
            reporting.Prediction(str(id), float(pred == true), info)
            for id, pred, true in zip(task.example_ids, y_pred, y_test)
        ]

        all_preds.extend(preds)

    return reporting.Report("newt", all_preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return np.mean([p.score for p in preds]).item()


@jaxtyped(typechecker=beartype.beartype)
class Sample(typing.TypedDict):
    """A dictionary representing a single image sample with its metadata.

    Attributes:
        img_id: Unique identifier for the image.
        img: The image tensor with shape [3, width, height] (RGB channels first).
        label: Binary class label (0 or 1) for the image.
    """

    img_id: str
    img: Float[Tensor, "3 width height"]
    label: Int[Tensor, ""]


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """A dataset that returns ImageSample dictionaries."""

    def __init__(
        self,
        root: str,
        img_ids: Shaped[np.ndarray, " n"],
        labels: Int[np.ndarray, " n"],
        transform=None,
    ):
        """Initialize the dataset with image paths and labels.

        Args:
            root: Root directory containing the images.
            img_ids: Array of image IDs.
            labels: Array of binary labels corresponding to the images.
            transform: Optional transform to apply to the images.
        """
        self.transform = transform
        self.root = root
        self.img_ids = img_ids
        self.labels = labels

    def __getitem__(self, i: int) -> Sample:
        """Get a sample by its index.

        Args:
            i: Index of the sample to retrieve.

        Returns:
            A dictionary containing the image ID, image tensor, and label.
        """
        img_id = self.img_ids[i]
        img = Image.open(os.path.join(self.root, f"{img_id}.jpg"))
        if self.transform is not None:
            img = self.transform(img)
        label = self.labels[i]
        return {"img_id": img_id, "img": img, "label": label}

    def __len__(self) -> int:
        """Return the number of samples in the dataset.

        Returns:
            The number of samples.
        """
        return len(self.img_ids)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Task:
    """
    Task is a group of features and labels for an SVM + a train/test split.
    """

    name: str
    cluster: str
    subcluster: str | None
    features: Float[np.ndarray, "batch dim"]
    labels: Int[np.ndarray, " batch"]
    is_train: Bool[np.ndarray, " batch"]
    example_ids: Shaped[np.ndarray, " batch"]  # Should be String[...]

    def __repr__(self) -> str:
        return f"Task(task={self.name}, cluster={self.cluster}, features={self.features.shape})"

    @property
    def splits(
        self,
    ) -> tuple[
        tuple[Float[np.ndarray, "n_train dim"], Int[np.ndarray, " n_train"]],
        tuple[Float[np.ndarray, "n_test dim"], Int[np.ndarray, " n_test"]],
    ]:
        """
        The features and labels for train and test splits.

        Returned as `(x_train, y_train), (x_test, y_test)`.
        """
        x_train = self.features[self.is_train]
        y_train = self.labels[self.is_train]
        x_test = self.features[~self.is_train]
        y_test = self.labels[~self.is_train]

        return (x_train, y_train), (x_test, y_test)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_all_tasks(cfg: config.Experiment) -> collections.abc.Iterator[Task]:
    """ """
    rng = np.random.default_rng(seed=cfg.seed)

    # Load model
    backbone = registry.load_vision_backbone(cfg.model)
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(cfg.data.newt, labels_csv_name)
    imgs_dir_name = "newt2021_images"
    imgs_dir_path = os.path.join(cfg.data.newt, imgs_dir_name)

    if not os.path.isfile(labels_csv_path):
        msg = f"Path '{labels_csv_path}' doesn't exist. Did you download the Newt dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--data'; see --help for more."
        raise RuntimeError(msg)

    # Read the CSV and add row indices
    df = pl.read_csv(labels_csv_path).with_row_index(name="original_index")

    # Sample balanced training data for each task
    df = sample(rng, df, cfg.n_train).with_row_index(name="sampled_index")

    # Get all image IDs and labels
    all_data = df.select("id", "label").to_numpy(structured=True)
    all_ids, all_labels = all_data["id"], all_data["label"]

    # Create dataset with all samples
    dataset = Dataset(
        imgs_dir_path,
        all_ids,
        all_labels,
        img_transform,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    def probe(batch):
        imgs = batch["img"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features  # forward only

    all_features, all_ids = [], []

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc="newt"):
            batch = next(it)
            imgs = batch["img"].to(cfg.device)

            with torch.amp.autocast("cuda"):
                features = backbone.img_encode(imgs).img_features
                features = torch.nn.functional.normalize(features, dim=-1)
                all_features.append(features.cpu())

            all_ids.extend(batch["img_id"])

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)

    for task in df.get_column("task").unique():
        task_df = df.filter(pl.col("task") == task)

        task_idx = task_df.get_column("sampled_index").to_numpy()
        features = all_features[task_idx].numpy()
        ids = all_ids[task_idx]

        labels = task_df.get_column("label").to_numpy()
        is_train = task_df.select(pl.col("split") == "train").get_column("split")

        cluster = task_df.item(row=0, column="task_cluster")
        subcluster = task_df.item(row=0, column="task_subcluster")
        yield Task(
            task, cluster, subcluster, features, labels, is_train.to_numpy(), ids
        )


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[np.ndarray, "batch dim"],
) -> Float[np.ndarray, "batch dim"]:
    """Normalizes a batch of vectors to have L2 unit norm."""
    norms = np.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


def init_svc(n_train: int):
    """Create a new, randomly initialized SVM with a random hyperparameter search over kernel, C and gamma. It uses only 16 jobs in parallel to prevent overloading the CPUs on a shared machine."""
    if n_train < 10:
        return sklearn.pipeline.make_pipeline(
            sklearn.svm.SVC(kernel="linear"),
        )

    return sklearn.model_selection.RandomizedSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.svm.SVC(C=1.0, kernel="rbf"),
        ),
        {
            "svc__C": scipy.stats.loguniform(a=1e-3, b=1e1),
            "svc__kernel": ["rbf", "linear", "sigmoid", "poly"],
            "svc__gamma": scipy.stats.loguniform(a=1e-4, b=1e-3),
        },
        n_iter=100,
        n_jobs=16,
        random_state=42,
    )


@jaxtyped(typechecker=beartype.beartype)
def sample(rng: np.random.Generator, df: pl.DataFrame, n_train: int) -> pl.DataFrame:
    """Sample a balanced subset of training data points for each task.

    Args:
        rng: Random number generator.
        df: NeWT dataframe.
        n_train: Number of training samples per task to return.

    Returns:
        A DataFrame with balanced training samples and all test samples.
    """
    if n_train <= 0:
        return df  # Return all data if n_train is not positive

    # Create a new dataframe to store the results
    result_dfs = []

    # Keep all test samples
    test_df = df.filter(pl.col("split") != "train")
    result_dfs.append(test_df)

    # Process each task separately
    for task in df.get_column("task").unique():
        task_df = df.filter((pl.col("task") == task) & (pl.col("split") == "train"))

        # Skip if the task has no training samples
        if task_df.height == 0:
            continue

        # Get samples for each class
        class0_df = task_df.filter(pl.col("label") == 0)
        class1_df = task_df.filter(pl.col("label") == 1)

        n0 = n_train // 2
        n1 = n_train - n0

        assert n0 > 0
        assert n1 > 0

        # Sample from each class
        if n0 < class0_df.height:
            indices0 = rng.choice(class0_df.height, size=n0, replace=False)
            result_dfs.append(
                class0_df.with_row_index(name="tmp")
                .filter(pl.col("tmp").is_in(indices0))
                .drop("tmp")
            )
        else:
            result_dfs.append(class0_df)

        if n1 < class1_df.height:
            indices1 = rng.choice(class1_df.height, size=n1, replace=False)
            result_dfs.append(
                class1_df.with_row_index(name="tmp")
                .filter(pl.col("tmp").is_in(indices1))
                .drop("tmp")
            )
        else:
            result_dfs.append(class1_df)

    # Combine all dataframes
    return pl.concat(result_dfs)

>>>> newt/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the NeWT dataset.

Run with:

1. `python biobench/newt/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.newt.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import tarfile

import requests
import tqdm
import tyro

images_url = (
    "https://ml-inat-competition-datasets.s3.amazonaws.com/newt/newt2021_images.tar.gz"
)
labels_url = "https://ml-inat-competition-datasets.s3.amazonaws.com/newt/newt2021_labels.csv.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    images: bool = True
    """Whether to download images [4.1GB]."""
    labels: bool = True
    """Whether to download labels."""


def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    labels_tar_path = os.path.join(args.dir, "labels.tar")
    images_tar_path = os.path.join(args.dir, "images.tar")
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.dir, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.dir, images_dir_name)

    if args.labels:
        # Download labels
        r = requests.get(labels_url, stream=True)
        r.raise_for_status()

        with open(labels_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
        print(f"Downloaded labels: {labels_tar_path}.")

    if args.images:
        # Download images.
        r = requests.get(images_url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(images_tar_path, "wb") as fd:
            for chunk in tqdm.tqdm(
                r.iter_content(chunk_size=chunk_size),
                total=n_bytes / chunk_size,
                unit="b",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading images",
            ):
                fd.write(chunk)
        print(f"Downloaded images: {images_tar_path}.")

    with tarfile.open(labels_tar_path, "r") as tar:
        tar.extract(labels_csv_name, path=args.dir, filter="data")
    print(f"Extracted labels: {labels_csv_path}.")

    with open(labels_csv_path) as fd:
        n_images = len(fd.read().split("\n")) - 1

    with tarfile.open(images_tar_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=n_images):
            tar.extract(member, path=args.dir, filter="data")
    print(f"Extracted images: {images_dir_path}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> openset.py
import beartype
import numpy as np
import scipy.stats
import sklearn.base
import sklearn.utils.validation


@beartype.beartype
class MahalanobisOpenSetClassifier(
    sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin
):
    """
    Wraps an arbitrary scikit-learn multiclass estimator with a Mahalanobis out-of-distribution detector.  Unknown samples are assigned `unknown_label`.

    @inproceedings{lee2018simple,
      title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
      author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
      year = 2018,
      booktitle = {Advances in Neural Information Processing Systems},
      publisher = {Curran Associates, Inc.},
      volume = 31,
      pages = {},
      url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf},
      editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
    }


    Parameters
    ----------
    base_estimator : scikit-learn estimator
        Must implement fit / predict (e.g. a Pipeline with a classifier).
    alpha : float, default=0.95
        Confidence level for the chi-squared cutoff; 1-alpha is the tail mass declared OOD.
    covariance_estimator : {"empirical", "ledoit"}, default="ledoit"
        Strategy for covariance. "ledoit" is shrinkage-robust and invertible.
    unknown_label : int | str, default=-1
        Label given to detections outside the known set.
    """

    def __init__(
        self,
        base_estimator,
        alpha: float = 0.95,
        covariance_estimator: str = "ledoit",
        unknown_label: int | str = -1,
    ):
        self.base_estimator = base_estimator
        self.alpha = alpha
        self.covariance_estimator = covariance_estimator
        self.unknown_label = unknown_label

    # ---------------- #
    # scikit-learn API #
    # ---------------- #
    def fit(self, X, y):
        X, y = sklearn.utils.validation.check_X_y(X, y, accept_sparse=False)
        self.classes_ = np.unique(y)

        # 1. fit the wrapped classifier
        self.clf_ = sklearn.base.clone(self.base_estimator).fit(X, y)

        # 2. compute per-class means
        self.means_ = np.vstack([X[y == c].mean(axis=0) for c in self.classes_])

        # 3. shared covariance
        if self.covariance_estimator == "ledoit":
            cov = sklearn.covariance.LedoitWolf().fit(X)
            self.Sigma_inv_ = cov.precision_
        elif self.covariance_estimator == "empirical":
            cov = np.cov(X, rowvar=False)
            self.Sigma_inv_ = np.linalg.pinv(cov)
        else:
            raise ValueError("covariance_estimator must be 'empirical' or 'ledoit'")

        # 4. analytic chi-squared cutoff
        d = X.shape[1]
        self.tau_ = scipy.stats.chi2.ppf(self.alpha, df=d)
        return self

    def predict(self, X):
        sklearn.utils.validation.check_is_fitted(self, "clf_")
        X = sklearn.utils.validation.check_array(X, accept_sparse=False)

        # Mahalanobis distance to nearest class mean
        d2 = self._min_mahala_sq(X)
        is_in = d2 <= self.tau_

        pred_known = self.clf_.predict(X)
        pred = np.where(is_in, pred_known, self.unknown_label)
        return pred

    def decision_function(self, X):
        """Negative min-Mahalanobis distance (higher = more in-dist)."""
        sklearn.utils.validation.check_is_fitted(self, "clf_")
        X = sklearn.utils.validation.check_array(X, accept_sparse=False)
        return -self._min_mahala_sq(X)

    # ------------------ #
    # internal utilities #
    # ------------------ #
    def _min_mahala_sq(self, X) -> np.ndarray:
        """
        Vectorised squared Mahalanobis distance to the *nearest* class mean.
        """
        diff = X[:, None, :] - self.means_  # (n, C, d)
        # (n, C) -> min over C -> (n,)
        d2 = np.einsum("ncd,dd,ncd->nc", diff, self.Sigma_inv_, diff).min(axis=1)
        return d2

>>>> plankton/__init__.py
"""
Classification of phytoplankton using ridge classifiers.
This task is particularly challenging because the image distribution is very different to typical pre-training datasets; it's all microscopic images in mono-channel (black and white).

If you use this task, please cite the original paper to propose this train/test split and the original datasets as well:

Paper:

```
@article{kaisa2022towards,
    author={Kraft, Kaisa  and Velhonoja, Otso  and Eerola, Tuomas  and Suikkanen, Sanna  and Tamminen, Timo  and Haraguchi, Lumi  and Ylöstalo, Pasi  and Kielosto, Sami  and Johansson, Milla  and Lensu, Lasse  and Kälviäinen, Heikki  and Haario, Heikki  and Seppälä, Jukka },
    title={Towards operational phytoplankton recognition with automated high-throughput imaging, near-real-time data processing, and convolutional neural networks},
    journal={Frontiers in Marine Science},
    volume={9},
    year={2022},
    url={https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2022.867695},
    doi={10.3389/fmars.2022.867695},
    issn={2296-7745},
}
```

Training data:

```
@misc{kaisa2022syke
    doi = {10.23728/B2SHARE.ABF913E5A6AD47E6BAA273AE0ED6617A},
    url = {https://b2share.eudat.eu/records/abf913e5a6ad47e6baa273ae0ed6617a},
    author = {Kraft, Kaisa and Velhonoja, Otso and Seppälä, Jukka and Hällfors, Heidi and Suikkanen, Sanna and Ylöstalo, Pasi and Anglès, Sílvia and Kielosto, Sami and Kuosa, Harri and Lehtinen, Sirpa and Oja, Johanna and Tamminen, Timo},
    keywords = {3.1.21 -> Biology -> Marine biology, phytoplankton image data set, imaging flow cytometry, Imaging FlowCytobot, IFCB, phytoplankton, Baltic Sea, image data, SYKE, Finnish Environment Institute, Marine Research Centre, Marine Ecological Research Laboratory, plankton image data, FINMARI},
    title = {SYKE-plankton_IFCB_2022},
    publisher = {https://b2share.eudat.eu},
    year = {2022},
    copyright = {open}
}
```

Evaluation data:

```
@misc{kaisa2021syke,
  doi = {10.23728/B2SHARE.7C273B6F409C47E98A868D6517BE3AE3},
  url = {https://b2share.eudat.eu/records/7c273b6f409c47e98a868d6517be3ae3},
  author = {Kraft, Kaisa and Haraguchi, Lumi and Velhonoja, Otso and Seppälä, Jukka},
  keywords = {3.1.21 -> Biology -> Marine biology, phytoplankton image data set, imaging flow cytometry, Imaging FlowCytobot, IFCB, Baltic Sea, image data, SYKE, Finnish Environment Institute, Marine Research Centre, Marine Ecological Research Laboratory, plankton image data, FINMARI, phytoplankton},
  title = {SYKE-plankton_IFCB_Utö_2021},
  publisher = {https://b2share.eudat.eu},
  year = {2022},
  copyright = {open}
}
```

This task was added because of interesting conversations with [Ekaterina Nepovinnykh](https://scholar.google.com/citations?user=lmYki4gAAAAJ) and [Heikki Kälviäinen](https://www.lut.fi/en/profiles/heikki-kalviainen).
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import sklearn.naive_bayes
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.svm
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("plankton")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using cross validation splits.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Get features
    train_features = get_features(cfg, backbone, is_train=True)
    val_features = get_features(cfg, backbone, is_train=False)

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    # 3. Predict.
    pred_labels = clf.predict(val_features.x)
    logger.info("Predicted classes for %d examples.", len(val_features.x))
    true_labels = val_features.y

    preds = [
        reporting.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(val_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("plankton", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@jaxtyped(typechecker=beartype.beartype)
class Sample(typing.TypedDict):
    """A dictionary representing a single image sample with its metadata.

    Attributes:
        img_id: Unique identifier for the image.
        img: The image tensor with shape [3, width, height] (RGB channels first).
        label: Binary class label (0 or 1) for the image.
    """

    img_id: str
    img: Float[Tensor, "3 width height"]
    label: Int[Tensor, ""]


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    transform: typing.Any | None
    """Optional function function that transforms an image into a format expected by a neural network."""
    samples: list[tuple[str, str, str]]
    """List of all image ids, image paths, and classnames."""

    def __init__(self, root: str, transform):
        self.transform = transform
        self.samples = []
        if not os.path.exists(root) or not os.path.isdir(root):
            msg = f"Path '{root}' doesn't exist. Did you download the plankton dataset? See the docstring at the top of this file for instructions."
            raise RuntimeError(msg)

        class_to_int = {}
        for dirname in sorted(os.listdir(root)):
            class_to_int[dirname] = len(class_to_int)

        for dirpath, dirnames, filenames in os.walk(root):
            img_class = os.path.relpath(dirpath, root)
            for filename in filenames:
                if not filename.endswith(".png"):
                    continue
                img_id = filename.removesuffix(".png")
                img_path = os.path.join(dirpath, filename)
                self.samples.append((img_id, img_path, class_to_int[img_class]))

    def __getitem__(self, i) -> Sample:
        img_id, img_path, label = self.samples[i]
        img = Image.open(img_path).convert("RGB")
        if self.transform is not None:
            img = self.transform(img)
        return {"img_id": img_id, "img": img, "label": label}

    def __len__(self) -> int:
        return len(self.samples)

    @property
    def labels(self) -> Int[np.ndarray, " n_samples"]:
        return np.array([label for _, _, label in self.samples])


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    split = "train" if is_train else "val"
    images_dir_path = os.path.join(cfg.data.plankton, split)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    dataset = Dataset(images_dir_path, img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    def probe(batch):
        imgs = batch["img"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features  # forward only

    all_ids, all_features, all_labels = [], [], []

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"plk/{split}"):
            batch = next(it)
            imgs = batch["img"].to(cfg.device)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_ids.extend(batch["img_id"])

            all_labels.extend(batch["label"])

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)
    assert len(all_ids) == len(dataset)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 5))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    if 0 < cfg.n_train <= 300:
        return sklearn.linear_model.RidgeClassifier()

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        factor=3,
        random_state=cfg.seed,
        scoring="f1_macro",
    )

>>>> plankton/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the SYKE-plankton_IFCB_2022 dataset.

Run with:

1. `python biobench/plankton/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.plankton.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os
import shutil
import zipfile

import requests
import tqdm
import tyro

train_url = "https://b2share.eudat.eu/api/files/63a79aff-4194-48c8-8055-0a73ecfcf183/phytoplankton_labeled.zip"
val_url = "https://b2share.eudat.eu/api/files/4a62bb1b-9bd0-4005-9217-7472ee6ed92c/phytoplankton_Ut%C3%B6_2021_labeled.zip"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""


def main(args: Args):
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    train_zip = os.path.join(args.dir, "train.zip")
    val_zip = os.path.join(args.dir, "val.zip")

    for filepath, url in [(train_zip, train_url), (val_zip, val_url)]:
        r = requests.get(url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(filepath, "wb") as fd:
            # Need to specify a manual progress bar in order to get units and such working.
            t = tqdm.tqdm(
                total=n_bytes,
                unit="B",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading images",
            )
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
            t.close()

        with zipfile.ZipFile(filepath, "r") as zip:
            for member in tqdm.tqdm(
                zip.infolist(), unit="img", desc="Extracting images"
            ):
                zip.extract(member, args.dir)

    # Move images to particular split-named folders.
    val_folder = "phytoplankton_Utö_2021_labeled"
    move(os.path.join(args.dir, val_folder), os.path.join(args.dir, "val"))
    train_folder = "labeled_20201020"
    move(os.path.join(args.dir, train_folder), os.path.join(args.dir, "train"))

    print(f"Downloaded, extracted and organized images in {args.dir}.")


def move(src: str, dst: str):
    """
    Moves _src_ to _dst_. If _dst_ exists, it will be overwritten.
    """
    if os.path.isdir(dst):
        shutil.rmtree(dst)
    os.rename(src, dst)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> plantnet/__init__.py
"""
Pl@ntNet is a "dataset with high label ambiguity and a long-tailed distribution" from NeurIPS 2021.
We fit a ridge classifier from scikit-learn to a backbone's embeddings and evaluate on the validation split.


There are two pieces that make Pl@ntNet more than a simple classification task:

1. Because of the long tail, we use `class_weight='balanced'` which adjusts weights based on class frequency.
2. We use macro F1 both to choose the alpha parameter and to evaluate the final classifier rather than accuracy due to the massive class imbalance.

If you use this task, please cite the original paper:

@inproceedings{plantnet-300k,
    author={Garcin, Camille and Joly, Alexis and Bonnet, Pierre and Lombardo, Jean-Christophe and Affouard, Antoine and Chouet, Mathias and Servajean, Maximilien and Lorieul, Titouan and Salmon, Joseph},
    booktitle={NeurIPS Datasets and Benchmarks 2021},
    title={{Pl@ntNet-300K}: a plant image dataset with high label ambiguity and a long-tailed distribution},
    year={2021},
}
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import sklearn.experimental.enable_halving_search_cv
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Float, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("plantnet")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    labels: Shaped[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using cross validation splits.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Get features
    val_features = get_features(cfg, backbone, split="val")
    train_features = get_features(cfg, backbone, split="train")

    encoder = sklearn.preprocessing.OrdinalEncoder()
    all_labels = np.concatenate((val_features.labels, train_features.labels))
    encoder.fit(all_labels.reshape(-1, 1))

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y(encoder))

    helpers.write_hparam_sweep_plot("plantnet", cfg.model.ckpt, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = val_features.y(encoder)
    pred_labels = clf.predict(val_features.x)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(val_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("plantnet", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.macro_f1(preds)


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    transform: typing.Any | None
    """Optional function function that transforms an image into a format expected by a neural network."""
    samples: list[tuple[str, str, str]]
    """List of all image ids, image paths, and classnames."""

    def __init__(self, root: str, transform):
        self.transform = transform
        self.samples = []
        if not os.path.exists(root) or not os.path.isdir(root):
            msg = f"Path '{root}' doesn't exist. Did you download the Pl@ntNet dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path as --dataset-dir PATH"
            raise RuntimeError(msg)

        for dirpath, dirnames, filenames in os.walk(root):
            img_class = os.path.relpath(dirpath, root)
            for filename in filenames:
                img_id = filename.removesuffix(".jpg")
                img_path = os.path.join(dirpath, filename)
                self.samples.append((img_id, img_path, img_class))

    def __getitem__(self, i: int) -> tuple[str, Float[Tensor, "3 width height"], str]:
        img_id, img_path, img_class = self.samples[i]
        img = Image.open(img_path)
        if self.transform is not None:
            img = self.transform(img)
        return img_id, img, img_class

    def __len__(self) -> int:
        return len(self.samples)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, split: str
) -> Features:
    imgs_dir_path = os.path.join(cfg.data.plantnet, "images", split)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    dataset = Dataset(imgs_dir_path, img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    all_ids, all_features, all_labels = [], [], []

    def probe(batch):
        _, imgs, _ = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs).img_features  # forward only

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"plnt/{split}"):
            ids, imgs, labels = next(it)
            imgs = imgs.to(cfg.device)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_ids.extend(ids)
            all_labels.extend(labels)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)

    assert len(all_ids) == len(dataset)
    if cfg.n_train >= 0:
        assert len(all_ids) == cfg.n_train
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 11))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0, class_weight="balanced"),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        # This uses sklearn.metrics.f1_score with average="macro"
        scoring="f1_macro",
        factor=3,
    )

>>>> plantnet/download.py
import dataclasses
import os.path
import zipfile

import requests
import tqdm
import tyro

images_url = "https://zenodo.org/records/5645731/files/plantnet_300K.zip"


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images [29.5GB]."""
    unzip: bool = True
    """whether to unzip images."""


def main(args: Args):
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_zip_path = os.path.join(args.dir, "plantnet_300K.zip")

    if args.download:
        # Download images.
        r = requests.get(images_url, stream=True)
        r.raise_for_status()

        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_zip_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_zip_path}.")

    if args.unzip:
        # Unzip images.
        zip_file = zipfile.ZipFile(images_zip_path)
        names = zip_file.namelist()
        for filename in tqdm.tqdm(names, desc="Unzipping images."):
            zip_file.extract(filename, path=args.dir)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> rarespecies/__init__.py
import collections
import dataclasses
import logging
import math

import beartype
import datasets
import numpy as np
import sklearn.neighbors
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from torch import Tensor

from biobench import config, helpers, registry, reporting

logger = logging.getLogger("rare-species")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    device: str = "cuda"
    """(computed at runtime) which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) whether to run in debug mode."""
    n_train: int = -1
    """(computed at runtime) number of maximum training samples. Negative number means use all of them."""


@beartype.beartype
def benchmark(cfg: config.Experiment) -> tuple[config.Model, reporting.Report]:
    backbone = registry.load_vision_backbone(cfg.model)
    features = get_features(cfg, backbone)

    train_i, test_i = make_split(features.y, k=1)

    scores = simpleshot(
        cfg,
        features.x[train_i],
        features.y[train_i],
        features.x[test_i],
        features.y[test_i],
    )
    examples = [
        reporting.Prediction(str(id), float(score), {})
        for id, score in zip(features.ids[test_i], scores.tolist())
    ]
    return cfg.model, reporting.Report("RareSpecies", examples)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[Tensor, " n dim"]
    y: Int[Tensor, " n"]
    ids: Shaped[np.ndarray, " n"]


class Preprocess:
    def __init__(self, img_transform):
        self._img_transform = img_transform

    def __call__(self, example):
        example["image"] = example["image"].convert("RGB")
        example["image"] = self._img_transform(example["image"])
        example["label"] = "-".join(
            example[key]
            for key in [
                "kingdom",
                "phylum",
                "class",
                "order",
                "family",
                "genus",
                "species",
            ]
        )
        return example


@beartype.beartype
@torch.no_grad
def get_features(args: Args, backbone: registry.VisionBackbone) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = (
        datasets.load_dataset("imageomics/rare-species", split="train")
        .to_iterable_dataset(num_shards=args.n_workers)
        .map(Preprocess(img_transform))
        .with_format("torch")
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,  # We use dataset.shuffle instead
    )

    all_features, all_labels, all_ids = [], [], []

    total = math.ceil(11984 / args.batch_size) if not args.debug else 2
    it = iter(dataloader)
    logger.debug("Need to embed %d batches of %d images.", total, args.batch_size)
    for b in helpers.progress(range(total), every=args.log_every, desc="rarespecies"):
        batch = next(it)

        images = batch["image"].to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())
        all_labels.extend(batch["label"])
        all_ids.extend(batch["rarespecies_id"])

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
def simpleshot(
    args: Args,
    x_train: Float[Tensor, "n_train dim"],
    y_train: Int[Tensor, " n_train"],
    x_test: Float[Tensor, "n_test dim"],
    y_test: Int[Tensor, " n_test"],
) -> Float[Tensor, " n_test"]:
    """
    Applies simpleshot to the video clips. We assign each clip the majority label. Return the list of scores for x_test.
    """
    x_mean = x_train.mean(axis=0, keepdims=True)

    x_train = x_train - x_mean
    x_train = l2_normalize(x_train)

    x_test = x_test - x_mean
    x_test = l2_normalize(x_test)

    clf = sklearn.neighbors.NearestCentroid()
    clf.fit(x_train, y_train)

    # Do this next step on the GPU to make it fast.
    # Goes from 1 batch/sec to 77 batch/sec
    centroids = torch.from_numpy(clf.centroids_).to(args.device)
    x_test = x_test.to(args.device)
    y_test = y_test.to(args.device)

    scores = []
    for start, stop in batched_idx(len(x_test), args.batch_size):
        x_batch = x_test[start:stop]
        y_batch = y_test[start:stop]
        distances = torch.linalg.vector_norm(x_batch[:, None] - centroids, axis=2)
        preds = torch.argmin(distances, dim=1)

        scores.append((preds == y_batch).type(torch.float32))

    return torch.cat(scores, axis=0)


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[Tensor, "n_examples dim"],
) -> Float[Tensor, "n_examples dim"]:
    norms = torch.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@jaxtyped(typechecker=beartype.beartype)
def make_split(
    labels: Int[Tensor, " n_examples"], *, k: int
) -> tuple[Int[Tensor, " n_train"], Int[Tensor, " n_test"]]:
    classes = np.unique(labels)

    train_indices = np.array([], dtype=int)
    test_indices = np.array([], dtype=int)

    # Iterate through each class to select indices
    for cls in classes:
        # Indices corresponding to the current class
        cls_indices = np.where(labels == cls)[0]
        # Randomly shuffle the indices
        np.random.shuffle(cls_indices)
        # Select the first K indices for the train set
        cls_train_indices = cls_indices[:k]
        # The rest go into the test set
        cls_test_indices = cls_indices[k:]
        # Append the selected indices to the train/test arrays
        train_indices = np.concatenate((train_indices, cls_train_indices))
        test_indices = np.concatenate((test_indices, cls_test_indices))

    # Shuffle the indices to mix classes
    np.random.shuffle(train_indices)
    np.random.shuffle(test_indices)

    return torch.from_numpy(train_indices), torch.from_numpy(test_indices)

>>>> registry.py
"""
Stores all vision backbones.
Users can register new custom backbones from their code to evaluate on biobench using `register_vision_backbone`.
As long as it satisfies the `VisionBackbone` interface, it will work will all tasks.
"""

import dataclasses
import logging

import beartype
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import config

logger = logging.getLogger(__name__)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class EncodedImgBatch:
    """The output of a `VisionBackbone`'s `VisionBackbone.img_encode()` method."""

    img_features: Float[Tensor, "batch img_dim"]
    """Image-level features. Each image is represented by a single vector."""
    patch_features: Float[Tensor, "batch n_patches patch_dim"] | None
    """Patch-level features. Only ViTs have patch-level features. These features might be a different dimension that the image features because of projection heads or such."""


@jaxtyped(typechecker=beartype.beartype)
class VisionBackbone(torch.nn.Module):
    """
    A frozen vision model that embeds batches of images into batches of vectors.

    To add new models to the benchmark, you can simply create a new class that satisfies this interface and register it.
    See `biobench.registry` for a tutorial on adding new vision backbones.
    """

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> EncodedImgBatch:
        """Encode a batch of images."""
        err_msg = f"{self.__class__.__name__} must implemented img_encode()."
        raise NotImplementedError(err_msg)

    def make_img_transform(self):
        """
        Return whatever function the backbone wants for image preprocessing.
        This should be an evaluation transform, not a training transform, because we are using the output features of this backbone as data and not updating this backbone.
        """
        err_msg = f"{self.__class__.__name__} must implemented make_img_transform()."
        raise NotImplementedError(err_msg)


_global_backbone_registry: dict[str, type[VisionBackbone]] = {}


@beartype.beartype
def load_vision_backbone(model_cfg: config.Model) -> VisionBackbone:
    """
    Load a pretrained vision backbone.
    """
    if model_cfg.org not in _global_backbone_registry:
        raise ValueError(f"Org '{model_cfg.org}' not found.")

    cls = _global_backbone_registry[model_cfg.org]
    return cls(model_cfg.ckpt)


@beartype.beartype
def register_vision_backbone(model_org: str, cls: type[VisionBackbone]):
    """
    Register a new vision backbone class.
    """
    if model_org in _global_backbone_registry:
        logger.warning("Overwriting key '%s' in registry.", model_org)
    _global_backbone_registry[model_org] = cls


def list_vision_backbones() -> list[str]:
    """
    List all vision backbone model orgs.
    """
    return list(_global_backbone_registry.keys())

>>>> reporting.py
import dataclasses
import json
import logging
import os.path
import pathlib
import socket
import sqlite3
import subprocess
import sys
import time

import beartype
import numpy as np
import sklearn.metrics
from jaxtyping import jaxtyped

from . import config, helpers

logger = logging.getLogger(__name__)

schema_fpath = pathlib.Path(__file__).parent / "schema.sql"


@beartype.beartype
def get_db(cfg: config.Experiment) -> sqlite3.Connection:
    """Get a connection to the reports database.

    Args:
        cfg: Experiment configuration

    Returns:
        sqlite3.Connection: A connection to the SQLite database
    """
    os.makedirs(os.path.expandvars(cfg.report_to), exist_ok=True)
    helpers.warn_if_nfs(cfg.report_to)
    db_fpath = os.path.join(os.path.expandvars(cfg.report_to), "reports.sqlite")
    db = sqlite3.connect(db_fpath, autocommit=True)

    with open(schema_fpath) as fd:
        schema = fd.read()
    db.executescript(schema)
    db.autocommit = False

    return db


@beartype.beartype
def already_ran(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> bool:
    """Check if an experiment has already been run.

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to check

    Returns:
        bool: True if the experiment has already been run, False otherwise
    """
    query = """
    SELECT COUNT(*)
    FROM experiments
    WHERE task_name = ?
    AND model_org = ?
    AND model_ckpt = ?
    AND n_train = ?
    """
    values = (task_name, cfg.model.org, cfg.model.ckpt, cfg.n_train)

    (count,) = db.execute(query, values).fetchone()
    return count > 0


@beartype.beartype
def is_claimed(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> bool:
    """Check if a run is already claimed by another process.

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to check

    Returns:
        bool: True if the run is already claimed, False otherwise
    """
    query = """
    SELECT COUNT(*)
    FROM runs
    WHERE task_name = ?
    AND model_org = ?
    AND model_ckpt = ?
    AND n_train = ?
    """
    values = (task_name, cfg.model.org, cfg.model.ckpt, cfg.n_train)

    (count,) = db.execute(query, values).fetchone()
    return count > 0


@beartype.beartype
def claim_run(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> bool:
    """Try to claim (task_name, model, n_train).

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to claim

    Returns:
        bool: True if this process inserted the row and now "owns" the run,
              False if row already existed and another worker has it
    """

    stmt = """
    INSERT OR IGNORE INTO runs
    (task_name, model_org, model_ckpt, n_train, pid, posix)
    VALUES (?,?,?,?,?,?)
    """
    values = (
        task_name,
        cfg.model.org,
        cfg.model.ckpt,
        cfg.n_train,
        os.getpid(),
        time.time(),
    )

    try:
        cur = db.execute(stmt, values)
        db.commit()
        return cur.rowcount == 1  # 1 row inserted -> we won
    except Exception:
        db.rollback()
        raise


@beartype.beartype
def release_run(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> None:
    """Delete the coordination row so others may claim again.

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to release
    """
    stmt = """
    DELETE FROM runs
    WHERE task_name=? AND model_org=? AND model_ckpt=? AND n_train=?
    """
    values = (task_name, cfg.model.org, cfg.model.ckpt, cfg.n_train)
    logger.info("Releasing claim on (%s, %s, %s, %d)", *values)

    try:
        db.execute(stmt, values)
        db.commit()
        logger.info("Released claim on (%s, %s, %s, %d)", *values)
    except Exception:
        db.rollback()
        raise


@beartype.beartype
def clear_stale_claims(db: sqlite3.Connection, *, max_age_hours: int = 72) -> int:
    """
    Delete rows in `runs` whose POSIX timestamp is older than `max_age_hours`.

    Returns
    -------
    int
        Number of rows deleted.
    """
    if max_age_hours <= 0:
        raise ValueError("max_age_hours must be positive")

    cutoff = time.time() - max_age_hours * 3600
    try:
        cur = db.execute("DELETE FROM runs WHERE posix < ?", (cutoff,))
        db.commit()
        return cur.rowcount
    except Exception:
        db.rollback()
        raise


@beartype.beartype
def get_git_hash() -> str:
    """Returns the hash of the current git commit.

    Returns:
        str: The hash of the current git commit, assuming we are in a git repo
    """
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("ascii").strip()


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Prediction:
    """An individual test prediction."""

    id: str
    """Whatever kind of ID; used to find the original image/example."""
    score: float
    """Test score; typically 0 or 1 for classification tasks."""
    info: dict[str, object]
    """Any additional information included. This might be the original class, the true label, etc."""


def get_gpu_name() -> str:
    import torch

    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).name
    else:
        return ""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Report:
    """
    The result of running a benchmark task.
    """

    # Actual details of the report
    task_name: str
    """The benchmark name."""
    predictions: list[Prediction]
    """A list of (example_id, score, info) objects"""
    cfg: config.Experiment
    """Experimental config."""
    _: dataclasses.KW_ONLY
    splits: dict[str, float] = dataclasses.field(default_factory=dict)
    """Other scores that you would like to report. These do not have confidence intervals."""

    # Stuff for trying to reproduce this result. These are filled in by default.
    argv: list[str] = dataclasses.field(default_factory=lambda: sys.argv)
    """Command used to get this report."""
    git_commit: str = get_git_hash()
    """Git commit for this current report."""
    posix: float = dataclasses.field(default_factory=time.time)
    """Time when this report was constructed."""
    gpu_name: str = dataclasses.field(default_factory=get_gpu_name)
    """Name of the GPU that ran this experiment."""
    hostname: str = dataclasses.field(default_factory=socket.gethostname)
    """Machine hostname that ran this experiment."""

    def __repr__(self):
        return f"Report({self.task_name} with {len(self.predictions)} predictions)"

    def __str__(self):
        return repr(self)

    @beartype.beartype
    def write(self) -> None:
        """Saves the report to disk in a machine-readable SQLite format."""
        db = get_db(self.cfg)

        preds_stmt = "INSERT INTO predictions(img_id, score, info, experiment_id) VALUES(?, ?, ?, ?)"
        exp_stmt = "INSERT INTO experiments(task_name, model_org, model_ckpt, n_train, exp_cfg, argv, git_commit, posix, gpu_name, hostname) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        try:
            cursor = db.cursor()

            exp_values = (
                self.task_name.lower(),
                self.cfg.model.org,
                self.cfg.model.ckpt,
                self.cfg.n_train,
                json.dumps(self.cfg.to_dict()),
                json.dumps(self.argv),
                self.git_commit,
                self.posix,
                self.gpu_name,
                self.hostname,
            )
            cursor.execute(exp_stmt, exp_values)
            exp_id = cursor.lastrowid
            preds_values = [
                (pred.id, pred.score, json.dumps(pred.info), exp_id)
                for pred in self.predictions
            ]
            cursor.executemany(preds_stmt, preds_values)

            # Commit the transaction if all statements succeed
            db.commit()
        except sqlite3.Error as err:
            # Roll back the transaction in case of error
            db.rollback()
            logger.critical("Error writing report for '%s': %s", self.task_name, err)
            raise


@beartype.beartype
def micro_acc(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.accuracy_score(y_true, y_pred)


@beartype.beartype
def macro_acc(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.balanced_accuracy_score(y_true, y_pred)


@beartype.beartype
def micro_f1(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.f1_score(y_true, y_pred, average="micro")


@beartype.beartype
def macro_f1(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.f1_score(
        y_true, y_pred, average="macro", labels=np.unique(y_true)
    )


##########
# COLORS #
##########


# https://coolors.co/palette/001219-005f73-0a9396-94d2bd-e9d8a6-ee9b00-ca6702-bb3e03-ae2012-9b2226

BLACK_HEX = "001219"
BLACK_RGB = (0, 18, 25)
BLACK_RGB01 = tuple(c / 256 for c in BLACK_RGB)

BLUE_HEX = "005f73"
BLUE_RGB = (0, 95, 115)
BLUE_RGB01 = tuple(c / 256 for c in BLUE_RGB)

CYAN_HEX = "0a9396"
CYAN_RGB = (10, 147, 150)
CYAN_RGB01 = tuple(c / 256 for c in CYAN_RGB)

SEA_HEX = "94d2bd"
SEA_RGB = (148, 210, 189)
SEA_RGB01 = tuple(c / 256 for c in SEA_RGB)

CREAM_HEX = "e9d8a6"
CREAM_RGB = (233, 216, 166)
CREAM_RGB01 = tuple(c / 256 for c in CREAM_RGB)

GOLD_HEX = "ee9b00"
GOLD_RGB = (238, 155, 0)
GOLD_RGB01 = tuple(c / 256 for c in GOLD_RGB)

ORANGE_HEX = "ca6702"
ORANGE_RGB = (202, 103, 2)
ORANGE_RGB01 = tuple(c / 256 for c in ORANGE_RGB)

RUST_HEX = "bb3e03"
RUST_RGB = (187, 62, 3)
RUST_RGB01 = tuple(c / 256 for c in RUST_RGB)

SCARLET_HEX = "ae2012"
SCARLET_RGB = (174, 32, 18)
SCARLET_RGB01 = tuple(c / 256 for c in SCARLET_RGB)

RED_HEX = "9b2226"
RED_RGB = (155, 34, 38)
RED_RGB01 = tuple(c / 256 for c in RED_RGB)

>>>> schema.sql
PRAGMA journal_mode = WAL;    -- Concurrent reads/writes
PRAGMA synchronous = NORMAL;  -- Good balance speed/safety
PRAGMA foreign_keys = ON;     -- Enforce FK constraints
PRAGMA busy_timeout = 30000;  -- Wait up to 30s before throwing timeout errors
PRAGMA strict = ON;           -- Enforce strict type checking (SQLite ≥ 3.37)
PRAGMA encoding = 'UTF-8';    -- Consistent text encoding


CREATE TABLE IF NOT EXISTS experiments (
    id INTEGER PRIMARY KEY AUTOINCREMENT,

    -- Task information
    task_name TEXT NOT NULL,

    -- Model information
    model_org TEXT NOT NULL,
    model_ckpt TEXT NOT NULL,

    -- Number of requested training samples.
    n_train INTEGER NOT NULL,

    exp_cfg TEXT NOT NULL,  -- JSON blob with full experiment configuration

    -- Metadata fields
    argv TEXT NOT NULL,  -- Command used to get this report (JSON array)
    git_commit TEXT NOT NULL,  -- Git commit hash
    posix INTEGER NOT NULL,  -- POSIX timestamp
    gpu_name TEXT,  -- Name of the GPU that ran this experiment
    hostname TEXT NOT NULL  -- Machine hostname that ran this experiment
);

CREATE TABLE IF NOT EXISTS predictions (
    img_id TEXT NOT NULL,  -- ID used to find the original image/example
    score REAL NOT NULL,  -- Test score; typically 0 or 1 for classification tasks
    info TEXT NOT NULL,  -- JSON blob with additional information (original class, true label, etc.)

    -- Foreign key to link to the experiments table
    experiment_id INTEGER NOT NULL,

    PRIMARY KEY (img_id, experiment_id),
    FOREIGN KEY (experiment_id) REFERENCES experiments(id)  ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS runs (
    task_name TEXT NOT NULL,
    model_org TEXT NOT NULL,
    model_ckpt TEXT NOT NULL,
    n_train INTEGER NOT NULL,

    -- For debugging
    posix INTEGER NOT NULL,  -- POSIX timestamp
    pid INTEGER NOT NULL,  -- os.getpid() of the worker
    PRIMARY KEY (task_name, model_org, model_ckpt, n_train)
);

>>>> simpleshot.py
"""
Implements normalized nearest-centroid classifiers, as described in [this paper](https://arxiv.org/abs/1911.04623).

If you use this work, be sure to cite the original work:

```
@article{wang2019simpleshot,
  title={Simpleshot: Revisiting nearest-neighbor classification for few-shot learning},
  author={Wang, Yan and Chao, Wei-Lun and Weinberger, Kilian Q and Van Der Maaten, Laurens},
  journal={arXiv preprint arXiv:1911.04623},
  year={2019}
}
```
"""

import beartype
import numpy as np
import sklearn.base
import sklearn.neighbors
import sklearn.utils.validation
import torch
from jaxtyping import Float, jaxtyped

from . import helpers


@beartype.beartype
class SimpleShotClassifier(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):
    """
    scikit-learn wrapper for the "normalized nearest-centroid" classifier (a.k.a. SimpleShot, Wang et al. ICCV 2019).

    Parameters
    ----------
    device : {'cpu','cuda'}, default='cpu'
        Used only during `predict`; centroids are pushed to this device for fast batched distance computation.
    """

    def __init__(self, batch_size: int = 2048, device: str = "cpu"):
        self.batch_size = batch_size
        self.device = device

    def fit(self, X, y):
        x, y = sklearn.utils.validation.check_X_y(X, y, dtype=np.float32, order="C")
        # centre the cloud
        self.x_mean_ = x.mean(axis=0, keepdims=True)

        x = x - self.x_mean_
        x = l2_normalize(x)

        self.clf_ = sklearn.neighbors.NearestCentroid()
        self.clf_.fit(x, y)
        return self

    def predict(self, X):
        sklearn.utils.validation.check_is_fitted(self, ["clf_", "x_mean_"])
        x = sklearn.utils.validation.check_array(X, dtype=np.float32, order="C")

        x = x - self.x_mean_
        x = l2_normalize(x)

        # Do this next step on the GPU to make it fast.
        centroids = torch.from_numpy(self.clf_.centroids_).to(
            self.device, non_blocking=True
        )
        x = torch.from_numpy(x).to(self.device, non_blocking=True)

        preds = []
        for start, stop in helpers.batched_idx(len(x), self.batch_size):
            x_batch = x[start:stop]
            distances = torch.linalg.vector_norm(x_batch[:, None] - centroids, axis=2)
            preds.append(torch.argmin(distances, dim=1).cpu())

        return torch.cat(preds, dim=0).numpy()


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[np.ndarray, "n_examples dim"],
) -> Float[np.ndarray, "n_examples dim"]:
    """L2-normalize a batch of features.

    Args:
        features: batch of $d$-dimensional vectors.

    Returns:
        batch of $d$-dimensional vectors with unit L2 norm.
    """
    norms = np.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms

>>>> test_aimv2.py
import pytest
import torch
import transformers

from . import config, helpers, registry

CKPTS = [
    "apple/aimv2-large-patch14-224",
    "apple/aimv2-large-patch14-224-distilled",
    "apple/aimv2-1B-patch14-224",
    "apple/aimv2-large-patch14-448",
]
DTYPE = torch.float32
ATOL, RTOL = 1e-5, 1e-4


@pytest.fixture(scope="session", params=CKPTS)
def models(request):
    ckpt = request.param
    hf = transformers.AutoModel.from_pretrained(
        ckpt, trust_remote_code=True, cache_dir=helpers.get_cache_dir()
    ).eval()
    bio = registry.load_vision_backbone(config.Model("aimv2", ckpt)).eval().to(DTYPE)
    return hf, bio


def _rand(batch: int = 1):
    torch.manual_seed(0)
    return torch.rand(batch, 3, 224, 224, dtype=DTYPE)


def test_same_shape_single(models):
    hf, bio = models
    batch = _rand()
    h = hf(batch).last_hidden_state
    b = bio.img_encode(batch).patch_features
    assert h.shape == b.shape


def test_values_close_single(models):
    hf, bio = models
    batch = _rand()
    h = hf(batch).last_hidden_state
    b = bio.img_encode(batch).patch_features
    assert torch.allclose(h, b, atol=ATOL, rtol=RTOL)


def test_values_close_batch(models):
    hf, bio = models
    batch = _rand(batch=4)
    h = hf(batch).last_hidden_state
    b = bio.img_encode(batch).patch_features
    assert torch.allclose(h, b, atol=ATOL, rtol=RTOL)

>>>> test_auto_batch_size.py
import itertools
import math
import threading
import typing

import pytest
import torch
import torch.utils.data

from . import helpers


def make_dataloader(size: int):
    data = torch.rand(size, 3, 32, 32)
    ds = torch.utils.data.TensorDataset(data)
    return torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, num_workers=0)


def make_probe(*, max_ok: int, style: typing.Literal["oom", "sdpa"] = "oom"):
    """
    Probe that raises a CUDA-style OOM if the *batch* is larger than `max_ok`. Works on CPU; keeps test independent of real GPU RAM.

    -1 indicates to never OOM.
    """

    def probe(batch):
        imgs = batch[0] if isinstance(batch, (list, tuple)) else batch
        if max_ok > 0 and imgs.shape[0] > max_ok:
            if style == "oom":
                raise RuntimeError("CUDA out of memory.")  # substring preserved
            elif style == "sdpa":
                raise RuntimeError("CUDA error: invalid configuration argument")
            else:
                typing.assert_never(style)
        return imgs.mean()  # cheap op

    return probe


def test_ctx_runs_and_restores():
    dataloader = make_dataloader(128)
    orig = dataloader.batch_sampler.batch_size
    probe = make_probe(max_ok=8)  # fail when >8

    with helpers.auto_batch_size(dataloader, probe=probe, schedule=(2, 4, 8, 16)):
        # context executes arbitrary user code w/out throwing
        for _ in itertools.islice(dataloader, 3):
            pass
        # inside ctx largest successful should be 8
        assert dataloader.batch_sampler.batch_size == 8

    # after exit, original value restored
    assert dataloader.batch_sampler.batch_size == orig


@pytest.mark.skipif(
    not torch.cuda.is_available(), reason="only meaningful on a GPU box"
)
def test_with_gpu():
    """Smoke-test true GPU execution (no artificial OOM)."""

    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(dataloader, probe=lambda x: x * 1, schedule=(2, 4)):
        next(iter(dataloader))


def test_len_adjusts_with_batch_size():
    """
    `len(dataloader)` reflects the tuned batch size inside the CM and restores afterward.
    """
    dataloader = make_dataloader(30)
    n_samples = len(dataloader.dataset)  # 30
    orig_bs = dataloader.batch_sampler.batch_size  # 2
    assert len(dataloader) == math.ceil(n_samples / orig_bs)  # 15

    probe = make_probe(max_ok=8)  # tuning will settle on 8

    with helpers.auto_batch_size(dataloader, probe=probe, schedule=(2, 4, 8, 16)):
        tuned_bs = dataloader.batch_sampler.batch_size
        assert tuned_bs == 8

        inside_len = len(dataloader)
        assert inside_len == math.ceil(n_samples / tuned_bs)  # 4

    # back to original
    assert dataloader.batch_sampler.batch_size == orig_bs
    assert len(dataloader) == math.ceil(n_samples / orig_bs)  # 15


def test_invalid_cfg_error_handled():
    """Helper treats 'invalid configuration argument' the same as OOM."""
    dataloader = make_dataloader(128)
    orig = dataloader.batch_sampler.batch_size
    probe = make_probe(max_ok=4, style="sdpa")  # fail when >4

    with helpers.auto_batch_size(dataloader, probe=probe, schedule=(2, 4, 8)):
        # should settle on 4
        assert dataloader.batch_sampler.batch_size == 4

    # restored afterwards
    assert dataloader.batch_sampler.batch_size == orig


def test_terminates_on_short_dataset():
    """If dataset has < tried batch-size, auto_batch_size must still terminate."""

    dataloader = make_dataloader(12)

    def run():
        with helpers.auto_batch_size(dataloader, probe=make_probe(max_ok=-1)):
            pass  # nothing

    t = threading.Thread(target=run, daemon=True)
    t.start()
    t.join(timeout=2.0)  # 2-second cap
    assert not t.is_alive(), "auto_batch_size never terminated on tiny dataset"


def test_caps_at_dataset_len_generic():
    class Sample(typing.NamedTuple):
        img: torch.Tensor
        meta: dict[str, int]

    def make_dataset_namedtuple(n):
        data = torch.rand(n, 3, 32, 32)
        ds = [Sample(img=x, meta={"idx": i}) for i, x in enumerate(data)]
        return ds

    ds = make_dataset_namedtuple(12)
    loader = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, num_workers=0)

    with helpers.auto_batch_size(
        loader, probe=lambda b: b.img.mean(), schedule=(4, 8, 16, 32)
    ):
        assert loader.batch_sampler.batch_size == len(ds)  # 12


def test_upper_caps_batch_size():
    """
    With upper=32 the helper must stop at 32 even though larger sizes fit.
    """

    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(
        dataloader,
        probe=make_probe(max_ok=-1),
        schedule=(2, 4, 8, 16, 32, 64, 128),
        upper=32,
    ):
        assert dataloader.batch_sampler.batch_size == 32, "should cap at upper"


def test_upper_below_start_allowed():
    """
    If upper < initial batch size, helper should *lower* to upper immediately.
    """
    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(
        dataloader, probe=make_probe(max_ok=-1), schedule=(2, 4, 8), upper=1
    ):
        assert dataloader.batch_sampler.batch_size == 1


def _probe_flaky(flake_at: int):
    """Calls above flake_at work once, but not again."""
    seen = set()

    def inner(batch):
        bsz = helpers.infer_batch_size(batch)
        if bsz not in seen:
            seen.add(bsz)

        if bsz not in seen or bsz < flake_at:
            return batch[0].mean()

        raise RuntimeError("CUDA out of memory.")

    return inner


def test_backoff_after_flaky_probe():
    # 8 succeeds then OOMs; helper should fall back to 6.
    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(
        dataloader, probe=_probe_flaky(flake_at=8), schedule=(2, 3, 4, 6, 8, 12, 16)
    ):
        assert dataloader.batch_sampler.batch_size == 6


def test_backoff_zero_keeps_largest():
    """
    backoff=0 must reproduce the "largest that works" policy.
    """
    loader = make_dataloader(64)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=16), schedule=(2, 4, 8, 16, 32), backoff=0
    ):
        assert loader.batch_sampler.batch_size == 16


def test_backoff_one_steps_back_once():
    """Largest success is 16, back-off by one => 8."""
    loader = make_dataloader(64)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=16), schedule=(2, 4, 8, 16, 32), backoff=1
    ):
        assert loader.batch_sampler.batch_size == 8


def test_backoff_clamped_to_smallest():
    """Only 3 successes (2,4,8). backoff=10 should clamp to 2."""
    loader = make_dataloader(64)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=8), schedule=(2, 4, 8, 16), backoff=10
    ):
        assert loader.batch_sampler.batch_size == 2


def test_negative_backoff_error():
    """A negative value is invalid."""
    loader = make_dataloader(32)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=8), schedule=(2, 4, 8), backoff=-1
        ):
            pass


def test_flaky_probe_backoff_zero():
    """Probe OOMs *after* first success at 8. Helper should settle on 8 with backoff=0."""
    loader = make_dataloader(128)
    with helpers.auto_batch_size(
        loader, probe=_probe_flaky(flake_at=8), schedule=(2, 4, 6, 8, 10), backoff=0
    ):
        assert loader.batch_sampler.batch_size == 6


def test_flaky_probe_backoff_one_skips_flake():
    """8 becomes flaky; second-largest stable is 6."""
    loader = make_dataloader(128)
    with helpers.auto_batch_size(
        loader, probe=_probe_flaky(flake_at=8), schedule=(2, 4, 6, 8, 10), backoff=1
    ):
        assert loader.batch_sampler.batch_size == 4


def test_unsorted_schedule_raises():
    """Schedule 4,16,8... has a downward step (16 -> 8) => should raise."""
    loader = make_dataloader(64)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=32), schedule=(4, 16, 8, 32)
        ):
            pass


def test_duplicate_size_raises():
    """Duplicate 8 breaks the strict-increasing rule (8 -> 8)."""
    loader = make_dataloader(64)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=32), schedule=(4, 8, 8, 16)
        ):
            pass


def test_iterator_schedule_checked():
    """Even if an iterator lazily yields an out-of-order value (8 then 4), the helper must detect it."""

    def gen():
        yield 2
        yield 8
        yield 4  # drop -> should raise

    loader = make_dataloader(64)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=16), schedule=gen()
        ):
            pass


def test_single_success_backoff_clamps():
    """Only size 2 succeeds. backoff=3 must still give 2."""
    loader = make_dataloader(16)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=2), schedule=(2, 4, 8), backoff=3
    ):
        assert loader.batch_sampler.batch_size == 2


def test_dataset_cap_and_backoff():
    """Dataset of 12 samples, schedule goes beyond 12. Largest feasible <=12 is 12; backoff=2 => 4."""
    loader = make_dataloader(12)  # initial bs=2
    with helpers.auto_batch_size(
        loader,
        probe=make_probe(max_ok=-1),  # never OOM
        schedule=(4, 8, 12, 16, 32),
        backoff=2,
    ):
        assert loader.batch_sampler.batch_size == 4
        assert loader.batch_sampler.batch_size <= len(loader.dataset)


@pytest.mark.timeout(1.0)
def test_schedule_not_materialised():
    """Passing an *infinite* strictly-increasing generator must *not* hang. If the implementation ever did `list(schedule)` or similar, this test would exceed the 1-second timeout and fail."""

    def infinite_pow2():
        n = 2
        while True:
            yield n  # 2, 4, 8, 16, ...
            n *= 2

    loader = make_dataloader(64)  # dataset len = 64
    # probe never OOMs -> helper should stop when it hits dataset size
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=-1), schedule=infinite_pow2()
    ):
        # final batch_size should equal dataset length (64)
        assert loader.batch_sampler.batch_size == len(loader.dataset)

>>>> test_config.py
import itertools
import pathlib
import textwrap

from . import config


def _write_toml(tmp_path: pathlib.Path) -> pathlib.Path:
    tmp_path.mkdir(exist_ok=True)
    tmp_file = tmp_path / "grid.toml"
    tmp_file.write_text(
        textwrap.dedent(
            """
            models = [
              { org = "timm",      ckpt = "resnet50" },
              { org = "open-clip", ckpt = "ViT-B-16/openai" },
            ]

            debug   = [ true, false ]
            n_train = [ 10, 100 ]

            [data]
            newt = "/data/newt"
            """
        )
    )
    return tmp_file


def test_load_returns_full_cartesian_product(tmp_path):
    toml_path = _write_toml(tmp_path)
    exps = config.load(str(toml_path))

    # --- expected triples ---
    expected = set(
        itertools.product(
            ["timm", "open-clip"],  # model_org
            [True, False],  # debug
            [10, 100],  # n_train
        )
    )

    # --- actual triples ---
    got = {(exp.model.org, exp.debug, exp.n_train) for exp in exps}

    assert got == expected, (
        f"Missing/extra combinations: expected {expected}, got {got}"
    )

    # data-block propagated
    for exp in exps:
        assert exp.data.newt == "/data/newt"

>>>> test_helpers.py
import collections

import beartype
import hypothesis.extra.numpy as npst
import numpy as np
from hypothesis import assume, given
from hypothesis import strategies as st
from jaxtyping import Int, jaxtyped

from . import helpers


@st.composite
def _labels_and_n(draw):
    """
    Helper strategy: (labels array, n) with n <= len(labels) and n >= #classes
    """
    labels_list = draw(
        st.lists(st.integers(min_value=0, max_value=50), min_size=1, max_size=300)
    )
    labels = np.array(labels_list, dtype=int)
    n_classes = len(np.unique(labels))
    # choose n in [n_classes, len(labels)]
    n = draw(st.integers(min_value=n_classes, max_value=len(labels)))
    return labels, n


@jaxtyped(typechecker=beartype.beartype)
def _measure_balance(
    labels: Int[np.ndarray, " n_labels"], indices: Int[np.ndarray, " n"]
) -> float:
    """
    Calculate a balance metric (coefficient of variation, lower is better) for the selected samples (labels[indices]).

    Returns 0 for perfect balance, higher for more imbalance.
    """
    if len(indices) == 0:
        return 0.0

    # Get the distribution of classes in the selected samples
    selected_labels = labels[indices]
    class_counts = collections.Counter(selected_labels)

    # Get all unique classes in the original dataset
    all_classes = set(labels)

    # Check if it was possible to include at least one of each class but didn't
    if len(indices) >= len(all_classes) and len(class_counts) < len(all_classes):
        return float("inf")

    # Calculate coefficient of variation (standard deviation / mean)
    counts = np.array(list(class_counts.values()))

    # If only one class is present, return a high value to indicate imbalance
    if len(counts) == 1:
        return float("inf")

    mean = np.mean(counts)
    std = np.std(counts, ddof=1)  # Using sample standard deviation

    # Return coefficient of variation (0 for perfect balance)
    return std / mean if mean > 0 else 0.0


@given(
    st.text(
        # printable ASCII (includes '/' ':' we want to scrub)
        alphabet=st.characters(min_codepoint=32, max_codepoint=126),
        max_size=100,
    )
)
def test_fs_safe_idempotent_and_clean(random_text):
    cleaned = helpers.fs_safe(random_text)

    # 1) Forbidden characters removed
    assert ":" not in cleaned and "/" not in cleaned

    # 2) Idempotent
    assert helpers.fs_safe(cleaned) == cleaned


@given(
    total=st.integers(min_value=0, max_value=1_000),
    batch=st.integers(min_value=1, max_value=400),
)
def test_batched_idx_covers_range_without_overlap(total, batch):
    """batched_idx must partition [0,total) into consecutive, non-overlapping spans, each of length <= batch."""
    spans = list(helpers.batched_idx(total, batch))

    # edge-case: nothing to iterate
    if total == 0:
        assert spans == []
        return

    # verify each span and overall coverage
    covered = []
    expected_start = 0
    for start, stop in spans:
        # bounds & width checks
        assert 0 <= start < stop <= total
        assert (stop - start) <= batch
        # consecutiveness (no gaps/overlap)
        assert start == expected_start
        expected_start = stop
        covered.extend(range(start, stop))

    # spans collectively cover exactly [0, total)
    assert covered == list(range(total))


@given(
    labels=npst.arrays(
        dtype=np.int32,
        shape=st.integers(min_value=1000, max_value=10000),
        elements=st.integers(min_value=0, max_value=100),
    ),
    n=st.integers(min_value=10, max_value=1000),
)
def test_correct_sample_size(labels, n):
    """Test that the function returns exactly n samples (or all if n > len(labels))"""
    assume(len(np.unique(labels)) > 1)  # Ensure we have at least 2 classes

    indices = helpers.balanced_random_sample(labels, n)

    # Check that the number of samples is correct
    n_expected = min(n, len(labels))
    assert len(indices) == n_expected

    # Check that all indices are valid
    assert np.all(indices < len(labels)), "Some indices are out of bounds"

    # Check that there are no duplicate indices
    assert len(indices) == len(np.unique(indices)), "Duplicate indices found"


# Test case 2: Class balance property
@given(
    labels=npst.arrays(
        dtype=np.int32,
        shape=st.integers(min_value=1000, max_value=10000),
        elements=st.integers(min_value=0, max_value=20),
    ),
    n=st.integers(min_value=100, max_value=1000),
)
def test_class_balance(labels, n):
    """
    Test that the class distribution in the sample is more balanced than random sampling would be.
    """
    unique_classes = np.unique(labels)
    assume(len(unique_classes) > 1)  # Ensure we have at least 2 classes
    assume(n >= len(unique_classes))  # Ensure we request at least one sample per class

    # Get balanced samples
    balanced_indices = helpers.balanced_random_sample(labels, n)
    balanced_balance = _measure_balance(labels, balanced_indices)

    # Get a normal random sample for comparison
    random_indices = np.random.choice(len(labels), min(n, len(labels)), replace=False)
    random_balance = _measure_balance(labels, random_indices)

    # Check if our balanced sampling is generally better than random
    # Note: This might occasionally fail due to randomness, but should pass most of the time
    assert balanced_balance <= random_balance * 1.5, (
        f"Balance metric: balanced={balanced_balance}, random={random_balance}"
    )


def test_single_class_sampling():
    """Test sampling when all samples are from the same class"""
    labels = np.array([1, 1, 1, 1, 1], dtype=int)
    indices = helpers.balanced_random_sample(labels, 3)
    assert len(indices) == 3
    assert len(np.unique(indices)) == 3


def test_small_sample_size():
    """Test sampling with a very small n"""
    labels = np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=int)
    indices = helpers.balanced_random_sample(labels, 2)
    assert len(indices) == 2


def test_sample_size_larger_than_dataset():
    """Test when requested sample size exceeds dataset size"""
    labels = np.array([0, 1, 2], dtype=int)
    indices = helpers.balanced_random_sample(labels, 10)
    assert len(indices) == 3  # Should return all samples
    assert set(indices) == {0, 1, 2}


def test_empty_dataset():
    """Test sampling from an empty dataset"""
    labels = np.array([], dtype=int)
    indices = helpers.balanced_random_sample(labels, 5)
    assert len(indices) == 0


def test_zero_samples_requested():
    """Test when zero samples are requested"""
    labels = np.array([0, 1, 2, 3], dtype=int)
    indices = helpers.balanced_random_sample(labels, 0)
    assert len(indices) == 0


@given(_labels_and_n())
def test_balanced_random_sample_includes_each_class_when_possible(data):
    labels, n = data
    idx = helpers.balanced_random_sample(labels, n)
    assert set(labels[idx]) == set(np.unique(labels))


@given(
    labels=st.lists(st.integers(min_value=0, max_value=100), min_size=10, max_size=500),
    n=st.integers(min_value=1, max_value=500),
)
def test_balanced_random_sample_never_duplicates_indices(labels, n):
    """Returned index list must have no duplicates."""
    labels = np.array(labels, dtype=int)
    n = min(n, len(labels))  # cap n to dataset size

    idx = helpers.balanced_random_sample(labels, n)

    # uniqueness & length
    assert len(idx) == len(set(idx))
    assert len(idx) == n or len(idx) == len(labels)  # handles n>len(labels)

>>>> test_openset.py
"""Unit tests for `openset.MahalanobisOpenSetClassifier`."""

import numpy as np
import pytest
import sklearn.linear_model
from hypothesis import given
from hypothesis import strategies as st

from . import openset

# --------#
# Helpers #
# --------#


def _simple_classifier():
    return openset.MahalanobisOpenSetClassifier(
        base_estimator=sklearn.linear_model.RidgeClassifier(),
        alpha=0.95,
        unknown_label=-1,
    )


def _toy_data():
    """Return a trivially separable 1-D two-class dataset."""
    x = np.array([-2.0, -1.5, -1.0, 1.0, 1.5, 2.0]).reshape(-1, 1)
    y = np.array([0, 0, 0, 1, 1, 1])
    return x, y


# -----------------------------------------------------------------------------
# Estimator API test
# -----------------------------------------------------------------------------


def test_estimator_api():
    clf = _simple_classifier()
    x, y = _toy_data()

    # scikit-learn compliance: fit / predict / get_params / set_params
    assert hasattr(clf, "get_params") and hasattr(clf, "set_params")

    clf.fit(x, y)
    preds = clf.predict(x)

    # Attributes created by fit
    for attr in ("clf_", "means_", "Sigma_inv_", "tau_", "classes_"):
        assert hasattr(clf, attr), f"missing attribute {attr} after fit()"

    # Perfect prediction on training data
    assert np.array_equal(preds, y)


# ----------------------------------------------- #
# Hypothesis: fuzz for exceptions (fit & predict) #
# ----------------------------------------------- #


@given(
    n_samples=st.integers(min_value=30, max_value=200),
    n_features=st.integers(min_value=2, max_value=60),
    n_classes=st.integers(min_value=2, max_value=40),
)
def test_fuzz_no_exceptions(n_samples, n_features, n_classes):
    rng = np.random.default_rng()
    x = rng.normal(size=(n_samples, n_features)).astype(np.float32)
    y = rng.integers(0, n_classes, size=n_samples)

    clf = _simple_classifier()
    clf.fit(x, y)  # should not raise
    preds = clf.predict(x)

    assert preds.shape[0] == n_samples


# -------------------------------- #
# Hand-written deterministic cases #
# -------------------------------- #


@pytest.mark.parametrize(
    "test_pt, expected",
    [
        (np.array([[0.0]]), 0),  # near class-0 centroid
        (np.array([[1.2]]), 1),  # near class-1 centroid
        (np.array([[100.0]]), -1),  # far away -> unknown
        (np.array([[-50.0]]), -1),  # far other side -> unknown
    ],
)
def test_known_vs_unknown(test_pt, expected):
    x, y = _toy_data()
    clf = _simple_classifier().fit(x, y)
    pred = clf.predict(test_pt)[0]
    assert pred == expected


# --------------------------------------------------------------#
# Property-based tests that performance is *better than random* #
# --------------------------------------------------------------#


def _make_cluster_data(n_classes=3, pts_per_class=30, d=4, noise=0.05):
    """Generate tight Gaussian clusters plus a distant OOD cluster."""
    rng = np.random.default_rng(0)
    centers = rng.normal(scale=3.0, size=(n_classes, d))

    x_known = []
    y_known = []
    for k, c in enumerate(centers):
        x_known.append(rng.normal(loc=c, scale=noise, size=(pts_per_class, d)))
        y_known.append(np.full(pts_per_class, k))
    x_known = np.vstack(x_known)
    y_known = np.concatenate(y_known)

    # OOD data around a far corner
    ood_center = np.full(d, 10.0)
    x_ood = rng.normal(loc=ood_center, scale=noise, size=(pts_per_class, d))
    y_ood = np.full(pts_per_class, -1)  # unknown label for evaluation

    return (x_known, y_known), (x_ood, y_ood)


@pytest.mark.parametrize("seed", [0, 42])
def test_high_known_accuracy(seed):
    (x_k, y_k), (x_ood, _) = _make_cluster_data()
    rng = np.random.default_rng(seed)

    idx = rng.choice(len(x_k), size=60, replace=False)
    x_train, y_train = x_k[idx], y_k[idx]
    x_test = np.vstack([x_k, x_ood])
    y_test = np.concatenate([y_k, np.full(len(x_ood), -1, dtype=y_k.dtype)])

    clf = _simple_classifier().fit(x_train, y_train)
    preds = clf.predict(x_test)

    # Basic expectations:
    acc = (preds == y_test).mean()
    assert acc > 0.8, f"Expected >80 % overall accuracy, got {acc:.2f}"


def test_roc_like_property():
    (x_k, y_k), (x_ood, _) = _make_cluster_data(n_classes=2)
    x_train, y_train = x_k[:40], y_k[:40]
    clf = _simple_classifier().fit(x_train, y_train)

    scores = clf.decision_function(np.vstack([x_k, x_ood]))

    # Simple AUC approximation: score separation
    separation = scores[: len(x_k)].mean() - scores[len(x_k) :].mean()
    assert separation > 0.5, "Mahalanobis scores should separate ID from OOD"

>>>> test_reporting.py
import json
import math
import multiprocessing
import pathlib
import sqlite3
import time

import beartype
import pytest
from hypothesis import given
from hypothesis import strategies as st

from . import config, reporting

multiprocessing.set_start_method("spawn", force=True)


@st.composite
def _prediction_list(draw):
    """Generate an arbitrary non-empty list[Prediction] for single-label multiclass."""

    n = draw(st.integers(min_value=1, max_value=256))
    # Allow up to 50 distinct class IDs (0-50) - plenty for the property test.
    y_true = draw(
        st.lists(st.integers(min_value=0, max_value=50), min_size=n, max_size=n)
    )
    y_pred = draw(
        st.lists(st.integers(min_value=0, max_value=50), min_size=n, max_size=n)
    )

    preds = [
        reporting.Prediction(
            id=str(i),
            score=float(y_pred[i] == y_true[i]),
            info={"y_true": y_true[i], "y_pred": y_pred[i]},
        )
        for i in range(n)
    ]
    return preds


@given(preds=_prediction_list())
def test_micro_f1_equals_micro_accuracy(preds):
    """Micro-averaged F1 must equal micro accuracy for single-label data."""

    acc = reporting.micro_acc(preds)
    f1 = reporting.micro_f1(preds)

    # Floating math can introduce tiny error, so compare with tolerance
    assert math.isclose(acc, f1, rel_tol=1e-12, abs_tol=1e-12)


##############################
# Tests for SQLite Reporting #
##############################

NOW = 1_700_000_000.0  # fixed "current" time so tests are deterministic
HOUR = 3600


@beartype.beartype
def make_cfg(tmp_path: pathlib.Path) -> config.Experiment:
    """Return a minimal Experiment that points its report DB inside tmp_path."""
    return config.Experiment(
        model=config.Model(org="openai", ckpt="ViT-B/16"),
        n_train=-1,
        report_to=str(tmp_path),
        log_to=str(tmp_path / "logs"),
    )


@beartype.beartype
def _insert_experiment(db: sqlite3.Connection, cfg: config.Experiment, task: str):
    """Directly insert a row into experiments (used to pre-populate scenario 1)."""
    values = (
        task,
        cfg.model.org,
        cfg.model.ckpt,
        cfg.n_train,
        json.dumps(cfg.to_dict()),
        "[]",
        "deadbeef",
        time.time(),
        "",
        "pytest",
    )
    stmt = """
    INSERT INTO experiments
    (task_name, model_org, model_ckpt, n_train,
     exp_cfg, argv, git_commit, posix, gpu_name, hostname)
    VALUES (?,?,?,?,?,?,?,?,?,?)
    """
    db.execute(stmt, values)
    db.commit()


@beartype.beartype
def _insert_claim(
    db: sqlite3.Connection, cfg: config.Experiment, task: str, *, age_h: float | int
):
    """Insert a claim with `age_h` hours of staleness via claim_run."""
    # fake the clock *during* the INSERT
    t0 = time.time
    time.time = lambda: NOW - age_h * HOUR
    try:
        assert reporting.claim_run(db, cfg, task)  # must succeed
    finally:
        time.time = t0


def test_skip_when_experiment_exists(tmp_path):
    cfg = make_cfg(tmp_path)
    task = "plantnet"

    db = reporting.get_db(cfg)
    _insert_experiment(db, cfg, task)

    assert reporting.already_ran(db, cfg, task) is True


BUSY_TIMEOUT = 30
WAIT = BUSY_TIMEOUT + 5


def _worker(cfg: config.Experiment, task: str, q, succeed: bool):
    db = reporting.get_db(cfg)
    if reporting.claim_run(db, cfg, task):
        time.sleep(20)
        if succeed:
            _insert_experiment(db, cfg, task)
            reporting.release_run(db, cfg, task)
            q.put("winner")
        else:
            reporting.release_run(db, cfg, task)
            q.put("failed")
    else:
        q.put("skip")


def test_one_winner_many_launchers(tmp_path):
    cfg = make_cfg(tmp_path)
    task = "kabr"
    q = multiprocessing.Queue()

    n_workers = 4

    procs = [
        multiprocessing.Process(target=_worker, args=(cfg, task, q, True))
        for _ in range(n_workers)
    ]
    for p in procs:
        p.start()
    for p in procs:
        p.join(timeout=WAIT)

    results = [q.get(timeout=WAIT) for _ in range(n_workers)]
    assert results.count("winner") == 1
    assert results.count("skip") == n_workers - 1

    db = reporting.get_db(cfg)
    # experiments row present
    assert reporting.already_ran(db, cfg, task)
    # runs table cleaned up
    assert db.execute("SELECT COUNT(*) FROM runs").fetchone()[0] == 0


def test_reclaim_after_failure(tmp_path):
    cfg = make_cfg(tmp_path)
    task = "kabr"
    q = multiprocessing.Queue()

    n_workers = 4

    # wave 1: one process fails
    procs1 = [multiprocessing.Process(target=_worker, args=(cfg, task, q, False))]
    procs1.extend([
        multiprocessing.Process(target=_worker, args=(cfg, task, q, True))
        for i in range(n_workers - 1)
    ])
    for p in procs1:
        p.start()
        time.sleep(0.5)
    for p in procs1:
        p.join(timeout=40)

    results1 = [q.get(timeout=40) for _ in range(n_workers)]
    assert results1.count("failed") == 1

    # wave 2: slot should be claimable again
    procs2 = [
        multiprocessing.Process(target=_worker, args=(cfg, task, q, True))
        for _ in range(n_workers)
    ]
    for p in procs2:
        p.start()
    for p in procs2:
        p.join(timeout=40)

    results2 = [q.get(timeout=40) for _ in range(n_workers)]
    assert results2.count("winner") == 1

    db = reporting.get_db(cfg)
    assert reporting.already_ran(db, cfg, task)
    assert db.execute("SELECT COUNT(*) FROM runs").fetchone()[0] == 0


def test_empty_db(tmp_path):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    assert reporting.clear_stale_claims(db, max_age_hours=72) == 0


def test_fresh_kept(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=5)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 0
    assert reporting.is_claimed(db, cfg, "task")


def test_stale_removed_and_reclaimable(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=100)

    # can't claim while it's stale & present
    assert reporting.is_claimed(db, cfg, "task")

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 1
    # now slot is free again
    assert not reporting.is_claimed(db, cfg, "task")
    assert reporting.claim_run(db, cfg, "task")


def test_mixed_rows(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task1", age_h=10)
    _insert_claim(db, cfg, "task2", age_h=90)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db, max_age_hours=72) == 1
    assert reporting.is_claimed(db, cfg, "task1")
    assert not reporting.is_claimed(db, cfg, "task2")


@pytest.mark.parametrize("hrs", [71.99, 72.0])
def test_exact_cutoff_not_deleted(tmp_path, monkeypatch, hrs):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=hrs)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 0
    assert reporting.is_claimed(db, cfg, "task")


def test_custom_threshold(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=50)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db, max_age_hours=45) == 1


@pytest.mark.parametrize("bad", [0, -5])
def test_bad_threshold_raises(tmp_path, bad):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    with pytest.raises(ValueError):
        reporting.clear_stale_claims(db, max_age_hours=bad)


def test_idempotent(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=150)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 1
    # second pass should remove nothing
    assert reporting.clear_stale_claims(db) == 0


def test_rowcount_exact(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task1", age_h=120)
    _insert_claim(db, cfg, "task2", age_h=130)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 2


def test_future_schema_extra_column(tmp_path, monkeypatch):
    # add extra column to ensure helper ignores unknown columns
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    db.execute("ALTER TABLE runs ADD COLUMN note TEXT")
    _insert_claim(db, cfg, "task", age_h=100)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 1

>>>> test_smoke.py
def test_smoke_imports():
    """Test that all modules can be imported without errors."""
    import biobench
    import biobench.aimv2
    import biobench.beluga
    import biobench.beluga.download
    import biobench.config
    import biobench.fishnet
    import biobench.fishnet.download
    import biobench.fungiclef
    import biobench.fungiclef.download
    import biobench.fungiclef.metrics
    import biobench.helpers
    import biobench.herbarium19
    import biobench.herbarium19.download
    import biobench.imagenet1k
    import biobench.inat21
    import biobench.inat21.download
    import biobench.iwildcam
    import biobench.iwildcam.download
    import biobench.newt
    import biobench.newt.download
    import biobench.openset
    import biobench.plankton
    import biobench.plankton.download
    import biobench.plantnet
    import biobench.plantnet.download
    import biobench.rarespecies
    import biobench.registry
    import biobench.reporting
    import biobench.simpleshot
    import biobench.third_party_models
    import biobench.vjepa

    # Check that key classes/functions exist
    assert hasattr(biobench.registry, "VisionBackbone")
    assert hasattr(biobench.config, "Experiment")
    assert hasattr(biobench.fungiclef, "benchmark")


def test_basic_instantiation():
    """Test that basic objects can be instantiated."""
    from biobench.config import Data, Experiment, Model

    # Create minimal valid objects
    model = Model(org="test", ckpt="test")
    data = Data()
    experiment = Experiment(model=model, data=data, seed=42)

    # Verify they have expected properties
    assert experiment.model.org == "test"
    assert experiment.seed == 42

>>>> test_vjepa.py
import pytest
import torch

from . import config, registry

DTYPE = torch.float32
ATOL, RTOL = 1e-5, 1e-4

CKPTS = ["vitl16", "vith16", "vith16-384"]


@pytest.fixture(scope="session", params=CKPTS, ids=lambda x: x[0])
def model(request):
    model_ckpt = request.param
    return registry.load_vision_backbone(config.Model("vjepa", model_ckpt))


def rand(b: int = 2):
    torch.manual_seed(0)
    return torch.rand(b, 3, 224, 224, dtype=DTYPE)


def test_is_nn(model):
    assert isinstance(model, torch.nn.Module)


def test_smoke(model):
    x = rand()
    model.img_encode(x.squeeze(1))

>>>> testing.md
# Testing

**Session‑scoped, parametrised fixture** is the centrepiece of out tests.

## TL;DR

We turn the checkpoint list into a **parametrised fixture** so that pytest automatically generates an independent test item for every `(test‑function, checkpoint)` pair, caches the expensive model‑load once per checkpoint for the whole session, and gives us clean, filterable IDs in the test report. This is the idiomatic way to fan‑out identical assertions over multiple datasets or configurations, and it avoids manual loops or repeated downloads.

## Separate test items & readable reports
`@pytest.fixture(..., params=CKPTS)` makes pytest expand each consuming test into *N* items (one per checkpoint). The resulting IDs appear as `test_same_shape_single[apple/aimv2‑large…]`, which pinpoints failures and lets you run a single case with `-k`.

## Automatic caching
With `scope="session"` the fixture is created once per parameter and reused everywhere. Heavy objects (two models and their weights) load exactly one time, saving minutes of startup.

## First‑class pytest features
Because each checkpoint is a real test item you can mark or skip it (`pytest.param(..., marks=...)`) or xfail just one checkpoint. This is impossible if you write an inner `for`‑loop instead.

## Composability
Other fixtures (e.g. a random image generator) can depend on `models`; they inherit the same parameter seamlessly without extra wiring.

## Why *session* scope?

* **Most expensive setup wins**: loading two ViT‑L checkpoints dwarfs per‑test compute, so widest scope gives biggest pay‑off.
* **Stateless models**: evaluation‑mode transformers are read‑only, so sharing them across tests is safe; there’s no gradient accumulation or RNG mutation.
* **Disk cache friendliness**: the `cache_dir` argument points to your global HF cache, so downloads happen once even across separate pytest sessions.


## Quick mental model

1. **Fixture creation**
   * Pytest iterates over `CKPTS`; for each, runs the body once, caches the `(hf,bio)` pair.
2. **Test expansion**
   * For every test that requests `models`, pytest clones it per checkpoint, injecting the cached pair.
3. **Execution order**
   * Fixture builds happen before the first test needing them. If ten tests all use `models`, the models load only once per checkpoint.
4. **Random helper `_rand()`**
   * Generates deterministic tensors (seed=0) so the value comparison is reproducible.

Keep this in mind and the tests should still make sense even after a long hiatus.

>>>> third_party_models.py
import logging
import os

import beartype
import einops
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import helpers, registry

logger = logging.getLogger("third_party")


@beartype.beartype
def get_ssl() -> bool:
    """
    Checks whether BIOBENCH_DISABLE_SSL is present in the environment.

    We use environment variables rather than a boolean argument because

    1. This is only needed on some systems, like OSC.
    2. Every benchmark needs it in exactly the same way, so it would show up in every benchmark script as more "noise".
    3. It is not manipulated throughout the running of the program. It's a global variable that's set at the start of the jobs.

    But in general, we should not use environment variables to manage program state.

    Returns:
        A boolean that's true if we should use SSL and false if not.
    """
    disable = os.environ.get("BIOBENCH_DISABLE_SSL", None)
    return not disable


@jaxtyped(typechecker=beartype.beartype)
class OpenClip(registry.VisionBackbone):
    """
    Loads checkpoints from [open_clip](https://github.com/mlfoundations/open_clip), an open-source reproduction of the original [CLIP](https://arxiv.org/abs/2103.00020) paper.

    Checkpoints are in the format `<ARCH>/<CKPT>`.
    Look at the [results file](https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv) for the pretrained models.
    For example, to load a ViT-B/16 train on Apple's Data Filtering Networks dataset, you would use `ViT-B-16/dfn2b`.
    """

    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import open_clip

        if not get_ssl():
            logger.warning("Ignoring SSL certs. Try not to do this!")
            # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761
            # Ideally we don't have to disable SSL but we are only downloading weights.
            import ssl

            ssl._create_default_https_context = ssl._create_unverified_context

        if ckpt.startswith("hf-hub:"):
            clip, self.img_transform = open_clip.create_model_from_pretrained(ckpt)
        else:
            arch, ckpt = ckpt.split("/")
            clip, self.img_transform = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )

        self.model = clip.visual
        self.model.output_tokens = True  # type: ignore

    def make_img_transform(self):
        return self.img_transform

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        result = self.model(batch)
        # Sometimes the model does not return patch features if it has none.
        if isinstance(result, tuple):
            img, patches = result
            return registry.EncodedImgBatch(img, patches)
        else:
            return registry.EncodedImgBatch(result, None)


@jaxtyped(typechecker=beartype.beartype)
class Timm(registry.VisionBackbone):
    """
    Wrapper for models from the Timm (PyTorch Image Models) library.

    This class provides an interface to use any model from the Timm library
    as a vision backbone in the biobench framework.
    """

    # TODO: docs + describe the ckpt format.
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import timm

        self.ckpt = ckpt

        self.model = timm.create_model(ckpt, pretrained=True)

        data_cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)
        self.img_transform = timm.data.create_transform(**data_cfg)

    def make_img_transform(self):
        return self.img_transform

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        feats = self.model.forward_features(batch)
        if feats.ndim == 4:
            # This is probably a convnet of some kind, with (batch, dim, width, height)
            bsz, d, w, h = feats.shape

            # Validate the shape of the features
            if not (d > w and d > h):
                raise ValueError(
                    f"Expected feature dimensions (d={d}) to be larger than spatial dimensions (w={w}, h={h}). This suggests the tensor dimensions may be in an unexpected order."
                )

            if w != h:
                raise ValueError(
                    f"Expected equal spatial dimensions, but got width={w} and height={h}. Unequal spatial dimensions indicate a mistake in our understanding of the output features from model '{self.ckpt}'."
                )

            # Reshape to (batch, patches, dim) format
            patches = feats.permute(0, 2, 3, 1).reshape(bsz, w * h, d)
            # TODO: should we only use max pooling?
            img = patches.max(dim=1).values  # Global max pooling
        elif feats.ndim == 3:
            # This is probably a ViT with (batch, patches, dim)
            bsz, num_patches, d = feats.shape
            patches = feats

            # For ViT models, we typically use the class token ([CLS]) as the image representation
            # if it exists, otherwise we do mean pooling over patches
            if self.model.num_prefix_tokens > 0:
                img = patches[:, 0]  # Use [CLS] token
                # Remove prefix tokens (like [CLS]) from patches
                patches = patches[:, self.model.num_prefix_tokens :]
            else:
                img = patches.max(dim=1).values  # Max pooling if no [CLS] token
        else:
            raise ValueError(
                f"Unexpected feature dimension: {feats.ndim}. Expected either 3 (ViT models) or 4 (ConvNet models). Check if the model architecture {self.ckpt} is supported."
            )

        return registry.EncodedImgBatch(img, patches)


@jaxtyped(typechecker=beartype.beartype)
class DinoV2(registry.VisionBackbone):
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()

        import torch

        self.model = torch.hub.load("facebookresearch/dinov2", ckpt)

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        dct = self.model.forward_features(batch)

        return registry.EncodedImgBatch(
            dct["x_norm_clstoken"], dct["x_norm_patchtokens"]
        )

    def make_img_transform(self):
        import torch
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=(256, 256)),
            v2.CenterCrop(size=(224, 224)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])


@jaxtyped(typechecker=beartype.beartype)
class TorchvisionModel(registry.VisionBackbone):
    def __init__(self, ckpt: str):
        import torchvision

        arch, weights = ckpt.split("/")
        self.model = getattr(torchvision, arch)(weights=weights)
        self.model.eval()

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        breakpoint()


@jaxtyped(typechecker=beartype.beartype)
class SAM2(registry.VisionBackbone):
    """
    A very small wrapper around the SAM-2 Hiera backbones exposed by `timm`.

    Design choices:

    * We rely 100 % on `timm` for model construction & weight loading (`timm.create_model("hf_hub:timm/<model_name>", pretrained=True)`).
    * The image transform is the exact one timm used during pre-training: `data.create_transform(**resolve_data_config(model.pretrained_cfg))`.
    """

    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import timm

        self.ckpt = ckpt

        self.model = timm.create_model(f"hf_hub:timm/{ckpt}", pretrained=True)

        self.data_cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)

    def make_img_transform(self):
        import timm

        return timm.data.create_transform(**self.data_cfg)

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        x = self.model.forward_features(batch)
        x = einops.rearrange(x, "b w h d -> b (w h) d")
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

>>>> vjepa.py
"""
# V-JEPA *frozen-feature* backbone port

This file is a self-contained re-implementation of the encoder used in *V-JEPA* (Meta FAIR, 2024) for frozen downstream evaluation. The design follows the public facebookresearch/jepa reference code but strips it down to the minimum needed for image-only tasks.

Key implementation choices
--------------------------
1. Most modules are verbatim (or lightly edited for typing/PEP-8) copies from the upstream repo so that we do *not* depend on the unpublished PyPI package:

2. The official frozen image classification script (https://github.com/facebookresearch/jepa/blob/51c59d518fc63c08464af6de585f78ac0c7ed4d5/evals/image_classification_frozen/eval.py#L451-L455) repeats a still image along the temporal axis before feeding it to the video-ViT. We reproduce that behaviour with `x = einops.repeat(batch, "b c h w -> b c f h w", f=self.n_frames)` so the model sees a 16-frame clip of identical images.

3. Checkpoints live at `https://dl.fbaipublicfiles.com/jepa/<ckpt>/<ckpt>.pth.tar`. We download them into an `$CACHE/vjepa` sub-folder (`download()` helper) to avoid git-annex or HF dependencies.

4. Only the EMA target encoder (`state["target_encoder"]`) is loaded, mirroring the authors' evaluation code.  The usual `module.` prefix is stripped so that the state dict matches our local module names.

5. `img_encode()` returns both the full patch grid and a **max-pooled** global descriptor (`x.max(dim=1).values`).  The attentive classifier used in the paper is *not* re-implemented here; you can bolt your own head on top of the returned per-patch features.

## Limitations / divergences from FAIR reference

* No mixed-precision, distributed training or attentive probe head. This module is encoder-only.
* Patch/Tubelet shapes are fixed (16x16x2) and `num_frames` is pinned to 16; if you need other variants, expose them through the constructor.
* Only the three public checkpoints (`vitl16`, `vith16`, `vith16-384`) are supported, but extending to future releases is one line in `__init__`.
"""

import functools
import math
import os
import pathlib

import beartype
import einops
import numpy as np
import requests
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import helpers, registry


@beartype.beartype
def get_3d_sincos_pos_embed(
    embed_dim, grid_size, grid_depth, cls_token=False, uniform_power=False
):
    """
    grid_size: int of the grid height and width
    grid_depth: int of the grid depth
    returns:
        pos_embed: [grid_depth*grid_size*grid_size, embed_dim] (w/o cls_token)
                or [1+grid_depth*grid_size*grid_size, embed_dim] (w/ cls_token)
    """
    grid_d = np.arange(grid_depth, dtype=float)
    grid_h = np.arange(grid_size, dtype=float)
    grid_w = np.arange(grid_size, dtype=float)
    grid_h, grid_d, grid_w = np.meshgrid(
        grid_h, grid_d, grid_w
    )  # order of meshgrid is very important for indexing as [d,h,w]

    if not uniform_power:
        h_embed_dim = embed_dim // 4
        w_embed_dim = embed_dim // 4
        d_embed_dim = embed_dim // 2
    else:
        h_embed_dim = w_embed_dim = d_embed_dim = int(np.ceil(embed_dim / 6) * 2)

    emb_h = get_1d_sincos_pos_embed_from_grid(h_embed_dim, grid_h)  # (T*H*W, D1)
    emb_w = get_1d_sincos_pos_embed_from_grid(w_embed_dim, grid_w)  # (T*H*W, D2)
    emb_d = get_1d_sincos_pos_embed_from_grid(d_embed_dim, grid_d)  # (T*H*W, D3)
    pos_embed = np.concatenate([emb_d, emb_h, emb_w], axis=1)
    pos_embed = pos_embed[:, :embed_dim]
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


@beartype.beartype
def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    returns:
        pos_embed: [grid_size*grid_size, embed_dim] (w/o cls_token)
                or [1+grid_size*grid_size, embed_dim] (w/ cls_token)
    """
    grid_h = np.arange(grid_size, dtype=float)
    grid_w = np.arange(grid_size, dtype=float)
    grid_w, grid_h = np.meshgrid(
        grid_w, grid_h
    )  # order of meshgrid is very important for indexing as [h, w]

    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_h)  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_w)  # (H*W, D/2)
    pos_embed = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    embed_dim: output dimension for each position
    grid_size: int of the grid length
    returns:
        pos_embed: [grid_size, embed_dim] (w/o cls_token)
                or [1+grid_size, embed_dim] (w/ cls_token)
    """
    grid = np.arange(grid_size, dtype=float)
    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    returns: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=float)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum("m,d->md", pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out)  # (M, D/2)
    emb_cos = np.cos(out)  # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


@beartype.beartype
def trunc_normal_(
    tensor: Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0
) -> Tensor:
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


@beartype.beartype
class PatchEmbed(torch.nn.Module):
    """
    Image to Patch Embedding
    """

    def __init__(self, patch_size: int = 16, in_chans: int = 3, embed_dim: int = 768):
        super().__init__()
        self.patch_size = patch_size
        self.proj = torch.nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


@beartype.beartype
class PatchEmbed3D(torch.nn.Module):
    """
    Image to Patch Embedding
    """

    def __init__(
        self,
        patch_size: int = 16,
        tubelet_size: int = 2,
        in_chans: int = 3,
        embed_dim: int = 768,
    ):
        super().__init__()
        self.patch_size = patch_size
        self.tubelet_size = tubelet_size

        self.proj = torch.nn.Conv3d(
            in_channels=in_chans,
            out_channels=embed_dim,
            kernel_size=(tubelet_size, patch_size, patch_size),
            stride=(tubelet_size, patch_size, patch_size),
        )

    def forward(self, x):
        B, C, T, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


@beartype.beartype
class MLP(torch.nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=torch.nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = torch.nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = torch.nn.Linear(hidden_features, out_features)
        self.drop = torch.nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


@beartype.beartype
class Attention(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        qk_scale=None,
        attn_drop: float = 0.0,
        proj_drop: float = 0.0,
        use_sdpa: bool = True,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5
        self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = torch.nn.Dropout(attn_drop)
        self.proj = torch.nn.Linear(dim, dim)
        self.proj_drop_prob = proj_drop
        self.proj_drop = torch.nn.Dropout(proj_drop)
        self.use_sdpa = use_sdpa

    def forward(self, x, mask=None):
        B, N, C = x.shape
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, C // self.num_heads)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, num_heads, N, D]

        if self.use_sdpa:
            x = torch.nn.functional.scaled_dot_product_attention(
                q, k, v, dropout_p=self.proj_drop_prob
            )
            attn = None
        else:
            attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, D, D]
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn


@beartype.beartype
class Block(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias=False,
        qk_scale=None,
        drop: float = 0.0,
        attn_drop: float = 0.0,
        act_layer=torch.nn.GELU,
        norm_layer=torch.nn.LayerNorm,
        grid_size=None,
        grid_depth=None,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
        )

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
        )

    def forward(self, x, return_attention=False, mask=None):
        y, attn = self.attn(self.norm1(x), mask=mask)
        if return_attention:
            return attn
        x = x + y
        x = x + self.mlp(self.norm2(x))
        return x


@beartype.beartype
class VisionTransformer(torch.nn.Module):
    """Vision Transformer"""

    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        num_frames: int = 1,
        tubelet_size: int = 2,
        in_chans: int = 3,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        qk_scale=None,
        drop_rate: float = 0.0,
        attn_drop_rate: float = 0.0,
        norm_layer=torch.nn.LayerNorm,
        init_std: float = 0.02,
        out_layers=None,
        uniform_power=False,
        **kwargs,
    ):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.out_layers = out_layers

        self.input_size = img_size
        self.patch_size = patch_size

        self.num_frames = num_frames
        self.tubelet_size = tubelet_size
        self.is_video = num_frames > 1

        grid_size = self.input_size // self.patch_size
        grid_depth = self.num_frames // self.tubelet_size

        # Tokenize pixels with convolution
        if self.is_video:
            self.patch_embed = PatchEmbed3D(
                patch_size=patch_size,
                tubelet_size=tubelet_size,
                in_chans=in_chans,
                embed_dim=embed_dim,
            )
            self.num_patches = (
                (num_frames // tubelet_size)
                * (img_size // patch_size)
                * (img_size // patch_size)
            )
        else:
            self.patch_embed = PatchEmbed(
                patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim
            )
            self.num_patches = (img_size // patch_size) * (img_size // patch_size)

        # Position embedding
        self.uniform_power = uniform_power
        self.pos_embed = None
        self.pos_embed = torch.nn.Parameter(
            torch.zeros(1, self.num_patches, embed_dim), requires_grad=False
        )

        # Attention Blocks
        self.blocks = torch.nn.ModuleList([
            Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                act_layer=torch.nn.GELU,
                grid_size=grid_size,
                grid_depth=grid_depth,
                attn_drop=attn_drop_rate,
                norm_layer=norm_layer,
            )
            for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)

        # ------ initialize weights
        if self.pos_embed is not None:
            self._init_pos_embed(self.pos_embed.data)  # sincos pos-embed
        self.init_std = init_std
        self.apply(self._init_weights)
        self._rescale_blocks()

    def _init_pos_embed(self, pos_embed):
        embed_dim = pos_embed.size(-1)
        grid_size = self.input_size // self.patch_size
        if self.is_video:
            grid_depth = self.num_frames // self.tubelet_size
            sincos = get_3d_sincos_pos_embed(
                embed_dim,
                grid_size,
                grid_depth,
                cls_token=False,
                uniform_power=self.uniform_power,
            )
        else:
            sincos = get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False)
        pos_embed.copy_(torch.from_numpy(sincos).float().unsqueeze(0))

    def _init_weights(self, m):
        if isinstance(m, torch.nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, torch.nn.Linear) and m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.LayerNorm):
            torch.nn.init.constant_(m.bias, 0)
            torch.nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, torch.nn.Conv2d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.Conv3d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)

    def _rescale_blocks(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def get_num_layers(self):
        return len(self.blocks)

    def no_weight_decay(self):
        return {}

    def forward(self, x):
        """
        :param x: input image/video
        """

        # Tokenize input
        pos_embed = self.pos_embed
        if pos_embed is not None:
            pos_embed = self.interpolate_pos_encoding(x, pos_embed)
        x = self.patch_embed(x)
        if pos_embed is not None:
            x += pos_embed
        B, N, D = x.shape

        # Fwd prop
        outs = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if self.out_layers is not None and i in self.out_layers:
                outs.append(self.norm(x))

        if self.out_layers is not None:
            return outs

        if self.norm is not None:
            x = self.norm(x)

        return x

    def interpolate_pos_encoding(self, x, pos_embed):
        _, N, dim = pos_embed.shape

        if self.is_video:
            # If pos_embed already corret size, just return
            _, _, T, H, W = x.shape
            if H == self.input_size and W == self.input_size and T == self.num_frames:
                return pos_embed

            # Convert depth, height, width of input to be measured in patches
            # instead of pixels/frames
            T = T // self.tubelet_size
            H = H // self.patch_size
            W = W // self.patch_size

            # Compute the initialized shape of the positional embedding measured
            # in patches
            N_t = self.num_frames // self.tubelet_size
            N_h = N_w = self.input_size // self.patch_size
            assert N_h * N_w * N_t == N, "Positional embedding initialized incorrectly"

            # Compute scale factor for spatio-temporal interpolation
            scale_factor = (T / N_t, H / N_h, W / N_w)

            pos_embed = torch.nn.functional.interpolate(
                pos_embed.reshape(1, N_t, N_h, N_w, dim).permute(0, 4, 1, 2, 3),
                scale_factor=scale_factor,
                mode="trilinear",
            )
            pos_embed = pos_embed.permute(0, 2, 3, 4, 1).view(1, -1, dim)
            return pos_embed

        else:
            # If pos_embed already corret size, just return
            _, _, H, W = x.shape
            if H == self.input_size and W == self.input_size:
                return pos_embed

            # Compute scale factor for spatial interpolation
            npatch = (H // self.patch_size) * (W // self.patch_size)
            scale_factor = math.sqrt(npatch / N)

            pos_embed = torch.nn.functional.interpolate(
                pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                    0, 3, 1, 2
                ),
                scale_factor=scale_factor,
                mode="bicubic",
            )
            pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
            return pos_embed


def vit_large(patch_size: int = 16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4.0,
        qkv_bias=True,
        norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model


def vit_huge(patch_size: int = 16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size,
        embed_dim=1280,
        depth=32,
        num_heads=16,
        mlp_ratio=4.0,
        qkv_bias=True,
        norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model


@jaxtyped(typechecker=beartype.beartype)
class VJEPA(registry.VisionBackbone):
    def __init__(self, ckpt: str):
        super().__init__()
        self.n_frames = 16
        if ckpt == "vitl16":
            size = 224
            vit = vit_large(img_size=size, num_frames=self.n_frames)
        elif ckpt == "vith16":
            size = 224
            vit = vit_huge(img_size=size, num_frames=self.n_frames)
            size = 224
        elif ckpt == "vith16-384":
            size = 384
            vit = vit_huge(img_size=size, num_frames=self.n_frames)
        else:
            raise ValueError(f"ckpt '{ckpt}' not recognized.")
        self.backbone = vit
        self.size = size

        ckpt_url = f"https://dl.fbaipublicfiles.com/jepa/{ckpt}/{ckpt}.pth.tar"
        state = torch.load(download(ckpt_url), map_location="cpu")
        state = {
            k.replace("module.", ""): v for k, v in state["target_encoder"].items()
        }
        self.load_state_dict(state)

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        x = einops.repeat(batch, "b c w h -> b c f w h", f=self.n_frames)
        x = self.backbone(x)

        # Reshape to (b, D, N, C) then average over D=8
        depth = self.n_frames // 2  # 8
        x = einops.rearrange(x, "b (d n) c -> b d n c", d=depth)
        x = x.mean(dim=1)

        # Return image features.
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

    def make_img_transform(self):
        import torch
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=self.size),
            v2.CenterCrop(self.size),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])


@beartype.beartype
def download(url: str, *, force: bool = False) -> pathlib.Path:
    root = pathlib.Path(helpers.get_cache_dir()) / "vjepa"
    root.mkdir(parents=True, exist_ok=True)
    fname = root / os.path.basename(url)
    if not fname.exists() or force:
        with requests.get(url, stream=True) as r, open(fname, "wb") as f:
            r.raise_for_status()
            for chunk in r.iter_content(1 << 20):
                f.write(chunk)
    return fname

