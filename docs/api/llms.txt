>>>> AGENTS.md
# Code Style

- Keep code simple, explicit, typed, test-driven, and ready for automation.
- Source files are UTF-8 but must contain only ASCII characters. Do not use smart quotes, ellipses, em-dashes, emoji, or other non-ASCII glyphs.
- Docstrings are a single unwrapped paragraph. Rely on your editor's soft-wrap.
- Prefer explicit over implicit constructs. No wildcard imports.

```python
import numpy as np
import polars as pl  # use Polars instead of Pandas
```

Always reference modules by their alias. Never use `from X import *`.

- Decorate every public function or class with `@beartype.beartype`.
- For tensors, add `@jaxtyped(typechecker=beartype.beartype)` and use `jaxtyping` shapes.
- Use frozen `@dataclasses.dataclass` for data containers such as `Config` or `Args`.
- Classes use `CamelCase`, for example `Dataset` or `FeatureExtractor`.
- Functions and variables use `snake_case`, for example `download_split` or `md5_of_file`.
- Constants are `UPPER_SNAKE`, defined at module top, for example `URLS = {...}`.
- File descriptors end in `fd`, for example `log_fd`.
- File paths end in `_fpath`; directories end in `_dpath`.
- Constructors follow verb prefixes:
  - `make_...` returns an object.
  - `get_...` returns a primitive value such as a string or path.
  - `setup_...` performs side effects and returns nothing.

## Shape-suffix notation

Attach suffixes to tensor variables to clarify shape:

- B – batch size
- W – patch grid width
- H – patch grid height
- D – feature dimension
- L – number of latents
- C – number of classes

Example: `acts_BWHD` has shape (batch, width, height, d).

## Logging and progress bars

```python
logger = logging.getLogger(__name__)
logger.info("message")

for x in helpers.progress(dataset):
    ...
```

Use `helpers.progress` instead of `tqdm` so that logging is useful in non-interactive contexts (log files, batch jobs, etc).

# Testing

- Use pytest with fixtures and parameterization.
- Use Hypothesis for property-based tests, especially in helpers.
- Mark slow integration tests with `@pytest.mark.slow`.

# Project layout

Each task lives in its own folder, for example `biobench/herbarium19/`.  
Inside a task folder:

- `download.py` fetches the dataset.
- `__init__.py` exposes the task API.

`__init__.py` includes a

```py
@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    ...
    

@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["TASKNAME"]
```

To add a new task, both of these functions must be implemented.

Some guidelines:

* You probably will record a bunch of frozen features, then do some CPU-only processing. If you do, call `torch.cuda.empty_cache()` to free up PyTorch's CUDA memory usage so that other users can use the GPU while your CPU-only work happens.
* Use `helpers.auto_batch_size()` to figure out an optimal batch size.
* Call `torch.compile()` after the batch size is determined. `torch.compile` records an optimized version for each forward pass, so each different batch size results in a new "compilation".

## Download scripts

- Start each downloader with a header line `/// script` and a `dependencies = [...]` list.
- Stream with `requests.get(..., stream=True)` and wrap in `tqdm` for progress.
- Verify checksums before extraction.

## Command Runner

A `justfile` is provided only as convenient documentation.
Do **not** call `just` (the binary may be missing).
Instead:

1. Open `justfile`, locate the target you need.
2. Copy the shell lines that follow the target and execute them directly.

Example

```sh
# don’t run
just test

# do run
uv run ruff format --preview .
uv run pytest --cov shhelp --cov-report term
```

>>>> CONTRIBUTING.md
# CONTRIBUTING

## 1. TL;DR

Install [uv](https://docs.astral.sh/uv/).
Clone this repository, then from the root directory:

```sh
uv run python -m saev --help
```

You also need [yek](https://github.com/bodo-run/yek) and [lychee](https://github.com/lycheeverse/lychee) for generating docs.

If you want to do any of the web interface work, you need [elm](https://guide.elm-lang.org/install/elm.html), [elm-format](https://github.com/avh4/elm-format/releases/latest), [tailwindcss](https://github.com/tailwindlabs/tailwindcss/releases/latest) and [bun](https://bun.sh/).

## 2. Repo Layout

```
benchmark.py  <- Launch script for benchmarking models on tasks.
report.py     <- Launch script for producing the report.json file.
biobench/     <- Source code for benchmarking.
  beluga/
    __init__.py
    download.py
  fishnet/
    __init__.py
    download.py
  ...
  config.py
  registry.py
  reporting.py
  schema.sql
  third_party_models.py
docs/
  api/
  assets/
  data/
  research/
  todo/
  index.html
web/
  src/
    Leaderboard.elm
  index.html
  main.css
```

## 3. Coding Style & Conventions

See [AGENTS.md](AGENTS.md).

## 4. Testing & Linting

1. Run `just test`.
2. Check that there are no regressions. Unless you are certain tests are not needed, the coverage % should either stay the same or increase.
3. Run `just docs`.
4. Fix any missing doc links.

## 5. Code of Conduct

Be polite, kind and assume good intent.

>>>> README.md
# Biology Benchmark (`biobench`)

![Coverage](docs/assets/coverage.svg)

This library is an easy-to-read benchmark for biology-related computer vision tasks.

It aims to make it easy to:

1. Evaluate new models.
2. Add new tasks.
3. Understand meaningful (or not) differences in model performance.

Check out the [docs](https://samuelstevens.me/biobench/) for an introduction.

## Getting Started

I use [uv](https://docs.astral.sh/uv/) for Python which makes it easy to manage Python versions, dependencies, virtual environments, etc.

To install uv, run `curl -LsSf https://astral.sh/uv/install.sh | sh`.

Then download at least one of the dataset.
NeWT is really easy to download.

```sh
uv run biobench/newt/download.py --dir ./newt
```

Download it wherever you want on your own filesystem.

## Why?

**For computational biologists:** biobench gives you an overview of how different models perform on different tasks. If you have a concrete task that you need to solve, you can easily write a script that matches other, existing tasks and then evaluate many different models on your task. If you have an idea of a task, you can find the most similar existing task(s) on the leaderboard and compare model performance.

**For computer vision researchers:** biobench is a realistic set of benchmarks that more accurately reflect how your model will be used by downstream users. If you aim to train a new foundation vision model, be aware that downstream users will likely not fine-tune it, and will instead use the image embeddings to do all sorts of weird things. Your foundation model should output representations that are universally useful; biobench lets you measure to what degree this is true.

## Concrete Goals

*Easy*, *fast*, *reproducible*, *understandable* evaluation of PyTorch computer vision models across a suite of realistic biology-related vision tasks.

- *Easy*: one launch script, with all options documented in the code and in auto-generated web documentation.
- *Fast*: Each evaluation takes at most 1 hour of A100 or A6000 time. There might be $n$ evaluations, so $n$ hours of A100, but it is embarrassingly parallel and the launch script supports easy parallel running and reporting.
- *Reproducible*: the results include instructions to regenerate these results from scratch, assuming access to the `biobench` Git repo and that web dependencies have not changed.[^web-deps]
- *Understandable*: results are in a machine-readable format, but include a simple human-readable notebook for reading. Common analyses (mean score across all tasks) are included in the notebook and take under one second to run.

[^web-deps]: Web dependencies include things like datasets being available from their original source, Huggingface datasets can be re-downloaded, model checkpoints do not change, etc.


We at [Imageomics](https://imageomics.osu.edu) use this library for testing [BioCLIP](https://imageomics.github.io/bioclip) and other internal models  during development.
Because of this, there are two main classes of tasks:

1. Downstream applications. These are tasks like [KABR](https://samuelstevens.me/biobench/api/biobench/kabr) or [Beluga whale re-ID](https://samuelstevens.me/biobench/api/biobench/beluga). These tasks represent real problems that computer vision systems fail to solve today.
2. Benchmarks. These are made-up tasks like [NeWT](https://samuelstevens.me/biobench/api/biobench/newt) that are artificial tasks, created to help us understand how useful a model might be in the real world for similar tasks.


## Road Map

1. Add contributing guide.
2. Add example images for each task to the docs.
3. Add 5-shot RareSpecies with simpleshot (like in BioCLIP paper). This is blocked because the Huggingface dataset doesn't work ([see this issue](https://huggingface.co/datasets/imageomics/rare-species/discussions/8)).
4. Add FishVista for localized trait prediction. This is another non-classification task, and we are specifically interested in traits. But it will take more work because we have to match bounding boxes and patch-level features which is challenging after resizes.

## Additional Tasks

[Counting insects on sticky insect traps](https://github.com/md-121/yellow-sticky-traps-dataset)
[Predicting plant stem angle](https://plantvision.unl.edu/datasets/download-panicoid-phenomap-1-dataset/)

## Contributing New Tasks

We welcome new tasks.
Here are a few guidelines for doing that.

Choose a task that offers new signal. We want tasks that:

* Uses a sensor or modality we do not cover (thermal, sonar, hyperspectral, LiDAR, microscopy, drone video, and so on),
* Introduces a different prediction type (counts, traits, time series, segmentation, ordinal labels),
* Or targets an under-represented group or environment (marine life, airborne organisms, underground roots, cell imagery).

Stay within our contraints:

* Evaluation must run on frozen image embeddings with a lightweight probe (logistic/linear, small MLP, or similar). See the `biobench.registry.VisionBackbone` class for the API that models conform to.
* A ViT-L/14 checkpoint should finish your task in under two hours on a single A6000 or A100 GPU.
* Data must be publicly downloadable and licensed for academic use; we redistribute predictions.

Match the style:

* `download.py` fetches the dataset and verifies checksums.
* `__init__.py` runs the benchmark, defines the bootstrapped evaluation metric.

If the task is simply another RGB species classification challenge, it probably fits better in iNat. Counting fish in noisy sonar frames or predicting tree-ring widths from microscopy slides—those are the kinds of additions we welcome.


>>>> REGRESSIONS.md
# Regressions

Last checked: 2025-05-31

# 6 failing test(s)

- biobench/jobkit/test_hooks.py::test_discarded_claims_on_one_hook_do_not_affect_others
- biobench/jobkit/test_hooks.py::test_lock_prevents_set_mutation_during_discards
- biobench/jobkit/test_hooks.py::test_release_run_invokes_callback_exactly_each_time
- biobench/jobkit/test_hooks.py::test_sigterm_handler_releases_only_current_claims
- biobench/test_openset.py::test_min_mahalanobis_sq_batched_equal
- biobench/test_openset.py::test_min_mahalanobis_sq_batched_no_error
# Coverage

Coverage: 3210/5326 lines (60.3%)

>>>> __init__.py
""" """

import typing

import tyro

from . import aimv2, third_party_models, vjepa
from .registry import list_vision_backbones, register_vision_backbone

register_vision_backbone("timm", third_party_models.Timm)
register_vision_backbone("open-clip", third_party_models.OpenClip)
register_vision_backbone("dinov2", third_party_models.DinoV2)
register_vision_backbone("sam2", third_party_models.SAM2)
register_vision_backbone("aimv2", aimv2.AIMv2)
register_vision_backbone("vjepa", vjepa.VJEPA)

# Some helpful types
if typing.TYPE_CHECKING:
    # Static type seen by language servers, type checkers, etc.
    ModelOrg = str
else:
    # Runtime type used by tyro.
    ModelOrg = tyro.extras.literal_type_from_choices(list_vision_backbones())

>>>> aimv2.py
import dataclasses
import json
import os
import re

import beartype
import requests
import safetensors.torch
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import helpers, registry


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """This is the configuration class to store the configuration of an `AIMv2Model`.

    Instantiating a configuration with the defaults will yield a similar configuration to that of the [apple/aimv2-large-patch14-224](https://huggingface.co/apple/aimv2-large-patch14-224).

    Args:
        hidden_size: Dimension of the hidden representations.
        intermediate_size: Dimension of the SwiGLU representations.
        num_hidden_layers: Number of hidden layers in the Transformer.
        num_attention_heads: Number of attention heads for each attention layer in the Transformer.
        num_channels: Number of input channels.
        image_size: Image size.
        patch_size: Patch size.
        rms_norm_eps: Epsilon value used for the RMS normalization layer.
        attention_dropout: Dropout ratio for attention probabilities.
        projection_dropout: Dropout ratio for the projection layer after the attention.
        torch_dtype: Data type.
        qkv_bias: Whether to add a bias to the queries, keys and values.
        use_bias: Whether to add a bias in the feed-forward and projection layers.
    """

    hidden_size: int
    intermediate_size: int
    num_hidden_layers: int
    num_attention_heads: int
    num_channels: int
    image_size: int
    patch_size: int
    rms_norm_eps: float
    attention_dropout: float
    projection_dropout: float
    torch_dtype: str
    qkv_bias: bool
    use_bias: bool


@beartype.beartype
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

    def extra_repr(self) -> str:
        return f"{tuple(self.weight.shape)}, eps={self.eps}"

    def _norm(self, x: torch.Tensor) -> torch.Tensor:
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)


@jaxtyped(typechecker=beartype.beartype)
class SwiGLUFFN(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()

        self.fc1 = torch.nn.Linear(
            cfg.hidden_size, cfg.intermediate_size, bias=cfg.use_bias
        )
        self.fc2 = torch.nn.Linear(
            cfg.intermediate_size, cfg.hidden_size, bias=cfg.use_bias
        )
        self.fc3 = torch.nn.Linear(
            cfg.hidden_size, cfg.intermediate_size, bias=cfg.use_bias
        )

    def forward(self, x: Float[Tensor, "*batch d"]) -> Float[Tensor, "*batch d"]:
        x = torch.nn.functional.silu(self.fc1(x)) * self.fc3(x)
        x = self.fc2(x)
        return x


@jaxtyped(typechecker=beartype.beartype)
class PatchEmbed(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.proj = torch.nn.Conv2d(
            cfg.num_channels,
            cfg.hidden_size,
            kernel_size=(cfg.patch_size, cfg.patch_size),
            stride=(cfg.patch_size, cfg.patch_size),
        )
        self.norm = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.proj(x).flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x


@jaxtyped(typechecker=beartype.beartype)
class ViTPreprocessor(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        num_patches = (cfg.image_size // cfg.patch_size) ** 2

        self.patchifier = PatchEmbed(cfg)
        self.pos_embed = torch.nn.Parameter(
            torch.zeros((1, num_patches, cfg.hidden_size))
        )

    def forward(
        self, x: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches d"]:
        tokens = self.patchifier(x)
        _, N, _ = tokens.shape
        pos_embed = self.pos_embed.to(tokens.device)
        tokens = tokens + pos_embed[:, :N]
        return tokens


@jaxtyped(typechecker=beartype.beartype)
class Attention(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        dim = cfg.hidden_size

        self.num_heads = cfg.num_attention_heads
        self.qkv = torch.nn.Linear(dim, dim * 3, bias=cfg.qkv_bias)
        self.attn_drop = torch.nn.Dropout(cfg.attention_dropout)
        self.proj = torch.nn.Linear(dim, dim, bias=cfg.use_bias)
        self.proj_drop = torch.nn.Dropout(cfg.projection_dropout)

    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        B, N, C = x.shape
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, C // self.num_heads)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = qkv.unbind(0)

        x = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask)
        x = x.transpose(1, 2).contiguous().reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


@jaxtyped(typechecker=beartype.beartype)
class Block(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.attn = Attention(cfg)
        self.norm_1 = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)
        self.mlp = SwiGLUFFN(cfg)
        self.norm_2 = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)

    def forward(
        self, x: torch.Tensor, mask: torch.Tensor | None = None
    ) -> torch.Tensor:
        x = x + self.attn(self.norm_1(x), mask)
        x = x + self.mlp(self.norm_2(x))
        return x


@jaxtyped(typechecker=beartype.beartype)
class Transformer(torch.nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.blocks = torch.nn.ModuleList([
            Block(cfg) for _ in range(cfg.num_hidden_layers)
        ])
        self.post_trunk_norm = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)

    def forward(self, tokens: Float[Tensor, "..."]) -> Float[Tensor, "..."]:
        for block in self.blocks:
            tokens = block(tokens)
        tokens = self.post_trunk_norm(tokens)
        return tokens


@jaxtyped(typechecker=beartype.beartype)
class AIMv2(registry.VisionBackbone):
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()

        # Config
        with open(download_hf_file(ckpt, "config.json"), "r") as fd:
            cfg_dct = json.load(fd)
        for key in ("architectures", "auto_map", "transformers_version"):
            cfg_dct.pop(key)
        assert cfg_dct.pop("model_type") == "aimv2"
        cfg = Config(**cfg_dct)

        # Model
        self.preprocessor = ViTPreprocessor(cfg)
        self.trunk = Transformer(cfg)

        # Pre-trained weights
        ckpt_fpath = download_hf_file(ckpt, "model.safetensors")
        state_dict = safetensors.torch.load_file(ckpt_fpath)
        self.load_state_dict(state_dict)

        # Extract image size from checkpoint name using regex

        match = re.search(r"patch\d+-(\d+)", ckpt)
        self.size = int(match.group(1)) if match else 224  # Default to 224 if not found

    def forward(self, x: Float[Tensor, "..."]) -> Float[Tensor, "..."]:
        x = self.preprocessor(x)
        x = self.trunk(x)
        return x

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        x = self.forward(batch)
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

    def make_img_transform(self):
        import torch
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=self.size),
            v2.CenterCrop(size=(self.size, self.size)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711],
            ),
        ])


@beartype.beartype
def download_hf_file(ckpt: str, filepath: str, *, force: bool = False) -> str:
    """
    Download a file from a Hugging Face model repository.

    Args:
        ckpt: The model checkpoint identifier (e.g., 'apple/aimv2-large-patch14-224')
        filepath: The path to the file within the repo (e.g., 'config.json')
        force: Whether to force download even if the file exists locally

    Returns:
        The path to the downloaded file on the local filesystem
    """

    # Construct the URL
    url = f"https://huggingface.co/{ckpt}/resolve/main/{filepath}"

    # Create the local path
    cache_dir = helpers.get_cache_dir()
    local_dir = os.path.join(cache_dir, "hf", ckpt)
    local_path = os.path.join(local_dir, filepath)

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(local_path), exist_ok=True)

    # Check if the file exists
    if os.path.exists(local_path) and not force:
        return local_path

    # Download the file
    response = requests.get(url, stream=True)
    response.raise_for_status()

    with open(local_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

    return local_path

>>>> beluga/__init__.py
"""
Individual re-identification of Beluga whales (*Delphinapterus leucas*) using [this LILA BC dataset](https://lila.science/datasets/beluga-id-2022/).

We use a very simple method:

1. Embed all images using a vision backbone.
2. For each image, treat it as a test image and find its nearest neighbor (k=1).
3. Give a score of 1.0 if the nearest neighbor is the same individual, otherwise 0.0.

You could improve this with nearest centroid classification, k>1, or any number of fine-tuning techniques.
But we are simply interested in seeing if models embed images of the same individual closer together in representation space.

If you use this task, please cite the original dataset paper and the paper that proposed this evaluation method:

```
@article{algasov2024understanding,
  title={Understanding the Impact of Training Set Size on Animal Re-identification},
  author={Algasov, Aleksandr and Nepovinnykh, Ekaterina and Eerola, Tuomas and K{\"a}lvi{\"a}inen, Heikki and Stewart, Charles V and Otarashvili, Lasha and Holmberg, Jason A},
  journal={arXiv preprint arXiv:2405.15976},
  year={2024}
}

@inproceedings{vcermak2024wildlifedatasets,
  title={WildlifeDatasets: An open-source toolkit for animal re-identification},
  author={{\v{C}}erm{\'a}k, Vojt{\v{e}}ch and Picek, Lukas and Adam, Luk{\'a}{\v{s}} and Papafitsoros, Kostas},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5953--5963},
  year={2024}
}
```
"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import polars as pl
import sklearn.neighbors
import torch
import torchvision.datasets
from jaxtyping import Float, Shaped, jaxtyped
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("beluga")


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """Run the BelugaID benchmark."""
    backbone = registry.load_vision_backbone(cfg.model)

    # Embed all images.
    features = get_features(cfg, backbone)
    # Convert string names into integer labels.
    encoder = sklearn.preprocessing.OrdinalEncoder(dtype=int)
    y = encoder.fit_transform(features.labels.reshape(-1, 1)).reshape(-1)

    clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights="uniform")
    clf.fit(features.x, y)
    y_hat = clf.predict(None)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(features.ids, y_hat, y)
    ]

    return reporting.Report("beluga", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["beluga"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    """A block of features."""

    x: Float[Tensor, "n dim"]
    """Input features; from a `biobench.registry.VisionBackbone`."""
    labels: Shaped[np.ndarray, " n"]
    """Individual name."""
    ids: Shaped[np.ndarray, " n"]
    """Array of image ids."""

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)

    @property
    def n(self) -> int:
        return len(self.ids)


@beartype.beartype
def collate_fn(batch):
    imgs = torch.stack([img for img, _ in batch])
    metadata = [meta for _, meta in batch]
    return imgs, metadata


@beartype.beartype
@torch.no_grad()
def get_features(cfg: config.Experiment, backbone: registry.VisionBackbone) -> Features:
    """
    Get a block of features from a vision backbone.

    Args:
        args: BelugaID arguments.
        backbone: visual backbone.
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    if not os.path.isdir(cfg.data.beluga):
        msg = f"Path '{cfg.data.beluga}' doesn't exist. Did you download the Beluga dataset?"
        raise ValueError(msg)

    dataset = torchvision.datasets.CocoDetection(
        os.path.join(cfg.data.beluga, "beluga.coco", "images", "train2022"),
        os.path.join(
            cfg.data.beluga, "beluga.coco", "annotations", "instances_train2022.json"
        ),
        img_transform,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        collate_fn=collate_fn,
    )

    all_features, all_labels, all_ids = [], [], []

    def probe(batch):
        imgs, _ = batch
        imgs = imgs.to(cfg.device, non_blocking=True)

        with torch.amp.autocast("cuda"):
            backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc="beluga"):
            imgs, metadata = next(it)
            imgs = imgs.to(cfg.device, non_blocking=True)

            with torch.amp.autocast("cuda"):
                features = backbone.img_encode(imgs).img_features

            assert all(len(meta) == 1 for meta in metadata)
            labels = [meta[0]["name"] for meta in metadata]
            ids = [str(meta[0]["image_id"]) for meta in metadata]

            all_features.append(features.cpu())
            all_labels.extend(labels)
            all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = np.array(all_labels)

    return Features(all_features, all_labels, all_ids)

>>>> beluga/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Downloads the Begula whale dataset from lila.science.
"""

import dataclasses
import os
import tarfile

import requests
import tqdm
import tyro

URL = "http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/wild-me/beluga.coco.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configuration."""

    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images."""
    expand: bool = True
    """whether to expand tarfiles into a folder."""


def main(args: Args):
    """Download and unzip the data."""
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_tar_path = os.path.join(args.dir, "beluga.coco.tar.gz")

    if args.download:
        # Download images.
        r = requests.get(URL, stream=True)
        r.raise_for_status()
        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_tar_path}.")

    if args.expand:
        with tarfile.open(images_tar_path, "r") as tar:
            for member in tqdm.tqdm(
                tar, desc="Extracting images", total=len(tar.getnames())
            ):
                tar.extract(member, path=args.dir, filter="data")

        print(f"Extracted images: {args.dir}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> benchmark.py
"""
Entrypoint for running all tasks in `biobench`.

Most of this script is self documenting.
Run `python benchmark.py --help` to see all the options.

Note that you will have to download all the datasets, but each dataset includes its own download script with instructions.
For example, see `biobench.newt.download` for an example.
"""

import collections
import importlib
import logging
import os

import beartype
import submitit
import tyro

from biobench import config, helpers, jobkit, reporting


@beartype.beartype
def main(cfgs: list[str], dry_run: bool = True, n_parallel: int = 1):
    """
    Launch all jobs, using either a local GPU or a Slurm cluster. Then report results and save to disk.

    Args:
        cfgs: List of paths to TOML config files.
        dry_run: If --no-dry-run, actually run experiment.
        n_parallel: Number of jobs that can be claimed by any one launcher process.
    """

    # Load all configs from the provided paths.
    cfgs = [cfg for path in cfgs for cfg in config.load(path)]

    if not cfgs:
        print("No configurations loaded.")
        return

    # ------------------------------------------------------
    # Verify all configs have consistent execution settings.
    # ------------------------------------------------------
    first = cfgs[0]
    for cfg in cfgs[1:]:
        if cfg.slurm_acct != first.slurm_acct:
            raise ValueError("All configs must have the same slurm_acct")
        if cfg.log_to != first.log_to:
            raise ValueError("All configs must have the same log_to directory")
        if cfg.ssl != first.ssl:
            raise ValueError("All configs must have the same ssl setting")

    # --------------
    # Setup logging.
    # --------------
    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    level = logging.DEBUG if first.debug else logging.INFO
    logging.basicConfig(level=level, format=log_format)
    logger = logging.getLogger("benchmark.py")
    logging.getLogger("PIL.TiffImagePlugin").setLevel(logging.INFO)

    # ---------------
    # Setup executor.
    # ---------------
    if first.slurm_acct:
        executor = submitit.SlurmExecutor(folder=first.log_to)
        executor.update_parameters(
            time=30,
            gpus_per_node=1,
            cpus_per_task=8,
            stderr_to_stdout=True,
            partition="debug",
            account=first.slurm_acct,
        )
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not first.ssl:
            executor.update_parameters(setup=["export BIOBENCH_DISABLE_SSL=1"])
    elif first.debug:
        executor = submitit.DebugExecutor(folder=first.log_to)
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not first.ssl:
            os.environ["BIOBENCH_DISABLE_SSL"] = "1"
    else:
        executor = jobkit.SerialExecutor(folder=first.log_to)
        # See biobench.third_party_models.get_ssl() for a discussion of this variable.
        if not first.ssl:
            os.environ["BIOBENCH_DISABLE_SSL"] = "1"

    db = reporting.get_db(first)

    # Clear old (5 days+) runs.
    cleared = reporting.clear_stale_claims(db, max_age_hours=24 * 5)
    logger.info("Cleared %d stale jobs from 'runs' table.", cleared)

    job_stats = collections.defaultdict(int)
    model_stats = collections.defaultdict(int)
    fq = jobkit.FutureQueue(max_size=n_parallel)
    exit_hook = jobkit.ExitHook(
        lambda args: reporting.release_run(db, *args)
    ).register()

    def flush_one():
        """
        Get the next finished job from queue, blocking if necessary, write the report and relinquish the claim.
        """
        job, cfg, task = fq.pop()

        try:
            report: reporting.Report = job.result()
            report.write()
            logger.info("%s+%s/%s done", task, cfg.model.org, cfg.model.ckpt)
        except Exception:
            logger.exception("%s+%s/%s failed", task, cfg.model.org, cfg.model.ckpt)
        finally:
            exit_hook.discard((cfg, task))

    for cfg in cfgs:
        for task, data_root in cfg.data.to_dict().items():
            reason = get_skip_reason(db, cfg, task, data_root, dry_run)
            if reason:
                job_stats[reason] += 1
                continue

            if dry_run:
                job_stats["todo"] += 1
                model_stats[cfg.model.ckpt] += 1
                continue  # no side-effect

            if not reporting.claim_run(db, cfg, task):
                job_stats["queued"] += 1  # someone else just grabbed it
                continue

            exit_hook.add((cfg, task))  # for signal/atexit handler
            job = executor.submit(worker, task, cfg)
            fq.submit((job, cfg, task))
            job_stats["submitted"] += 1

            while fq.full():
                flush_one()

    if dry_run:
        logger.info("Job Summary:")
        logger.info("%-20s | %-5s", "Reason", "Count")
        logger.info("-" * 31)
        for reason, count in sorted(job_stats.items()):
            logger.info("%-20s | %5d", reason, count)
        logger.info("-" * 31)

        logger.info("Model Summary:")
        logger.info("%-50s | %-5s", "Model", "Count")
        logger.info("-" * 61)
        for model, count in sorted(model_stats.items()):
            logger.info("%-50s | %5d", model, count)
        logger.info("-" * 61)
        return

    while fq:
        flush_one()

    logger.info("Finished.")


@beartype.beartype
def worker(task_name: str, cfg: config.Experiment) -> reporting.Report:
    helpers.bump_nofile(512)

    module = importlib.import_module(f"biobench.{task_name}")
    return module.benchmark(cfg)


@beartype.beartype
def get_skip_reason(
    db, cfg: config.Experiment, task: str, data_root: str, dry_run: bool
) -> str | None:
    """Return a short reason string if we should skip (None -> keep)."""
    try:
        importlib.import_module(f"biobench.{task}")
    except ModuleNotFoundError:
        return "no code"

    if not data_root:
        return "no data"

    if reporting.already_ran(db, cfg, task):
        return "done"

    if reporting.is_claimed(db, cfg, task):
        return "queued"

    return None


if __name__ == "__main__":
    tyro.cli(main)

>>>> confidence-intervals.md
# Confidence Intervals

Recommended Reading:

* [Sebastian Raschka's blogpost on confidence intervals for ML](https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html)

## BioBench Confidence Intervals

We resample from the test predictions $N$ times.
Then we re-calculate the statistic of interest (typically just mean accuracy) for each resampling.
Over $N$ re-samples, we collect a distribution of possible mean accuracies.
Then, if we repeated this process 100 times, we could assume that in 95 of the cases, the true mean accuracy is in the sampled confidence interval.

> For me personally, this is pretty convoluted logic.
> I just assume that my true mean is 95% likely to be in my confidence interval.[^wrong]

In practice, we use $N = 500$.

[^wrong]: I know this is wrong. I know. You don't need to tell me. I know it's wrong. But it's way freakin' easier to reason about and it's only subtley wrong and it's still better than just a mean. I know it's wrong. I know.

## Note on Confidence Interval Types

For different tasks, we *could* use different kinds of bootstrapping.
Non-parametric methods like nearest centroid classifiers or KNN are particularly easy to re-evaluate and so we could bootstrap the entire process.
Parametric methods like linear classifiers or SVMs are slow to train, so we don't want to repeat the training step 200+ times.

In practice, however, we simply bootstrap test set predictions because it can be cheaply applied to all benchmarks regardless of training or inference cost.
It requires only that we have a $N$-dimensional vector of scores, where $N$ is the number of test examples, which is always under 10M, so at most a 40MB vector.

>>>> config.py
import dataclasses
import os
import tomllib
import typing

import beartype


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Model:
    """Configuration for a model to be evaluated.

    This class defines the essential parameters needed to identify and load a specific model for evaluation in the benchmark.

    Attributes:
        org: Organization or source of the model (e.g., "open-clip").
        ckpt: Checkpoint or specific model identifier (e.g., "ViT-B-16/openai").
    """

    org: str
    ckpt: str
    _: dataclasses.KW_ONLY
    drop_keys: tuple[str, ...] = dataclasses.field(default_factory=tuple)

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)

    @classmethod
    def from_dict(cls, dct: dict[str, object]) -> "Model":
        drop_keys = tuple(dct.pop("drop_keys", []))
        return cls(**dct, drop_keys=drop_keys)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Data:
    beluga: str = ""
    """Data pathfor the Beluga whale re-ID benchmark."""
    fishnet: str = ""
    """Data path for the FishNet benchmark."""
    fungiclef: str = ""
    """Data path for the FungiCLEF benchmark."""
    imagenet1k: str = ""
    """Data path for the ImageNet-1K benchmark. You can put anything (like 'huggingface') because it is downloaded from HF."""
    newt: str = ""
    """Data path for the NeWT benchmark."""
    herbarium19: str = ""
    """Data path for the Herbarium19 benchmark."""
    inat21: str = ""
    """Data path for the iNat2021 benchmark."""
    kabr: str = ""
    """Data path for the KABR benchmark."""
    mammalnet: str = ""
    """Data path for the MammalNet benchmark."""
    plantnet: str = ""
    """Data path for the Pl@ntNet benchmark."""
    plankton: str = ""
    """Data path for the planktok classification benchmark."""
    iwildcam: str = ""
    """Data path for the iWildCam benchmark."""

    def to_dict(self) -> dict[str, str]:
        return dataclasses.asdict(self)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Experiment:
    """Configuration to run one or more benchmarks in a parallel setting."""

    model: Model

    slurm_acct: str = ""
    """Slurm account. A non-empty string means using Slurm."""
    cfg: str = os.path.join("configs", "neurips.toml")
    """Path to TOML config file."""
    device: typing.Literal["cpu", "mps", "cuda"] = "cuda"
    """which kind of accelerator to use."""
    debug: bool = False
    """whether to run in debug mode."""
    n_train: int = -1
    """Number of maximum training samples. Negative number means use all of them."""
    ssl: bool = True
    """Use SSL when connecting to remote servers to download checkpoints; use --no-ssl if your machine has certificate issues. See `biobench.third_party_models.get_ssl()` for a discussion of how this works."""

    n_workers: int = 4
    """Number of dataloader workers."""
    batch_size: int = 8
    """Initial batch size to start with for tuning."""

    data: Data = dataclasses.field(default_factory=Data)

    report_to: str = os.path.join(".", "results")
    """where to save reports to."""
    log_to: str = os.path.join(".", "logs")
    """where to save logs to."""
    seed: int = 17
    """Random seed."""

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)

    def update(self, other):
        return dataclasses.replace(
            other,
            device=self.device,
            debug=self.debug,
            n_train=self.n_train,
            parallel=self.parallel,
        )


def load(path: str) -> list[Experiment]:
    """Load experiments from a TOML file.

    None of the fields in Experiment are lists, so anytime we find a list in the TOML, we add another dimension to our grid search over all possible experiments.
    """
    with open(path, "rb") as f:
        raw = tomllib.load(f)

    if not isinstance(raw, dict):
        raise ValueError(
            f"TOML file {path} must contain a dictionary at the root level"
        )

    # Extract models list
    models = raw.pop("models", [])
    if not isinstance(models, list):
        raise ValueError("models must be a list of tables in TOML")

    # Start with models as base experiments
    experiments = [{"model": Model.from_dict(model)} for model in models]

    # Handle data config specially
    data = raw.pop("data", {})

    # For each remaining field in the TOML
    for key, value in raw.items():
        new_experiments = []

        # Convert single values to lists
        if not isinstance(value, list):
            value = [value]

        # For each existing partial experiment
        for exp in experiments:
            # Add every value for this field
            for v in value:
                new_exp = exp.copy()
                new_exp[key] = v
                new_experiments.append(new_exp)

        experiments = new_experiments

    # Now add the NeWT config to all experiments
    for exp in experiments:
        exp["data"] = Data(**data)

    # Convert dictionaries to Experiment objects
    return [Experiment(**exp) for exp in experiments]

>>>> fishcounting/__init__.py
import beartype
import numpy as np
import polars as pl
from jaxtyping import Float, jaxtyped

from .. import config, reporting


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report: ...


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]: ...

>>>> fishnet/__init__.py
"""
# FishNet: Fish Recognition, Detection, and Functional Traits Prediction

FishNet ([paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_FishNet_A_Large-scale_Dataset_and_Benchmark_for_Fish_Recognition_Detection_ICCV_2023_paper.pdf), [code](https://github.com/faixan-khan/FishNet)) is a large-scale diverse dataset containing 94,532 images from 17,357 aquatic species.
It contains three benchmarks: fish classification, fish detection, and functional traits prediction.

We mainly focus on the third task.
We train an two-layer MLP on the visual features extracted by different model backbones to predict the presence or absence of 9 different traits.

If you use this evaluation, be sure to cite the original work:

```
@inproceedings{fishnet,
    author    = {Khan, Faizan Farooq and Li, Xiang and Temple, Andrew J. and Elhoseiny, Mohamed},
    title     = {FishNet: A Large-scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {20496-20506}
}
```

This task was contributed by [Jianyang Gu](https://vimar-gu.github.io/).
"""

import logging
import os.path

import beartype
import numpy as np
import polars as pl
import sklearn.metrics
import torch
from jaxtyping import Float, Int, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("fishnet")

batch_size = 1024
learning_rate = 3e-4
n_steps = 30_000
threshold = 0.5


@jaxtyped(typechecker=beartype.beartype)
class Features(torch.utils.data.Dataset):
    """
    A dataset of learned features (dense vectors).
    """

    x: Float[Tensor, " n dim"]
    """Dense feature vectors from a vision backbone."""
    y: Int[Tensor, " n 9"]
    """0/1 labels of absence/presence of 9 different traits."""
    ids: list[str]
    """Image ids."""

    def __init__(
        self,
        x: Float[Tensor, " n dim"],
        y: Int[Tensor, " n n_classes"],
        ids: list[str],
    ):
        self.x = x
        self.y = y
        self.ids = ids

    @property
    def dim(self) -> int:
        """Dimension of the dense feature vectors."""
        _, dim = self.x.shape
        return dim

    def __len__(self) -> int:
        return len(self.x)

    def __getitem__(
        self, index
    ) -> tuple[Float[Tensor, " dim"], Int[Tensor, " n_classes"], str]:
        return self.x[index], self.y[index], self.ids[index]


@beartype.beartype
def init_clf(input_dim: int) -> torch.nn.Module:
    """A simple MLP classifier consistent with the design in FishNet."""
    return torch.nn.Sequential(
        torch.nn.Linear(input_dim, 512),
        torch.nn.Dropout(0.5),
        torch.nn.Linear(512, 9),
    )


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    """
    Calculate the macro-averaged F1 score across all fish trait predictions.

    For each fish image, we predict 9 binary traits:

    1. Feeding Path (benthic/pelagic)
    2. Tropical habitat (yes/no)
    3. Temperate habitat (yes/no)
    4. Subtropical habitat (yes/no)
    5. Boreal habitat (yes/no)
    6. Polar habitat (yes/no)
    7. Freshwater habitat (yes/no)
    8. Saltwater habitat (yes/no)
    9. Brackish water habitat (yes/no)

    The macro-averaging:

    1. Calculates an F1 score for each trait independently
    2. Takes the unweighted mean of these 9 F1 scores

    This ensures each trait contributes equally to the final score, regardless of class imbalance in the dataset (e.g., if there are many more tropical fish than brackish water fish).

    Args:
        preds: List of predictions, each containing:
            - info["y_pred"]: List of 9 binary predictions
            - info["y_true"]: List of 9 binary ground truth values

    Returns:
        The macro-averaged F1 score across all 9 traits
    """
    y_pred = np.array([pred.info["y_pred"] for pred in preds])
    y_true = np.array([pred.info["y_true"] for pred in preds])
    return sklearn.metrics.f1_score(
        y_true, y_pred, average="macro", labels=np.unique(y_true)
    )


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["fishnet"]

    n, *rest = df.group_by("model_ckpt").agg(n=pl.len()).get_column("n").to_list()
    assert all(n == i for i in rest)

    if b > 0:
        assert rng is not None, "must provide rng argument"
        i_bs = rng.integers(0, n, size=(b, 9, n), dtype=np.int32)

    scores = {}

    y_pred_buf = np.empty((b, 9, n), dtype=np.int32)
    y_true_buf = np.empty((b, 9, n), dtype=np.int32)

    for model_ckpt in helpers.progress(
        df.get_column("model_ckpt").unique().sort().to_list(),
        desc="fishnet/bootstrap",
        every=3,
    ):
        # pull y_true and y_pred for *one* model
        y_pred = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_pred")
            .unique()
            .sort("img_id")
            .get_column("y_pred")
            .str.json_decode()
            .to_numpy()
        )
        y_pred = np.stack(y_pred).astype(np.int32).T

        if len(y_pred) == 0:
            continue

        y_true = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_true")
            .unique()
            .sort("img_id")
            .get_column("y_true")
            .str.json_decode()
            .to_numpy()
        )
        y_true = np.stack(y_true).astype(np.int32).T

        assert y_true.size == y_pred.size

        if b > 0:
            # bootstrap resample into pre-allocated buffers
            np.take(y_pred, i_bs, axis=None, out=y_pred_buf)
            np.take(y_true, i_bs, axis=None, out=y_true_buf)
            score = reporting.macro_f1_batch(y_true_buf, y_pred_buf).mean(axis=-1)
            scores[model_ckpt] = score
        else:
            f1s = reporting.macro_f1_batch(y_true, y_pred)
            scores[model_ckpt] = f1s.mean(keepdims=True)

    return scores


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    The FishNet benchmark.
    """
    # 1. Load model.
    backbone = registry.load_vision_backbone(cfg.model)

    # 2. Get features.
    train_dataset = get_features(cfg, backbone, is_train=True)
    test_dataset = get_features(cfg, backbone, is_train=False)

    # 3. Set up classifier.
    classifier = init_clf(train_dataset.dim).to(cfg.device)

    # 4. Load datasets for classifier.
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False
    )
    optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)
    criterion = torch.nn.BCEWithLogitsLoss()
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

    # 5. Fit the classifier.
    it = helpers.infinite(train_loader)
    for step in range(n_steps):
        features, labels, _ = next(it)
        features = features.to(cfg.device)
        labels = labels.to(cfg.device, dtype=torch.float)
        output = classifier(features)
        loss = criterion(output, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        # Evaluate the classifier.
        if (step + 1) % 1_000 == 0:
            preds = predict(cfg, classifier, test_loader)
            logger.info(
                "Step %d/%d (%.1f%%): %.3f (macro F1)",
                step + 1,
                n_steps,
                (step + 1) / n_steps * 100,
                score(preds),
            )
    preds = predict(cfg, classifier, test_loader)

    return reporting.Report("fishnet", preds, cfg)


@beartype.beartype
def predict(
    cfg: config.Experiment, classifier: torch.nn.Module, dataloader
) -> list[reporting.Prediction]:
    """
    Evaluates the trained classifier on a test split.

    Returns:
        List of `reporting.Prediction`s
    """
    total = 2 if cfg.debug else len(dataloader)
    it = iter(dataloader)
    preds = []
    for b in range(total):
        features, labels, ids = next(it)
        features = features.to(cfg.device)
        labels = labels.numpy()
        with torch.no_grad():
            pred_logits = classifier(features)
        pred_logits = (pred_logits > threshold).cpu().numpy()
        for id, pred, true in zip(ids, pred_logits, labels):
            info = {"y_pred": pred.tolist(), "y_true": true.tolist()}
            preds.append(reporting.Prediction(id, (pred == true).mean().item(), info))

    return preds


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    """Extract visual features."""
    if not os.path.isdir(cfg.data.fishnet):
        msg = f"Path '{cfg.data.fishnet}' doesn't exist. Did you download the FishNet dataset? See the docstring at the top of this file for instructions."
        raise ValueError(msg)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    file = "train.csv" if is_train else "test.csv"
    dataset = ImageDataset(cfg.data.fishnet, file, transform=img_transform)
    if is_train and cfg.n_train > 0:
        i = np.random.default_rng(seed=cfg.seed).choice(
            len(dataset), cfg.n_train, replace=False, shuffle=False
        )
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        shuffle=False,
    )

    @beartype.beartype
    def debug_cuda_mem(tag: str):
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("%s: %d", tag, torch.cuda.memory_allocated())

    def probe(batch):
        imgs, labels, ids = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        _ = backbone.img_encode(imgs).img_features  # forward only

    all_features, all_labels, all_ids = [], [], []

    with helpers.auto_batch_size(dataloader, probe=probe, upper=2048):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"fishnet/{file}"):
            debug_cuda_mem("loop start")
            images, labels, ids = next(it)
            debug_cuda_mem("after batch")
            images = images.to(cfg.device)
            debug_cuda_mem("imgs.to(device)")

            features = backbone.img_encode(images).img_features

            all_features.append(features.cpu())
            all_labels.append(labels)

            all_ids.extend(ids)

    # Keep the Tensor data type for subsequent training
    all_features = torch.cat(all_features, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    assert len(all_ids) == len(dataset)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
class ImageDataset(torch.utils.data.Dataset):
    """
    A dataset for CV+ML that loads the required attribute labels.
    """

    def __init__(self, root_dir: str, csv_file: str, transform):
        self.root_dir = root_dir
        self.csv_file = os.path.join(self.root_dir, csv_file)
        self.df = pl.read_csv(self.csv_file).with_row_index()
        self.all_columns = [
            "FeedingPath",
            "Tropical",
            "Temperate",
            "Subtropical",
            "Boreal",
            "Polar",
            "freshwater",
            "saltwater",
            "brackish",
        ]
        for col in self.all_columns:
            self.df = self.df.filter(self.df[col].is_not_null())
        self.transform = transform

        # Corresponding column indices
        self.image_col = 4
        self.folder_col = 13
        self.label_cols = [15, 16, 17, 18, 19, 20, 21, 22, 23]
        logger.info("csv file: %s has %d item.", csv_file, len(self.df))

    def __getitem__(
        self, index
    ) -> tuple[Float[Tensor, "3 width height"], Int[Tensor, "9"], str]:
        row_data = self.df.row(index)
        image_name = row_data[self.image_col]
        image_name = image_name.split("/")[-1]
        folder = row_data[self.folder_col]
        image_path = os.path.join(self.root_dir, "Image_Library", folder, image_name)
        image = Image.open(image_path)

        # Extract the required attribute labels.
        label = []
        for col in self.label_cols:
            value = row_data[col]
            if col == 15:
                if value == "pelagic":
                    value = 1
                elif value == "benthic":
                    value = 0
                else:
                    raise ValueError("FeedingPath can only be pelagic or benthic.")
            label.append(value)
        label = torch.tensor(label)

        if self.transform:
            image = self.transform(image)

        return image, label, image_path

    def __len__(self) -> int:
        return len(self.df)

    @property
    def labels(self) -> Int[np.ndarray, "n 9"]:
        return (
            self.df.select(pl.nth(self.label_cols))
            .with_columns(
                FeedingPath=pl.when(pl.col("FeedingPath") == "benthic")
                .then(0)
                .otherwise(1)
            )
            .to_numpy()
        )

>>>> fishnet/baseline.py
"""
Uniform-random baseline for FishNet functional-trait prediction.

Each bootstrap trial:

1. Loads the FishNet test split specified in the experiment config.
2. Generates a 9-bit prediction vector for every image by i.i.d. Bernoulli(0.5).
3. Scores macro-F1 via the existing `fishnet.score` helper.

After *n* trials it prints the mean and standard deviation of the macro-F1 (expressed as a percentage). No training statistics are used; this is a pure chance-level reference.

Run from the command line:

uv run python -m biobench.fishnet.random_baseline --cfg configs/neurips.toml
"""

import logging

import beartype
import numpy as np
import tyro

from .. import config, helpers, reporting
from . import ImageDataset, score


@beartype.beartype
def main(cfg: str, n: int = 1_000, seed: int = 42):
    """Estimate the macro-F1 of uniform random guessing on FishNet.

    Args:
        cfg: Path or key understood by `config.load`; must point to an experiment that defines `data.fishnet` and the `verbose` flag.
        n: Number of bootstrap trials. Default is 1_000.
        seed: Seed for NumPy's `default_rng`. Default is 42.

    Prints:
        "Mean score: mean (std dev)" where scores are macro-F1 x 100.
    """
    cfg = next(cfg for cfg in config.load(cfg))

    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    level = logging.DEBUG if cfg.debug else logging.INFO
    logging.basicConfig(level=level, format=log_format)
    logger = logging.getLogger(__name__)

    rng = np.random.default_rng(seed)

    test_ds = ImageDataset(cfg.data.fishnet, "test.csv", transform=None)
    logger.info("Loaded test dataset.")

    scores = []
    for _ in helpers.progress(range(n), every=n // 100, desc="bootstrapping"):
        y_pred = (rng.random((len(test_ds), 9)) < 0.5).astype(int)
        preds = [
            reporting.Prediction(
                "deadbeef",
                (p == t).mean().item(),
                {"y_pred": p.tolist(), "y_true": t.tolist()},
            )
            for p, t in zip(y_pred, test_ds.labels)
        ]
        scores.append(score(preds) * 100)

    print(f"Mean score: {np.mean(scores):.3f} ({np.std(scores):.3f})")


if __name__ == "__main__":
    tyro.cli(main)

>>>> fishnet/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "gdown",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the FishNet dataset

Run with:

1. `python biobench/fishnet/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.fishnet.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import zipfile

import gdown
import requests
import tqdm
import tyro

dataset_url = "https://drive.google.com/uc?id=1mqLoap9QIVGYaPJ7T_KSBfLxJOg2yFY3"

labels_urls = [
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/train_full_meta_new.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/train.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/test.csv",
    "https://raw.githubusercontent.com/faixan-khan/FishNet/refs/heads/main/anns/spec_gen_map.csv",
]


@dataclasses.dataclass()
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    images: bool = True
    """Whether to download the images zip file [5.4GB]."""
    labels: bool = True
    """Whether to download the labels."""
    extract: bool = True
    """Whether to extract the zip file."""


def main(args: Args):
    """Download FishNet."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    output_name = "fishnet.zip"
    zipfile_path = os.path.join(args.dir, output_name)

    # Download the zip file.
    if args.images:
        gdown.download(dataset_url, zipfile_path, quiet=False)
        print(f"Downloaded zip file: {zipfile_path}.")

    if args.labels:
        for labels_url in labels_urls:
            r = requests.get(labels_url, stream=True)
            r.raise_for_status()

            labels_path = os.path.join(args.dir, labels_url.split("/")[-1])
            with open(labels_path, "wb") as fd:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    fd.write(chunk)
            print(f"Downloaded labels: {labels_path}.")

    # Extract the zip file.
    if args.extract:
        with zipfile.ZipFile(zipfile_path, "r") as zip:
            for member in tqdm.tqdm(zip.infolist(), desc="Extracting images"):
                zip.extract(member, args.dir)
        print(f"Extracted images: {args.dir}/Image_Library.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> fungiclef/__init__.py
"""
FungiCLEF2023: classify fungal species using Danish Fungi preprocessed images.

Citations:
```
@inproceedings{BohemianVRA2023,
  title={FungiCLEF 2023 challenge evaluation},
  author={BohemianVRA},
  booktitle={ImageCLEF},
  year={2023}
}
```
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image

from .. import config, helpers, openset, registry, reporting, simpleshot
from . import metrics

logger = logging.getLogger("fungiclef")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]
    """image IDs for validation, observation IDs for training."""


@beartype.beartype
class FungiDataset(torch.utils.data.Dataset):
    def __init__(self, root: str, split: typing.Literal["train", "val"], transform):
        img_dpath = os.path.join(root, "DF20_300" if split == "train" else "DF21_300")
        if not os.path.isdir(img_dpath):
            raise RuntimeError(f"Image directory not found: {img_dpath}")

        csv_fpath = os.path.join(root, f"FungiCLEF2023_{split}_metadata_PRODUCTION.csv")
        if not os.path.isfile(csv_fpath):
            raise RuntimeError(f"CSV not found: {csv_fpath}")

        df = pl.read_csv(csv_fpath)
        self.img_names = df.get_column("image_path").to_numpy()
        self.labels = df.get_column("class_id").to_numpy().astype(int)
        self.obs_ids = df.get_column("observationID").to_numpy()

        self.img_dpath = img_dpath
        self.transform = transform

    def __len__(self) -> int:
        return len(self.img_names)

    def __getitem__(self, idx) -> dict[str, object]:
        img_name = self.img_names[idx]
        label = self.labels[idx].item()
        # try exact case, then lowercase
        p1 = os.path.join(self.img_dpath, img_name)
        if os.path.exists(p1):
            path = p1
        else:
            p2 = os.path.join(self.img_dpath, img_name.lower())
            if os.path.exists(p2):
                path = p2
                # logger.debug("Using lowercase image path for %s", img_name)
            else:
                raise FileNotFoundError(
                    f"Image '{img_name}' not found in {self.img_dpath} (tried '{p1}', '{p2}')"
                )
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)

        return {
            "img_id": img_name,
            "img": img,
            "label": label,
            "obs_id": self.obs_ids[idx].item(),
        }


@beartype.beartype
@torch.inference_mode()
def get_features(
    cfg: config.Experiment,
    backbone: registry.VisionBackbone,
    *,
    is_train: bool,
    pool: bool,
) -> Features:
    transform = backbone.make_img_transform()

    split = "train" if is_train else "val"
    dataset = FungiDataset(cfg.data.fungiclef, split, transform)

    # subsample for train
    if is_train and cfg.n_train > 0:
        idxs = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        dataset = torch.utils.data.Subset(dataset, idxs)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.n_workers,
        pin_memory=False,
    )

    backbone = backbone.to(cfg.device)
    feats_list, labs_list, img_ids_list, obs_ids_list = [], [], [], []

    @beartype.beartype
    def debug_cuda_mem(tag: str):
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("%s: %s", tag, torch.cuda.memory_summary())

    def probe(batch):
        imgs = batch["img"].to(cfg.device)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs)

    with helpers.auto_batch_size(dataloader, probe=probe):
        backbone = torch.compile(backbone)
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for _ in helpers.progress(range(total), desc=f"fungi/{split}"):
            debug_cuda_mem("loop start")
            batch = next(it)
            debug_cuda_mem("after batch")
            imgs = batch["img"].to(cfg.device)
            debug_cuda_mem("imgs.to(device)")
            with torch.amp.autocast(cfg.device):
                out = backbone.img_encode(imgs).img_features
            debug_cuda_mem("after forward pass")
            feats_list.append(out.cpu().numpy())
            debug_cuda_mem("appended feats")

            # I was getting some CUDA OOM errors due to PyTorch reserving/allocating and memory fragmentation. Rather than adjust the recommended env variable, I added this line manual GC.
            del out
            torch.cuda.empty_cache()

            debug_cuda_mem("emptied cache")
            labs_list.extend(batch["label"].tolist())
            img_ids_list.extend(batch["img_id"])
            obs_ids_list.extend(batch["obs_id"].tolist())
            debug_cuda_mem("appended metadata")

    x = np.concatenate(feats_list, axis=0)
    y = np.array(labs_list)
    img_ids = np.array(img_ids_list)
    obs_ids = np.array(obs_ids_list)

    if pool:
        # for each unique obs take mean of its image features
        uniq, inv = np.unique(obs_ids, return_inverse=True)
        pooled = np.empty((len(uniq), x.shape[1]), dtype=x.dtype)
        for k, u in enumerate(uniq):
            pooled[k] = x[inv == k].mean(axis=0)
        # labels should be identical within an observation
        pooled_y = np.array([y[inv == k][0] for k in range(len(uniq))], dtype=int)
        return Features(pooled, pooled_y, uniq)

    # image-level output
    return Features(x, y, img_ids)


@beartype.beartype
@torch.inference_mode()
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)
    train_feats = get_features(cfg, backbone, is_train=True, pool=False)
    val_feats = get_features(cfg, backbone, is_train=False, pool=True)

    torch.cuda.empty_cache()  # be nice to others on the machine.

    clf = init_clf(cfg)
    clf.fit(train_feats.x, train_feats.y)

    logger.info("Classifying %d examples.", len(val_feats.x))
    preds = clf.predict(val_feats.x)
    logger.info("Classified %d examples.", len(val_feats.x))

    # Identify train and test classes
    train_classes = set(np.unique(train_feats.y))

    examples = [
        reporting.Prediction(
            str(img_id),
            float(p == t),
            {"y_pred": int(p), "y_true": int(t), "ood": t not in train_classes},
        )
        for img_id, p, t in zip(val_feats.ids, preds, val_feats.y)
    ]
    return reporting.Report("fungiclef", examples, cfg)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    return openset.MahalanobisOpenSetClassifier(
        simpleshot.SimpleShotClassifier(device="cuda:0")
    )


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["fungiclef"]

    n, *rest = df.group_by("model_ckpt").agg(n=pl.len()).get_column("n").to_list()
    assert all(n == i for i in rest)

    if b > 0:
        assert rng is not None, "must provide rng argument"
        i_bs = rng.integers(0, n, size=(b, n), dtype=np.int32)

    scores = {}

    y_pred_buf = np.empty((b, n), dtype=np.int32)
    y_true_buf = np.empty((b, n), dtype=np.int32)

    for model_ckpt in df.get_column("model_ckpt").unique().sort().to_list():
        y_pred = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_pred")
            .unique()
            .sort("img_id")
            .get_column("y_pred")
            .cast(pl.Int32)
            .to_numpy()
        )

        if len(y_pred) == 0:
            continue

        y_true = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_true")
            .unique()
            .sort("img_id")
            .get_column("y_true")
            .cast(pl.Int32)
            .to_numpy()
        )
        assert y_true.size == y_pred.size

        if b > 0:
            # bootstrap resample into pre-allocated buffers
            np.take(y_pred, i_bs, axis=0, out=y_pred_buf)
            np.take(y_true, i_bs, axis=0, out=y_true_buf)
            scores[model_ckpt] = metrics.user_loss_score_normalized(
                y_true_buf, y_pred_buf
            )
        else:
            scores[model_ckpt] = np.array([
                metrics.user_loss_score_normalized(y_true, y_pred)
            ])
    return scores


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    """
    Return the **User-Focused Loss** used in FungiCLEF:
        user_loss = classification_error + PSC/ESC cost

    Notes
    -----
    * `info['y_true']` and `info['y_pred']` are ints; unknown is -1.
    * The helper in metrics.py already combines CE and PSC/ESC.
    """
    y_true = np.fromiter(
        (-1 if p.info["ood"] else p.info["y_true"] for p in preds), dtype=int
    )
    y_pred = np.fromiter((p.info["y_pred"] for p in preds), dtype=int)
    user_loss = metrics.user_loss_score(y_true, y_pred)

    n = y_true.size
    n_unknown = (y_true == -1).sum()
    n_poisonous = np.isin(y_true, metrics.POISONOUS_SPECIES).sum()

    cost_unknown_mis = 10.0
    cost_psc = 100.0
    cost_esc = 1.0

    worst_ce = (cost_unknown_mis - 1) * n_unknown / n + 1
    worst_psc = (cost_psc * n_poisonous + cost_esc * (n - n_poisonous)) / n

    score = 1 - user_loss / (worst_ce + worst_psc)
    return score

>>>> fungiclef/baseline.py
"""
Uniform-random baseline for FungiCLEF.

After *n* trials it prints the mean and standard deviation of the user score (expressed as a percentage). No training statistics are used; this is a pure chance-level reference.

Run from the command line:

uv run python -m biobench.fungiclef.random_baseline --cfg configs/neurips.toml
"""

import logging

import beartype
import numpy as np
import tyro

from .. import config, helpers, reporting
from . import FungiDataset, score


@beartype.beartype
def main(cfg: str, n: int = 1_000, seed: int = 42):
    """Evaluate uniform random guessing on FungiCLEF.

    Args:
        cfg: Path or key understood by `config.load`; must point to an experiment that defines `data.fungiclef` and the `verbose` flag.
        n: Number of bootstrap trials. Default is 1_000.
        seed: Seed for NumPy's `default_rng`. Default is 42.

    Prints:
        "Mean score: mean (std dev)" where scores are fungiclef.score x 100.
    """
    cfg = next(cfg for cfg in config.load(cfg))

    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    level = logging.DEBUG if cfg.debug else logging.INFO
    logging.basicConfig(level=level, format=log_format)
    logger = logging.getLogger(__name__)

    train_ds = FungiDataset(cfg.data.fungiclef, "train", None)
    logger.info("Loaded train dataset.")
    val_ds = FungiDataset(cfg.data.fungiclef, "val", None)
    logger.info("Loaded val dataset.")

    rng = np.random.default_rng(seed=seed)
    label_space = np.concatenate(([-1], sorted(train_ds.labels))).astype(int)

    scores = []
    for _ in helpers.progress(range(n), every=n // 100, desc="bootstrapping"):
        preds = [
            reporting.Prediction(
                "deadbeef",
                float(p == t),
                {"y_pred": int(p), "y_true": int(t), "ood": t == -1},
            )
            for p, t in zip(rng.choice(label_space, size=len(val_ds)), val_ds.labels)
        ]
        scores.append(score(preds) * 100)

    print(f"Mean score: {np.mean(scores):.3f} ({np.std(scores):.3f})")


if __name__ == "__main__":
    tyro.cli(main)

>>>> fungiclef/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the FungiCLEF 2023 challenge dataset based on the Danish Fungi 2020 and 2021 preprocessed images and metadata.

Downloads:
 - Training images (max side size 300px; DF20) [~6.5GB]
 - Validation + Public Test images (max side size 300px; DF21) [~2.5GB]
 - Training, Validation, and Public Test metadata CSVs
"""

import dataclasses
import os
import tarfile

import requests
import tqdm
import tyro

URLS = {
    "train_imgs": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/DF20-300px.tar.gz",
    "val_imgs": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/DF21_300px.tar.gz",
    "train_metadata": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/FungiCLEF2023_train_metadata_PRODUCTION.csv",
    "val_metadata": "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/FungiCLEF2023_val_metadata_PRODUCTION.csv",
}


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save downloaded archives and extract them."""
    chunk_size_kb: int = 1024
    """Download chunk size in KB."""
    download_train_imgs: bool = True
    """Whether to download training images (DF20) [~6.5GB]."""
    download_val_imgs: bool = True
    """Whether to download validation and public test images (DF21) [~2.5GB]."""
    download_train_metadata: bool = True
    """Whether to download training metadata CSV."""
    download_val_metadata: bool = True
    """Whether to download validation metadata CSV."""
    unzip: bool = True
    """Whether to extract downloaded archives."""


def download_file(name: str, url: str, dest_dir: str, chunk_size: int) -> str:
    os.makedirs(dest_dir, exist_ok=True)
    filename = os.path.basename(url)
    dest_path = os.path.join(dest_dir, filename)
    if os.path.exists(dest_path):
        print(f"{filename} already exists, skipping download")
        return dest_path
    response = requests.get(url, stream=True)
    response.raise_for_status()
    total = int(response.headers.get("content-length", 0))
    with (
        open(dest_path, "wb") as f,
        tqdm.tqdm(
            total=total, unit="B", unit_scale=True, desc=f"Downloading {filename}"
        ) as bar,
    ):
        for chunk in response.iter_content(chunk_size=chunk_size):
            f.write(chunk)
            bar.update(len(chunk))
    return dest_path


def extract_file(archive_path: str, dest_dir: str):
    with tarfile.open(archive_path, "r:gz") as tar:
        for member in tqdm.tqdm(
            tar, desc=f"Extracting {os.path.basename(archive_path)}"
        ):
            tar.extract(member, path=dest_dir)


def main(args: Args):
    base_dir = args.dir
    chunk_size = args.chunk_size_kb * 1024
    for key, url in URLS.items():
        flag = getattr(args, f"download_{key}")
        if not flag:
            continue
        path = download_file(key, url, base_dir, chunk_size)
        if args.unzip and key.endswith("_imgs"):
            extract_file(path, base_dir)
            print(f"Extracted {key} into {base_dir}")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> fungiclef/metrics.py
"""
FungiCLEF2023 custom metrics implementation.
Adapted from BohemianVRA's evaluate.py:
https://github.com/BohemianVRA/FGVC-Competitions/blob/main/FungiCLEF2023/evaluate.py
"""

import beartype
import numpy as np
import polars as pl
import sklearn.metrics
from jaxtyping import Float, Int, jaxtyped

# Load poison status via Polars
_poison_df = pl.read_csv(
    "http://ptak.felk.cvut.cz/plants/DanishFungiDataset/poison_status_list.csv"
)
POISONOUS_SPECIES = (
    _poison_df.filter(pl.col("poisonous") == 1)
    .select("class_id")
    .unique()
    .to_series()
    .to_numpy()
)


@jaxtyped(typechecker=beartype.beartype)
def classification_error_with_unknown(
    y_true: Int[np.ndarray, "*batch n"],
    y_pred: Int[np.ndarray, "*batch n"],
    *,
    cost_unknown_mis: float = 10.0,
    cost_mis_as_unknown: float = 0.1,
) -> Float[np.ndarray, "*batch"]:
    """
    Classification error allowing for an "unknown" class (encoded as -1).

    Args:
        y_true: ground-truth labels (with -1 for unknown)
        y_pred: predicted labels (with -1 for unknown)
        cost_unknown_mis: cost of misclassifying a true unknown as known
        cost_mis_as_unknown: cost of misclassifying a true known as unknown

    Returns:
        normalized error rate
    """
    *b, n = y_true.shape
    is_true_unknown = y_true == -1
    is_pred_unknown = y_pred == -1

    n_mis_unknown = np.sum(is_true_unknown & ~is_pred_unknown, axis=-1)
    n_mis_as_unknown = np.sum(~is_true_unknown & is_pred_unknown, axis=-1)
    n_other = np.sum((y_true != y_pred) & ~is_true_unknown & ~is_pred_unknown, axis=-1)

    err = (
        n_other
        + cost_unknown_mis * n_mis_unknown
        + cost_mis_as_unknown * n_mis_as_unknown
    ) / n

    return np.asarray(err)


@jaxtyped(typechecker=beartype.beartype)
def classification_error(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> Float[np.ndarray, "*batch"]:
    """
    Standard classification error (unknown treated as error equally).
    """
    return classification_error_with_unknown(
        y_true, y_pred, cost_unknown_mis=1.0, cost_mis_as_unknown=1.0
    )


@beartype.beartype
def num_psc_decisions(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> Int[np.ndarray, "*batch"]:
    """
    Number of poisonous species incorrectly predicted as non-poisonous.
    """
    is_true_poison = np.isin(y_true, POISONOUS_SPECIES)
    is_pred_poison = np.isin(y_pred, POISONOUS_SPECIES)
    return np.asarray(np.sum(is_true_poison & ~is_pred_poison, axis=-1))


@beartype.beartype
def num_esc_decisions(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> Int[np.ndarray, "*batch"]:
    """
    Number of non-poisonous species incorrectly predicted as poisonous.
    """
    is_true_poison = np.isin(y_true, POISONOUS_SPECIES)
    is_pred_poison = np.isin(y_pred, POISONOUS_SPECIES)
    return np.asarray(np.sum(~is_true_poison & is_pred_poison, axis=-1))


@jaxtyped(typechecker=beartype.beartype)
def psc_esc_cost_score(
    y_true: Int[np.ndarray, "*batch n"],
    y_pred: Int[np.ndarray, "*batch n"],
    *,
    cost_psc: float = 100.0,
    cost_esc: float = 1.0,
) -> Float[np.ndarray, "*batch"]:
    """
    Weighted cost for poisonousness confusion per sample.
    """
    *batch, n = y_true.shape
    psc = num_psc_decisions(y_true, y_pred)
    esc = num_esc_decisions(y_true, y_pred)
    cost = (cost_psc * psc + cost_esc * esc) / n
    return np.asarray(cost)


@jaxtyped(typechecker=beartype.beartype)
def user_loss_score(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> Float[np.ndarray, "*batch"]:
    ce_unk = classification_error_with_unknown(
        y_true, y_pred, cost_unknown_mis=10.0, cost_mis_as_unknown=0.1
    )
    psc = psc_esc_cost_score(y_true, y_pred, cost_psc=100.0, cost_esc=1.0)
    return np.asarray(ce_unk + psc)


@jaxtyped(typechecker=beartype.beartype)
def user_loss_score_normalized(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> Float[np.ndarray, "*batch"]:
    cost_unknown_mis = 10.0
    cost_mis_as_unknown = 0.1
    ce_unk = classification_error_with_unknown(
        y_true,
        y_pred,
        cost_unknown_mis=cost_unknown_mis,
        cost_mis_as_unknown=cost_mis_as_unknown,
    )
    cost_psc = 100.0
    cost_esc = 1.0
    psc = psc_esc_cost_score(y_true, y_pred, cost_psc=cost_psc, cost_esc=cost_esc)

    n = y_true.size
    n_unknown = (y_true == -1).sum()
    n_poisonous = np.isin(y_true, POISONOUS_SPECIES).sum()

    # TODO: is this 1 supposed to be 0.1?
    worst_ce = (cost_unknown_mis - 1) * n_unknown / n + 1
    worst_psc = (cost_psc * n_poisonous + cost_esc * (n - n_poisonous)) / n

    score = 1 - (ce_unk + psc) / (worst_ce + worst_psc)
    return np.asarray(score)


@jaxtyped(typechecker=beartype.beartype)
def evaluate_metrics(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> dict[str, Float[np.ndarray, "*batch"]]:
    """
    Compute all four FungiCLEF custom metrics plus macro F1.

    Returns:
        dict with keys:
            - F1_macro
            - Classification_Error
            - PSC_ESC_Cost
            - User_Focused_Loss
            - Classification_Error_with_Unknown
    """
    f1 = np.asarray(sklearn.metrics.f1_score(y_true, y_pred, average="macro") * 100.0)
    ce = classification_error(y_true, y_pred)
    psc = psc_esc_cost_score(y_true, y_pred)
    ce_unk = classification_error_with_unknown(y_true, y_pred)
    user_loss = np.asarray(ce_unk + psc)
    return {
        "f1_macro": f1,
        "ce": ce,
        "psc_esc_cost": psc,
        "user_loss": user_loss,
        "ce_unk": ce_unk,
    }

>>>> fungiclef/test_metrics.py
import numpy as np
import pytest
from hypothesis import assume, given
from hypothesis import strategies as st

from . import metrics


# Override poisonous species for deterministic testing
@pytest.fixture(autouse=True)
def stub_poison(monkeypatch):
    monkeypatch.setattr(metrics, "POISONOUS_SPECIES", np.array([1, 3]), raising=True)


def test_classification_error_all_correct():
    y = np.array([0, 1, 2, 3])
    y_pred = y.copy()
    assert metrics.classification_error(y, y_pred).item() == 0.0
    # with unknown costs both = 1 => same
    assert metrics.classification_error_with_unknown(y, y_pred).item() == 0.0


def test_classification_error_all_wrong():
    y = np.array([0, 1, 2, 3])
    y_pred = np.array([4, 5, 6, 7])
    # all mismatches, cost_unknown_mis=1, cost_mis_as_unknown=1
    assert metrics.classification_error(y, y_pred) == 1.0
    # no unknowns => same
    assert metrics.classification_error_with_unknown(y, y_pred) == 1.0


def test_classification_error_with_unknown_edge():
    y_true = np.array([-1, 10])
    y_pred = np.array([5, -1])
    # one true unknown mispredicted (cost 10), one true known predicted unknown (cost 0.1)
    ce_unk = metrics.classification_error_with_unknown(
        y_true, y_pred, cost_unknown_mis=10.0, cost_mis_as_unknown=0.1
    )
    # (10 + 0.1) / 2 = 5.05
    assert pytest.approx(ce_unk, abs=1e-6) == 5.05
    # classification_error uses cost=1,1 => (1 + 1)/2 = 1
    assert metrics.classification_error(y_true, y_pred) == 1.0


def test_num_psc_decisions_and_num_esc_decisions():
    # with stubbed POISONOUS_SPECIES = [1,3]
    y_true = np.array([1, 2, 3, 4])
    y_pred = np.array([2, 3, 4, 1])
    # true poison at positions 0 and 2; pred poison when pred in [1,3]
    # pos 0: true poison, pred=2 not poison => PSC
    # pos 2: true poison, pred=4 not poison => PSC
    assert metrics.num_psc_decisions(y_true, y_pred) == 2
    # ESC: non-poison predicted as poison
    # pos1: true=2 not poison, pred=3 poison => ESC
    # pos3: true=4 not poison, pred=1 poison => ESC
    assert metrics.num_esc_decisions(y_true, y_pred) == 2


def test_psc_esc_cost_score_default_costs():
    y_true = np.array([1, 2, 3, 4])
    y_pred = np.array([2, 3, 4, 1])
    # PSC=2, ESC=2, cost_psc=100, cost_esc=1 => (200 + 2)/4 = 50.5
    cost = metrics.psc_esc_cost_score(y_true, y_pred)
    assert pytest.approx(cost, abs=1e-6) == 50.5


def test_evaluate_metrics_basic():
    # simple balanced case
    y_true = np.array([0, 1, 2, 3])
    y_pred = y_true.copy()
    res = metrics.evaluate_metrics(y_true, y_pred)
    assert res["f1_macro"] == 100.0
    assert res["ce"] == 0.0
    assert res["psc_esc_cost"] == 0.0
    assert res["user_loss"] == 0.0
    assert res["ce_unk"] == 0.0


def test_all_correct():
    y = np.arange(5)
    y_pred = y.copy()
    assert metrics.classification_error(y, y_pred) == 0.0


def test_all_wrong():
    y = np.arange(5)
    y_pred = np.arange(5) + 10
    assert metrics.classification_error(y, y_pred) == 1.0


@given(
    y_true=st.lists(st.integers(min_value=-1, max_value=10), min_size=1, max_size=20),
    y_pred=st.lists(st.integers(min_value=-1, max_value=10), min_size=1, max_size=20),
)
def test_fuzz_raises_or_returns(y_true, y_pred):
    assume(len(y_true) == len(y_pred))
    y_t = np.array(y_true)
    y_p = np.array(y_pred)
    out = metrics.evaluate_metrics(y_t, y_p)
    # Expect dict keys and numeric values
    assert isinstance(out, dict)
    for k, v in out.items():
        assert isinstance(k, str)
        assert isinstance(v, np.ndarray)


def _gen_labels():
    """[-1 = unknown] U [0..50], 1 - 100 samples"""
    return st.lists(st.integers(min_value=-1, max_value=50), min_size=1, max_size=100)


@given(y_true=_gen_labels(), y_pred=_gen_labels())
def test_user_loss_score_normalized_in_0_1(y_true, y_pred):
    assume(len(y_true) == len(y_pred))
    y_t, y_p = map(np.asarray, (y_true, y_pred))
    val = metrics.user_loss_score_normalized(y_t, y_p).item()
    assert 0.0 <= val <= 1.0


@given(y_true=_gen_labels(), y_pred=_gen_labels())
def test_user_loss_score_batched(y_true, y_pred):
    assume(len(y_true) == len(y_pred))
    y_t, y_p = map(np.asarray, (y_true, y_pred))
    bsz = 4
    y_t_b = np.stack([y_t] * bsz)
    y_p_b = np.stack([y_p] * bsz)
    vals_b = metrics.user_loss_score_normalized(y_t_b, y_p_b)
    vals_ref = np.stack([metrics.user_loss_score_normalized(y_t, y_p)] * bsz)
    np.testing.assert_allclose(vals_b, vals_ref)

>>>> helpers.py
"""
Useful helpers for more than two tasks that don't fit anywhere else.
"""

import collections
import collections.abc
import contextlib
import dataclasses
import gc
import itertools
import logging
import os
import os.path
import pathlib
import resource
import subprocess
import sys
import time
import warnings

import beartype
import numpy as np
import scipy.stats
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Int, jaxtyped

from . import config


@beartype.beartype
def get_cache_dir() -> str:
    cache_dir = ""
    for var in ("BIOBENCH_CACHE", "HF_HOME", "HF_HUB_CACHE"):
        cache_dir = cache_dir or os.environ.get(var, "")
    return cache_dir or "."


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress"):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)

    def __iter__(self):
        start = time.time()
        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if isinstance(self.it, collections.abc.Sized):
                    pred_min = (len(self) - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        len(self),
                        (i + 1) / len(self) * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        return len(self.it)


@beartype.beartype
def fs_safe(string: str) -> str:
    """Makes a string safe for filesystems by removing typical special characters."""
    return string.replace(":", "_").replace("/", "_")


@beartype.beartype
def write_hparam_sweep_plot(
    task: str,
    model: str,
    clf,
    x: str = "param_ridgeclassifier__alpha",
    y: str = "mean_test_score",
) -> str:
    import matplotlib.pyplot as plt
    import polars as pl

    if not hasattr(clf, "cv_results_"):
        return ""

    df = pl.DataFrame(clf.cv_results_)

    fig, ax = plt.subplots()

    if "n_resources" in df.columns:
        for n_resources in df.get_column("n_resources").unique().sort():
            ax.scatter(
                x=df.filter(pl.col("n_resources") == n_resources)[x],
                y=df.filter(pl.col("n_resources") == n_resources)[y],
                label=f"{n_resources} ex.",
            )
        fig.legend()
    else:
        ax.scatter(x=df[x], y=df[y])

    ax.set_xlabel(x)
    ax.set_ylabel(y)
    ax.set_xscale("log")
    ax.set_title(model)

    fig.tight_layout()
    filepath = os.path.join("logs", f"{task}_{fs_safe(model)}_hparam.png")
    fig.savefig(filepath)
    return filepath


@jaxtyped(typechecker=beartype.beartype)
def balanced_random_sample(
    labels: Int[np.ndarray, " n_labels"], n: int
) -> Int[np.ndarray, " n"]:
    """
    Select n random examples while balancing the number of examples per class.
    """
    # Count the occurrences of each class
    class_counts = collections.Counter(labels)
    unique_classes = list(class_counts.keys())
    n_classes = len(unique_classes)

    if not n_classes:
        return np.array([], dtype=int)

    # Calculate ideal number of samples per class
    samples_per_class = n // n_classes

    # Handle remainder by allocating extra samples to random classes
    remainder = n % n_classes
    extra_samples = np.zeros(n_classes, dtype=int)
    if remainder > 0:
        extra_indices = np.random.choice(n_classes, remainder, replace=False)
        extra_samples[extra_indices] = 1

    # Calculate final samples per class
    final_samples = np.array([samples_per_class] * n_classes) + extra_samples

    # Initialize result array
    selected_indices = []

    # For each class, select random samples
    for i, class_label in enumerate(unique_classes):
        # Get all indices for this class
        class_indices = np.where(labels == class_label)[0]

        # Calculate how many to take (minimum of available samples and desired samples)
        n_to_take = min(len(class_indices), final_samples[i])

        # Randomly sample without replacement
        if n_to_take > 0:
            sampled_indices = np.random.choice(class_indices, n_to_take, replace=False)
            selected_indices.extend(sampled_indices)

    # If we still don't have enough samples (due to some classes having too few examples),
    # sample from the remaining examples across all classes
    if len(selected_indices) < n:
        # Create a mask of already selected indices
        mask = np.ones(len(labels), dtype=bool)
        mask[selected_indices] = False
        remaining_indices = np.where(mask)[0]

        # How many more do we need?
        needed = n - len(selected_indices)

        # Sample without replacement from remaining indices
        if needed > 0 and len(remaining_indices) > 0:
            additional_indices = np.random.choice(
                remaining_indices, min(needed, len(remaining_indices)), replace=False
            )
            selected_indices.extend(additional_indices)

    return np.array(selected_indices, dtype=int)


@beartype.beartype
class batched_idx:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """

    def __init__(self, total_size: int, batch_size: int):
        """
        Args:
            total_size: total number of examples
            batch_size: maximum distance between the generated indices
        """
        self.total_size = total_size
        self.batch_size = batch_size

    def __iter__(self) -> collections.abc.Iterator[tuple[int, int]]:
        """Yield (start, end) index pairs for batching."""
        for start in range(0, self.total_size, self.batch_size):
            stop = min(start + self.batch_size, self.total_size)
            yield start, stop

    def __len__(self) -> int:
        """Return the number of batches."""
        return (self.total_size + self.batch_size - 1) // self.batch_size


@beartype.beartype
def bump_nofile(margin: int = 512) -> None:
    """
    Make RLIMIT_NOFILE.soft = RLIMIT_NOFILE.hard - margin (if that is higher than the current soft limit).  No change if margin would push soft < 1. Raises RuntimeError if hard <= margin.
    """
    if margin < 0:
        raise ValueError("margin must be non-negative")

    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)

    if hard <= margin:
        raise RuntimeError(
            f"hard limit ({hard}) is <= margin ({margin}); ask an admin to raise the hard limit."
        )

    target_soft = hard - margin
    if soft < target_soft:
        resource.setrlimit(resource.RLIMIT_NOFILE, (target_soft, hard))


@beartype.beartype
def _default_batchsize_schedule(start: int = 2) -> collections.abc.Iterable[int]:
    """
    2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 196, 256, 384, 512, 768, 1024, ...
    """

    while start < 2:
        yield start
        start += 1

    x = start
    for m in itertools.cycle((3 / 2, 4 / 3)):  # 3/2, 4/3, 3/2, 4/3, ...
        yield int(x)
        x *= m


@beartype.beartype
def infer_batch_size(batch: object) -> int | None:
    """
    Return the leading dimension of the *first* tensor found inside `batch`. Works for arbitrary nested structures.
    """
    if isinstance(batch, torch.Tensor):
        return batch.shape[0]

    if isinstance(batch, (list, tuple)):
        for item in batch:
            bs = infer_batch_size(item)
            if bs is not None:
                return bs

    if isinstance(batch, dict):
        for item in batch.values():
            bs = infer_batch_size(item)
            if bs is not None:
                return bs

    if dataclasses.is_dataclass(batch):
        return infer_batch_size(dataclasses.asdict(batch))

    # Fallback: inspect attributes (namedtuple, SimpleNamespace, custom)
    if hasattr(batch, "__dict__"):
        return infer_batch_size(vars(batch))

    return None


@contextlib.contextmanager
@beartype.beartype
def auto_batch_size(
    dataloader: torch.utils.data.DataLoader,
    *,
    probe: collections.abc.Callable[[torch.Tensor], torch.Tensor],
    schedule: collections.abc.Iterable[int] | None = None,
    upper: int = 4096,
    backoff: int = 0,
):
    """Context manager that mutates `dataloader.batch_size` in-place to use the largest batch that fits GPU RAM.

    This function tests progressively larger batch sizes until it finds the maximum that can be processed without running out of memory.

    Args:
        dataloader: The already constructed loader you use in your loop. Its `batch_sampler.batch_size` attribute is patched on the fly.
        probe: A 1-argument callable used to test memory usage. Typical: `lambda x: backbone.img_encode(x).img_features`.
        schedule: An iterator of candidate batch sizes. If None, use the canonical schedule.
        schedule: An iterator of strictly increasing candidate batch sizes (2, 4, 8, ...). A *ValueError* is raised when a non-increasing value is encountered. If None, use the canonical schedule.
        upper: Maximum batch size to try, regardless of available memory.
        backoff: int, default = 0. How far to step **back** in the candidate schedule from the largest batch-size that completes without OOM  (clamped to the smallest candidate if ``n`` is too big).
        * `backoff = 0`  -> use the **largest** successful size
        * `backoff = 1`  -> use the **second-largest** successful size
        * `backoff = n`  -> use the *n*-th size below the largest success

    Yields:
        int: The selected batch size.
    """

    @beartype.beartype
    class OrderedSet[T]:
        def __init__(self):
            self._lst = []

        def __bool__(self) -> bool:
            return bool(self._lst)

        @property
        def last(self) -> T:
            return self._lst[-1]

        def append(self, t: T) -> None:
            if t in self._lst:
                return

            self._lst.append(t)

        def pop(self) -> T:
            return self._lst.pop()

        def __repr__(self) -> str:
            return f"{self.__class__.__name__}({self._items!r})"

        def __str__(self) -> str:
            return str(self._items)

    if dataloader.batch_sampler is None:
        raise ValueError("dataloader must have a batch_sampler")

    if backoff < 0:
        raise ValueError(f"backoff '{backoff}' < 0; must be >= 0")

    logger = logging.getLogger("auto-bsz")

    oom_signatures = (
        "out of memory",
        "cuda error: invalid configuration argument",  # SPM-efficient-attn OOM
        "expected canuse32bitindexmath(input) && canuse32bitindexmath(output) to be true, but got false.",  # Conv layers with big batch sizes.
    )

    dataloader.batch_sampler.batch_size = min(
        dataloader.batch_sampler.batch_size, upper
    )

    orig_bsz = int(dataloader.batch_sampler.batch_size)
    good_bszs = OrderedSet()
    schedule_iter = schedule or _default_batchsize_schedule(orig_bsz)

    torch.cuda.empty_cache()  # be nice

    t_start = time.perf_counter()

    for tried_bsz in schedule_iter:
        if good_bszs and tried_bsz <= good_bszs.last:
            raise ValueError(
                f"Schedule not monotonically increasing: {tried_bsz} <= {good_bszs.last}"
            )

        if tried_bsz > upper:
            tried_bsz = upper

        # patch sampler attr
        dataloader.batch_sampler.batch_size = tried_bsz
        logger.info("Trying batch_size=%d", tried_bsz)

        # pull ONE mini-batch, send through probe
        try:
            batch = next(iter(dataloader))
            probe(batch)  # forward only; discard output

            # If the loader produced fewer items than we asked for, we've reached the dataset size -- any larger batch will give the same tensor, so stop growing.
            effective_bsz = infer_batch_size(batch)
            if effective_bsz is None:
                raise RuntimeError(
                    "Unable to deduce batch size from probe batch; ensure it contains at least one torch.Tensor."
                )

            if effective_bsz < tried_bsz:
                logger.info(
                    "Dataset exhausted at %d examples (asked for %d); capping batch size.",
                    effective_bsz,
                    tried_bsz,
                )
                ok_bsz = effective_bsz
                good_bszs.append(ok_bsz)
                break

            ok_bsz = tried_bsz
            good_bszs.append(ok_bsz)
            logger.info("batch_size=%d succeeded", ok_bsz)

            # honor explicit ceiling
            if upper is not None and ok_bsz >= upper:
                logger.info("Reached ok_bsz (%d) >= upper (%d)", ok_bsz, upper)
                ok_bsz = upper
                break

        except RuntimeError as err:
            msg = str(err).lower()
            if any(sig in msg for sig in oom_signatures):
                logger.info("OOM at batch_size=%d; reverting to %d", tried_bsz, ok_bsz)
                torch.cuda.empty_cache()
                break
            else:
                raise

    # (re-)verify ok_bs once more in a clean context
    while good_bszs:
        ok_bsz = good_bszs.pop()
        dataloader.batch_sampler.batch_size = ok_bsz
        try:
            batch = next(iter(dataloader))
            probe(batch)
            break  # we know ok_bsz is actually good.

        except RuntimeError as err:
            if any(sig in str(err).lower() for sig in oom_signatures):
                logger.info("Still OOM at %d; trying previous candidate", ok_bsz)
            else:
                raise

    while good_bszs and backoff:
        backoff -= 1
        ok_bsz = good_bszs.pop()
        dataloader.batch_sampler.batch_size = ok_bsz

    elapsed = time.perf_counter() - t_start
    logger.info("Selected batch_size %d after %.2f s", ok_bsz, elapsed)

    # Final tidy-up to avoid residual OOMs
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()  # frees cached blocks from other procs
    gc.collect()  # clears Python refs / fragments

    try:
        yield ok_bsz  # user code runs here with patched batch_size
    finally:
        # always restore original value
        dataloader.batch_sampler.batch_size = orig_bsz


NFS_TYPES = {"nfs", "nfs4", "nfsd", "autofs"}  # extend if you wish


@beartype.beartype
def warn_if_nfs(path: str | os.PathLike):
    """
    If *path* is on an NFS mount, emit a RuntimeWarning.

    Works on Linux (/proc/mounts) and macOS/BSD (`mount` CLI); silently returns on other OSes or if detection fails.
    """
    p = pathlib.Path(path).resolve()

    # Linux: /proc/self/mountinfo
    if sys.platform.startswith("linux"):
        try:
            with open("/proc/self/mountinfo") as fd:
                entries = [line.split() for line in fd]
            # fields: 4= mount point, - separator -, fstype
            mounts = {fields[4]: fields[-3] for fields in entries}
        except Exception:
            return

    # macOS / BSD: `mount`
    elif sys.platform in {"darwin", "freebsd"}:
        try:
            out = subprocess.check_output(["mount", "-p"], text=True)
            mounts = {}
            for line in out.splitlines():
                mp, _dev, fstype, *_ = line.split()  # mount-point, ...
                mounts[mp] = fstype
        except Exception:
            return
    else:
        return  # unsupported OS

    # find longest mount-point prefix of *p*
    mount_point = max(
        (mp for mp in mounts if p.is_relative_to(mp) or mp == "/"),
        key=len,
        default="/",
    )
    if mounts.get(mount_point) in NFS_TYPES:
        warnings.warn(
            f"SQLite database '{path}' appears to be on an NFS mount (fs type: {mounts[mount_point]}). Concurrent writers over NFS can corrupt the journal; consider using a local SSD or tmpfs instead.",
            RuntimeWarning,
            stacklevel=2,
        )


def infinite(dataloader):
    """Creates an infinite iterator from a dataloader by creating a new iterator each time the previous one is exhausted.

    Args:
        dataloader: A PyTorch dataloader or similar iterable

    Yields:
        Batches from the dataloader, indefinitely
    """
    while True:
        # Create a fresh iterator from the dataloader
        it = iter(dataloader)
        for batch in it:
            yield batch


@beartype.beartype
def init_logreg_clf(cfg: config.Experiment):
    clf = sklearn.pipeline.make_pipeline(
        sklearn.preprocessing.StandardScaler(),
        sklearn.linear_model.LogisticRegression(
            max_iter=1_000,
            class_weight="balanced",
            penalty="l2",
            random_state=cfg.seed,
            solver="saga",
        ),
    )

    return sklearn.model_selection.RandomizedSearchCV(
        clf,
        {"logisticregression__C": scipy.stats.loguniform(a=1e-3, b=1e1)},
        n_iter=30,
        n_jobs=8,
        verbose=3,
        random_state=cfg.seed,
        scoring="f1_macro",
    )

>>>> herbarium19/__init__.py
"""
Herbarium19: classify specimens into species based on the 2019 FGVC6 competition.

```
@article{tan2019herbarium,
  title={The herbarium challenge 2019 dataset},
  author={Tan, Kiat Chuan and Liu, Yulong and Ambrose, Barbara and Tulig, Melissa and Belongie, Serge},
  journal={arXiv preprint arXiv:1906.05372},
  year={2019}
}
```
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import torch
import torchvision.datasets
from jaxtyping import Float, Int, Shaped, jaxtyped
from torch import Tensor

from .. import config, helpers, linear_probing, registry, reporting

logger = logging.getLogger("herbarium19")


@jaxtyped(typechecker=beartype.beartype)
class Sample(typing.TypedDict):
    """A dictionary representing a single image sample with its metadata.

    Attributes:
        img_id: Unique identifier for the image.
        img: The image tensor with shape [3, width, height] (RGB channels first).
        label: Binary class label (0 or 1) for the image.
    """

    img_id: str
    img: Float[Tensor, "3 width height"]
    label: Int[Tensor, ""]


@beartype.beartype
class Dataset(torchvision.datasets.ImageFolder):
    """ImageFolder but returns Sample."""

    def __getitem__(self, index) -> Sample:
        path, label = self.samples[index]
        img = self.loader(path)
        if self.transform:
            img = self.transform(img)
        return {"img_id": path, "img": img, "label": label}

    @property
    def labels(self) -> Int[np.ndarray, " n"]:
        return np.array([label for _, label in self.samples])


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)

    train_feats = get_features(cfg, backbone, is_train=True)
    test_feats = get_features(cfg, backbone, is_train=False)

    torch.cuda.empty_cache()  # Be nice to others on the machine.

    clf = init_clf(cfg)
    clf.fit(train_feats.x, train_feats.y)

    preds = clf.predict(test_feats.x)
    examples = [
        reporting.Prediction(
            img_id, float(p == t), {"y_pred": p.item(), "y_true": t.item()}
        )
        for img_id, p, t in zip(test_feats.ids, preds, test_feats.y)
    ]
    return reporting.Report("herbarium19", examples, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["herbarium19"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    split = "train" if is_train else "validation"
    images_dir_path = os.path.join(cfg.data.herbarium19, split)

    img_transform = backbone.make_img_transform()
    backbone = backbone.to(cfg.device)
    dataset = Dataset(images_dir_path, img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    def probe(batch):
        imgs = batch["img"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            # forward only
            _ = backbone.img_encode(imgs).img_features

    all_ids, all_features, all_labels = [], [], []
    # Set an upper limit. Otherwise we spend a lot of time picking an optimal batch size when we could just rip through the dataset.
    with helpers.auto_batch_size(dataloader, probe=probe, upper=512):
        backbone = torch.compile(backbone)

        for batch in helpers.progress(dataloader, every=10, desc=f"hb19/{split}"):
            imgs = batch["img"].to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_ids.extend(batch["img_id"])
            all_labels.extend(batch["label"])

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)
    assert len(all_ids) == len(dataset)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    clf = linear_probing.LinearProbeClassifier(device=cfg.device)
    return clf

>>>> herbarium19/download.py
# biobench/herbarium19/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the Herbarium 2019 FGVC6 dataset directly from storage.googleapis.com.

Splits:
 - train (34,225 images; 38 GB)
 - validation (2,679 images; 3 GB)
 - test (9,565 images; 11 GB)

Verifies MD5 checksums before extraction.
"""

import dataclasses
import hashlib
import os
import tarfile

import requests
import tqdm
import tyro

URLS = {
    "train": {
        "url": "https://storage.googleapis.com/nybg/herbarium-2019-fgvc6/train.tar.gz",
        "md5": "53c6b9ee2f831f5101dbe00958091dc8",
    },
    "validation": {
        "url": "https://storage.googleapis.com/nybg/herbarium-2019-fgvc6/validation.tar.gz",
        "md5": "2f854d580949e54f114993a74adc3d4b",
    },
    "test": {
        "url": "https://storage.googleapis.com/nybg/herbarium-2019-fgvc6/test.tar.gz",
        "md5": "297648fb76eed1b1c6f0ca1fd8188de0",
    },
}


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """Where to save the downloaded archives and extract them."""
    chunk_size_kb: int = 1024
    """Download chunk size in KB."""
    download_train: bool = True
    download_validation: bool = True
    download_test: bool = True
    unzip: bool = True


def md5_of_file(path: str, chunk_size: int = 8192) -> str:
    h = hashlib.md5()
    file_size = os.path.getsize(path)
    with open(path, "rb") as f:
        desc = f"MD5 for {os.path.basename(path)}"
        with tqdm.tqdm(total=file_size, unit="B", unit_scale=True, desc=desc) as bar:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                h.update(chunk)
                bar.update(len(chunk))
    return h.hexdigest()


def download_split(name: str, dest_dir: str, chunk_size: int) -> str:
    info = URLS[name]
    url, expected_md5 = info["url"], info["md5"]
    os.makedirs(dest_dir, exist_ok=True)
    archive_path = os.path.join(dest_dir, f"{name}.tar.gz")

    # Skip download if present and checksum matches
    if os.path.exists(archive_path):
        if md5_of_file(archive_path) == expected_md5:
            print(f"{name}.tar.gz already exists and MD5 matches; skipping download")
            return archive_path
        else:
            print(f"MD5 mismatch for existing {name}.tar.gz; re-downloading")

    # Stream-download
    r = requests.get(url, stream=True)
    r.raise_for_status()
    total = int(r.headers.get("content-length", 0))
    with (
        open(archive_path, "wb") as f,
        tqdm.tqdm(
            total=total, unit="B", unit_scale=True, desc=f"download {name}"
        ) as bar,
    ):
        for chunk in r.iter_content(chunk_size=chunk_size):
            f.write(chunk)
            bar.update(len(chunk))

    # verify
    actual_md5 = md5_of_file(archive_path)
    if actual_md5 != expected_md5:
        raise ValueError(
            f"MD5 mismatch for {name}.tar.gz: expected {expected_md5}, got {actual_md5}"
        )

    return archive_path


def extract_split(archive_path: str, dest_dir: str):
    with tarfile.open(archive_path, "r:gz") as tar:
        for m in tqdm.tqdm(tar, desc=f"Extracting {os.path.basename(archive_path)}"):
            tar.extract(m, path=dest_dir)


def main(args: Args):
    splits = {
        "train": args.download_train,
        "validation": args.download_validation,
        "test": args.download_test,
    }
    for name, do_dl in splits.items():
        if not do_dl:
            continue
        archive = download_split(name, args.dir, args.chunk_size_kb * 1024)
        if args.unzip:
            extract_split(archive, args.dir)
            print(f"Extracted `{name}` into {args.dir}/{name}/")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> imagenet1k/__init__.py
import dataclasses
import io
import logging
import math
import warnings

import beartype
import datasets
import datasets.formatting.torch_formatter
import numpy as np
import polars as pl
import torch
from jaxtyping import Float, Float16, Int, Shaped, jaxtyped
from PIL import Image

from .. import config, helpers, linear_probing, registry, reporting

logger = logging.getLogger("imagenet1k")

warnings.filterwarnings(
    "ignore",
    message="To copy construct from a tensor",
    category=UserWarning,
    module=datasets.formatting.torch_formatter.__name__,
)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float16[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)
    test_features = get_features(cfg, backbone, is_train=False)
    train_features = get_features(cfg, backbone, is_train=True)

    torch.cuda.empty_cache()  # Be nice to others on the machine.

    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)
    logger.info("Trained a classifier on %d examples.", len(train_features.y))

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("imagenet1k", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.micro_acc(preds)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["imagenet1k"]

    # For some reason, one of my models only has 49.3K predictions, so I only use that many. Compared to 50K it's probably fine.
    n = 49_362

    if b > 0:
        assert rng is not None, "must provide rng argument"
        i_bs = rng.integers(0, n, size=(b, n), dtype=np.int32)

    scores = {}

    correct_buf = np.zeros((b, n), dtype=bool)

    for model_ckpt in df.get_column("model_ckpt").unique().sort().to_list():
        # pull y_true and y_pred for *one* model
        y_pred = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_pred")
            .unique()
            .sort("img_id")
            .get_column("y_pred")
            .cast(pl.Int32)
            .to_numpy()
        )

        if len(y_pred) == 0:
            continue

        y_true = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_true")
            .unique()
            .sort("img_id")
            .get_column("y_true")
            .cast(pl.Int32)
            .to_numpy()
        )
        assert y_true.size == y_pred.size

        if b > 0:
            # bootstrap resample into pre-allocated buffers
            np.take(y_pred == y_true, i_bs, axis=0, out=correct_buf)
            scores[model_ckpt] = correct_buf.mean(axis=1)
        else:
            scores[model_ckpt] = np.array([(y_pred == y_true).mean()])

    return scores


class Transform:
    def __init__(self, img_transform):
        self._img_transform = img_transform

    def __call__(self, example):
        img_bytes = example["image"]["bytes"]
        img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
        example["image"] = self._img_transform(img)
        return example


@beartype.beartype
@torch.no_grad
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = backbone.to(cfg.device)
    split = "train" if is_train else "validation"

    dataset = datasets.load_dataset(
        "ILSVRC/imagenet-1k",
        split=split,
        cache_dir=helpers.get_cache_dir(),
        trust_remote_code=True,
    )

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(np.array(dataset["label"]), cfg.n_train)
        assert len(i) == cfg.n_train
    else:
        i = np.arange(dataset.num_rows)

    n_workers = min(len(i), cfg.n_workers)

    # Map
    dataset = (
        dataset.cast_column("image", datasets.Image(decode=False))
        .map(lambda ex, idx: {"id": str(idx)}, with_indices=True)
        .select(i)
        .to_iterable_dataset(num_shards=n_workers)
        .map(Transform(img_transform))
        .with_format("torch")
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.batch_size,
        num_workers=n_workers,
        drop_last=False,
        shuffle=False,
    )

    all_features, all_labels, all_ids = [], [], []

    def probe(batch):
        imgs = batch["image"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(dataloader, probe=probe) as batch_size:
        backbone = torch.compile(backbone)

        total = max(n_workers, math.ceil(len(i) / batch_size))
        it = iter(dataloader)

        logger.info("Need to embed %d batches of %d images.", total, batch_size)

        for b in helpers.progress(range(total), every=10, desc=f"in1k/{split}"):
            batch = next(it)

            images = batch["image"].to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(images).img_features

            all_features.append(features.cpu())
            all_labels.extend(batch["label"])
            all_ids.extend(batch["id"])

    all_features = torch.cat(all_features, dim=0).cpu().to(torch.float16).numpy()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels).numpy()
    assert len(all_ids) == len(i) or cfg.n_train < 0
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    clf = linear_probing.LinearProbeClassifier(device=cfg.device)
    return clf

>>>> inat21/__init__.py
"""
Trains a simple ridge regression classifier on visual representations for the iNat21 challenge.
In the challenge, there are 10K different species (classes).
We use the mini training set with 50 images per species, and test on the validation set, which has 10 images per species.

This task is a benchmark: it should help you understand how general a vision backbone's representations are.
This is not a true, real-world task.

If you use this task, be sure to cite the original iNat21 dataset paper:

```
@misc{inat2021,
  author={Van Horn, Grant and Mac Aodha, Oisin},
  title={iNat Challenge 2021 - FGVC8},
  publisher={Kaggle},
  year={2021},
  url={https://kaggle.com/competitions/inaturalist-2021}
}
```
"""

import dataclasses
import logging
import os

import beartype
import numpy as np
import torch
import torchvision.datasets
from jaxtyping import Float16, Int, Shaped, jaxtyped

from .. import config, helpers, linear_probing, registry, reporting

logger = logging.getLogger("inat21")

n_classes = 10_000


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float16[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using validation data.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Get features
    val_features = get_features(cfg, backbone, is_train=False)
    train_features = get_features(cfg, backbone, is_train=True)

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    true_labels = val_features.y
    pred_labels = clf.predict(val_features.x)

    preds = [
        reporting.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(val_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("inat21", preds, cfg)


@beartype.beartype
def score(preds: list[reporting.Prediction]) -> float:
    return reporting.micro_acc(preds)


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torchvision.datasets.ImageFolder):
    """
    Subclasses ImageFolder so that `__getitem__` includes the path, which we use as the ID.
    """

    def __getitem__(self, index: int) -> tuple[str, object, object]:
        """
        Args:
            index (int): Index

        Returns:
            tuple: (path, sample, target) where target is class_index of the target class.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return path, sample, target

    @property
    def labels(self) -> Int[np.ndarray, " n"]:
        return np.array([label for _, label in self.samples])


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = backbone.to(cfg.device)
    split = "train_mini" if is_train else "val"

    root = os.path.join(cfg.data.inat21, split)

    if not os.path.isdir(root):
        msg = f"Path '{root}' doesn't exist. Did you download the iNat21 dataset?"
        raise ValueError(msg)

    dataset = Dataset(root, img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=True,
    )

    all_ids, all_features, all_labels = [], [], []

    def probe(batch):
        ids, imgs, labels = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(
        dataloader,
        probe=probe,
        # Set an upper limit that's around 1/40 of the dataset size. Otherwise we spend a lot of time picking an optimal batch size when we could just rip through the dataset. And naturally we want a power of 2.
        upper=2 ** np.log2(len(dataset) / 40).astype(int).item(),
    ):
        backbone = torch.compile(backbone)
        for batch in helpers.progress(dataloader, desc=f"inat21/{split}"):
            ids, images, labels = batch
            images = images.to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(images).img_features

            all_features.append(features.cpu())
            all_labels.extend(labels)
            all_ids.extend(ids)

    all_features = torch.cat(all_features, dim=0).cpu().to(torch.float16).numpy()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels).numpy()
    if is_train and cfg.n_train > 0:
        assert len(all_ids) == cfg.n_train

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    clf = linear_probing.LinearProbeClassifier(device=cfg.device)
    return clf

>>>> inat21/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the iNat21 (mini) dataset.

Run with:

1. `python biobench/inat21/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.inat21.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import tarfile

import beartype
import requests
import tqdm
import tyro

val_images_url = "https://ml-inat-competition-datasets.s3.amazonaws.com/2021/val.tar.gz"
train_mini_images_url = (
    "https://ml-inat-competition-datasets.s3.amazonaws.com/2021/train_mini.tar.gz"
)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    val: bool = True
    """Whether to download validation images [8.4GB]."""
    train: bool = True
    """Whether to download (mini) train images [42GB]."""


@beartype.beartype
def download_tar(url: str, tar_path: str, chunk_size: int):
    r = requests.get(url, stream=True)
    r.raise_for_status()

    n_bytes = int(r.headers["content-length"])

    with open(tar_path, "wb") as fd:
        for chunk in tqdm.tqdm(
            r.iter_content(chunk_size=chunk_size),
            total=n_bytes / chunk_size,
            unit="b",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading",
        ):
            fd.write(chunk)


def extract_tar(tar_path: str, n_images: int, dir: str):
    with tarfile.open(tar_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=n_images + 10_000):
            tar.extract(member, path=dir, filter="data")


@beartype.beartype
def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    train_tar_path = os.path.join(args.dir, "train_mini.tar.gz")
    val_tar_path = os.path.join(args.dir, "val.tar.gz")

    if args.val:
        download_tar(val_images_url, val_tar_path, chunk_size)
        print(f"Downloaded validation images: {val_tar_path}.")

    extract_tar(val_tar_path, 100_000, args.dir)
    print("Extracted validation images.")

    if args.train:
        download_tar(train_mini_images_url, train_tar_path, chunk_size)
        print(f"Downloaded train (mini) images: {train_tar_path}.")

    extract_tar(train_tar_path, 500_000, args.dir)
    print("Extracted training images.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> iwildcam/__init__.py
"""
Fits a linear classifier that is trained using cross-entropy on the training set of iWildCam 2020.

Please cite both the Wilds paper (provides the great package code) and the original iWildCam dataset:

```
@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International conference on machine learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}

@article{beery2020iwildcam,
    title={The iWildCam 2020 Competition Dataset},
    author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},
    journal={arXiv preprint arXiv:2004.10340},
    year={2020}
}
```
"""

import dataclasses
import logging
import os.path

import beartype
import numpy as np
import polars as pl
import torch
import wilds
import wilds.common.data_loaders
from jaxtyping import Float, Int, Shaped, jaxtyped

from .. import config, helpers, linear_probing, registry, reporting

logger = logging.getLogger("iwildcam")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Load features.
    test_features = get_features(cfg, backbone, is_train=False)
    logger.info("Got test features.")

    train_features = get_features(cfg, backbone, is_train=True)
    logger.info("Got train features.")

    torch.cuda.empty_cache()  # Be nice to others on the machine.

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("iwildcam", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["iwildcam"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    if not os.path.exists(cfg.data.iwildcam) or not os.path.isdir(cfg.data.iwildcam):
        msg = f"Path '{cfg.data.iwildcam}' doesn't exist. Did you download the iWildCam dataset? See the docstring at the top of this file for instructions."
        raise RuntimeError(msg)

    dataset = wilds.get_dataset(
        dataset="iwildcam", download=False, root_dir=cfg.data.iwildcam
    )

    backbone = backbone.to(cfg.device)
    transform = backbone.make_img_transform()

    if is_train:
        subset = "train"
        dataset = dataset.get_subset(subset, transform=transform)
        if cfg.n_train > 0:
            i = helpers.balanced_random_sample(dataset.y_array.numpy(), cfg.n_train)
            assert len(i) == cfg.n_train
            dataset = torch.utils.data.Subset(dataset, i)
            # When we create a Subset, it doesn't inherit the collate method from the original dataset. The WILDS dataloader expects this attribute to be present as it uses it for the collate_fn parameter. We need to copy it from the original dataset to avoid AttributeError.
            dataset.collate = dataset.dataset.collate
        dataloader = wilds.common.data_loaders.get_train_loader(
            "standard",
            dataset,
            batch_size=cfg.batch_size,
            num_workers=cfg.n_workers,
        )
    else:
        subset = "test"
        dataset = dataset.get_subset(subset, transform=transform)
        dataloader = wilds.common.data_loaders.get_eval_loader(
            "standard",
            dataset,
            batch_size=cfg.batch_size,
            num_workers=cfg.n_workers,
        )

    all_features, all_labels, all_ids = [], [], []

    def probe(batch):
        imgs, labels, _ = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features

    with helpers.auto_batch_size(
        dataloader,
        probe=probe,
        # Set an upper limit that's around 1/40 of the dataset size. Otherwise we spend a lot of time picking an optimal batch size when we could just rip through the dataset. And naturally we want a power of 2.
        upper=2 ** np.log2(len(dataset) / 40).astype(int).item(),
    ):
        backbone = torch.compile(backbone)
        for batch in helpers.progress(dataloader, desc=f"iwildcam/{subset}"):
            imgs, labels, _ = batch
            imgs = imgs.to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_labels.extend(labels)

            ids = [str(i + len(all_ids)) for i in range(len(labels))]
            all_ids.extend(ids)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = torch.tensor(all_labels).numpy()
    all_ids = np.array(all_ids)

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    clf = linear_probing.LinearProbeClassifier(device=cfg.device)
    return clf

>>>> iwildcam/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "wilds",
#     "tyro",
# ]
# ///
import dataclasses

import tyro
import wilds


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """where to save data."""
    download: bool = True
    """whether to download the data."""


def main(args: Args):
    wilds.get_dataset(dataset="iwildcam", download=args.download, root_dir=args.dir)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> jobkit/__init__.py
from .executors import SerialExecutor
from .futures import FutureQueue
from .hooks import ExitHook

__all__ = ["ExitHook", "FutureQueue", "SerialExecutor"]

>>>> jobkit/executors.py
import logging
import os
import pathlib
import typing

import beartype
import submitit


@beartype.beartype
class SerialJob(submitit.DebugJob):
    """DebugJob without the pdb step."""

    def results(self) -> list[object]:
        self._check_not_cancelled()
        if self._submission.done():
            return [self._submission._result]

        environ_backup = dict(os.environ)
        # Restore os.environ from job creation time.
        os.environ.clear()
        os.environ.update(self.environ)

        root_logger = logging.getLogger("")
        self.paths.stdout.parent.mkdir(exist_ok=True, parents=True)
        stdout_handler = logging.FileHandler(self.paths.stdout)
        stdout_handler.setLevel(logging.DEBUG)
        stderr_handler = logging.FileHandler(self.paths.stderr)
        stderr_handler.setLevel(logging.WARNING)
        root_logger.addHandler(stdout_handler)
        root_logger.addHandler(stderr_handler)
        root_logger.warning(
            f"Logging is written both to stderr/stdout and to {self.paths.stdout}/err. "
            "But call to print will only appear in the console."
        )
        try:
            return [self._submission.result()]
        except Exception as e:
            print(e)
            raise
        finally:
            os.environ.clear()
            os.environ.update(environ_backup)
            root_logger.removeHandler(stdout_handler)
            root_logger.removeHandler(stderr_handler)


@beartype.beartype
class SerialExecutor(submitit.Executor):
    """
    Execute submitit jobs **sequentially in-process** with no interactive debugger.

    * One Python process, one GPU/CPU context: the function is called directly in the parent interpreter--exactly like DebugExecutor--but any un-caught exception is *immediately* re-raised instead of dropping into ``pdb.post_mortem``.

    * This lets a launcher loop handle failures programmatically (log, skip, retry, etc.) while preserving DebugExecutor's simple bookkeeping, stdout/stderr redirection, and environment capture.

    * Contrast:
        DebugExecutor  - in-process, blocks in pdb on error.
        LocalExecutor  - spawns a child process per job, may run jobs concurrently depending on submitit version.
        SerialExecutor - in-process, **no pdb**, always exactly one job running.

    Usage
    -----
    >>> ex = SerialExecutor("logs")
    >>> job = ex.submit(train_one, cfg)
    >>> try:
    ...     result = job.result()   # raises on failure, no (pdb) prompt
    ... except Exception as err:
    ...     handle(err, job)

    Notes
    -----
    * Inherits all parameter handling from DebugExecutor; only ``job_class`` is swapped to remove the post-mortem call.
    * Suitable for deterministic, single-GPU jobs where you want clean failure handling without interactive prompts.
    """

    job_class = SerialJob

    def __init__(self, folder: str | pathlib.Path):
        super().__init__(folder)

    def _internal_process_submissions(
        self, delayed_submissions: list[submitit.core.utils.DelayedSubmission]
    ) -> list[submitit.core.core.Job[typing.Any]]:
        return [self.job_class(self.folder, ds) for ds in delayed_submissions]

>>>> jobkit/futures.py
import logging
import time
import typing

import beartype

T = typing.TypeVar("T")

logger = logging.getLogger(__name__)


@beartype.beartype
class FutureQueue(typing.Generic[T]):
    def __init__(self, max_size: int):
        """Create queue. max_size >= 0; 0 => always full."""
        self._max_size = max_size
        self._items = []  # FIFO queue of items

    def submit(self, item: T) -> None:
        """RuntimeError if full()."""
        if self.full():
            raise RuntimeError(f"Queue is full (max_size={self._max_size})")
        self._items.append(item)

    def pop(self) -> T:
        """Block until *some* contained Job is done, remove and return its payload."""
        if not self._items:
            return None

        # First, check if any job is already done (non-blocking)
        for i, item in enumerate(self._items):
            if self._is_done(item):
                return self._items.pop(i)

        # If no job is done, wait for the first one to complete
        while self._items:
            for i, item in enumerate(self._items):
                if self._is_done(item):
                    return self._items.pop(i)
            # No job is done yet, sleep briefly before checking again
            time.sleep(0.1)

        return None

    def _is_obj_done(self, obj) -> bool:
        # Direct check for objects with done() method
        if hasattr(obj, "done") and callable(obj.done):
            try:
                # done() may raise when the job failed. Swallow it.
                return obj.done()  # True => finished (success or failed)
            except Exception:
                # Treat "raises inside done()" as "finished but failed".
                logger.exception("obj.done() failed")
                return True

        return False

    def _is_done(self, obj) -> bool:
        """Check if an object or any of its nested items is done."""
        if self._is_obj_done(obj):
            return True

        # Check first level of nesting (tuples, lists, dicts)
        if isinstance(obj, (tuple, list)):
            for item in obj:
                if self._is_obj_done(item):
                    return True

                # Check second level of nesting
                if isinstance(item, (tuple, list)):
                    for subitem in item:
                        if self._is_obj_done(subitem):
                            return True
                elif isinstance(item, dict):
                    for subitem in item.values():
                        if self._is_obj_done(subitem):
                            return True
        elif isinstance(obj, dict):
            for item in obj.values():
                if self._is_obj_done(item):
                    return True

                # Check second level of nesting
                if isinstance(item, (tuple, list)):
                    for subitem in item:
                        if self._is_obj_done(subitem):
                            return True
                elif isinstance(item, dict):
                    for subitem in item.values():
                        if self._is_obj_done(subitem):
                            return True

        return False

    def full(self) -> bool:
        """Return True if the queue is at capacity."""
        if self._max_size == 0:
            return True
        return len(self._items) >= self._max_size

    def __len__(self) -> int:
        """Return the number of items in the queue."""
        return len(self._items)

    def __bool__(self) -> bool:
        """Return True if the queue is not empty."""
        return bool(self._items)

    def __iter__(self):
        """Iterate over items in FIFO order."""
        return iter(self._items)

>>>> jobkit/hooks.py
import atexit
import collections.abc
import signal
import sys
import threading
import typing
import weakref

import beartype

HashableT = typing.TypeVar("HashableT", bound=collections.abc.Hashable)


@beartype.beartype
class ExitHook(typing.Generic[HashableT]):
    """
    Keep track of outstanding *claims* and make sure each one is released
    on SIGINT/SIGTERM or normal interpreter shutdown.

    Parameters
    ----------
    release_fn
        Callback that frees a single claim (e.g. lambda c: reporting.release_run(db, *c)).
    lock_factory
        Optional constructor--defaults to `threading.Lock` but can be swapped
        for a stub in tests.

    Typical usage
    -------------
    >>> hook = ExitHook(release_fn).register()
    >>> if claim():              # user-defined "claim" operation
    ...     hook.add(payload)
    ...     try:
    ...         run_job()
    ...     finally:
    ...         hook.discard(payload)
    """

    # ---------------------------------------------------------------------
    # Class-level state shared by all ExitHook instances
    # ---------------------------------------------------------------------

    # WeakSet lets us track "all currently alive hooks" without accidentally keeping them alive.
    _live: "weakref.WeakSet[ExitHook]" = weakref.WeakSet()
    _installed: bool = False
    _install_lock: threading.Lock = threading.Lock()

    def __init__(
        self,
        release_fn: collections.abc.Callable[[HashableT], None],
        *,
        lock_factory: collections.abc.Callable[[], threading.Lock] = threading.Lock,
    ):
        self._release_fn = release_fn
        self._claims = set()
        self._lock = lock_factory()
        self._registered = False

    def register(self) -> "ExitHook[HashableT]":
        """Install signal & atexit hooks to ensure claims are released."""
        # Register this instance and, once per process, hook up the signals.
        ExitHook._live.add(self)
        with ExitHook._install_lock:
            if not ExitHook._installed:
                signal.signal(signal.SIGINT, ExitHook._shared_handler)
                signal.signal(signal.SIGTERM, ExitHook._shared_handler)
                ExitHook._installed = True
        # Even if we installed signals earlier, tests expect atexit.register to be invoked for *each* new `register()` call after they monkey-patch it.
        atexit.register(ExitHook._shared_exit_handler)
        return self

    def add(self, claim: HashableT) -> None:
        """Add a claim to be tracked and released on exit."""
        with self._lock:
            self._claims.add(claim)

    def discard(self, claim: HashableT) -> None:
        """Remove a claim from tracking."""
        with self._lock:
            self.release_run(claim)
            self._claims.discard(claim)

    def release_run(self, claim: HashableT) -> None:
        """Thin wrapper around the release_fn."""
        self._release_fn(claim)

    # ------------------------------------------------------------------
    # Shared callbacks
    # ------------------------------------------------------------------

    @staticmethod
    def _shared_handler(signum, frame):
        """Flush claims for *all* live hooks."""
        for hook in list(ExitHook._live):
            hook._release_all_claims()

        # propagate the original intent
        if signum == signal.SIGINT:
            raise KeyboardInterrupt
        elif signum == signal.SIGTERM:
            sys.exit(128 + signum)

    @staticmethod
    def _shared_exit_handler():
        for hook in list(ExitHook._live):
            hook._release_all_claims()

    def _release_all_claims(self):
        """Release all tracked claims and clear the set."""
        with self._lock:
            for claim in self._claims:
                self._release_fn(claim)
            self._claims.clear()

>>>> jobkit/test_futures.py
"""
Unit tests for the `JobQueue` helper described in Coding-spec: ClaimReaper & JobQueue.

We use a minimal `FakeJob` stub that mimics the `submitit.Job` API (`done()` -> bool) so the tests remain independent of SubmitIt. Each test includes a docstring explaining the contract being verified.
"""

import queue
import threading
import time

import pytest

from .futures import FutureQueue


class FakeJob:
    """Simple stub that reports completion via `done()`."""

    def __init__(self, done: bool = False):
        self._done = done

    def done(self) -> bool:
        return self._done

    def mark_done(self):
        self._done = True

    # Make debugging nicer.
    def __repr__(self) -> str:
        return f"<FakeJob done={self._done}>"


class DelayedJob(FakeJob):
    """`done()` flips to True after `delay` wall-seconds."""

    def __init__(self, delay: float):
        super().__init__(done=False)
        self._ready_at = time.time() + delay

    def done(self) -> bool:
        if not self._done and time.time() >= self._ready_at:
            self._done = True
        return self._done


def test_submit_and_len():
    """
    `submit()` appends items and `len(queue)` tracks the count.

    A queue with `max_size=3` should grow from 0 -> 1 -> 2 as items are added.
    """
    q = FutureQueue(max_size=3)
    assert len(q) == 0

    q.submit(FakeJob())
    assert len(q) == 1

    q.submit(FakeJob())
    assert len(q) == 2


def test_full_and_submit_raises():
    """
    `submit()` must raise `RuntimeError` when the queue is full.

    This covers both:
    * normal capacity exhaustion (`max_size=1`)
    * the "always full" sentinel (`max_size=0`)
    """
    single = FutureQueue(max_size=1)
    single.submit(FakeJob())  # fills the queue
    with pytest.raises(RuntimeError):
        single.submit(FakeJob())  # should fail

    always_full = FutureQueue(max_size=0)
    assert always_full.full() is True
    with pytest.raises(RuntimeError):
        always_full.submit(FakeJob())  # should fail immediately


def test_bool_dunder_and_full():
    """
    `__bool__` mirrors "non-empty" and `full()` reflects capacity status.

    * Empty queue -> `bool(q)` is `False`.
    * After one insert -> `bool(q)` is `True`.
    * When at capacity -> `full()` becomes `True`.
    """
    q = FutureQueue(max_size=2)
    assert not q and len(q) == 0

    q.submit(FakeJob())
    assert q and not q.full()

    q.submit(FakeJob())
    assert q.full()


@pytest.mark.timeout(5)
def test_pop_returns_first_finished_payload():
    """
    `pop()` must return the *earliest finished* payload, not simply FIFO order.

    We enqueue two jobs:
    * `slow` -> not yet done
    * `fast` -> already done

    Even though `slow` was submitted first, `pop()` should unblock on `fast`.
    """
    slow, fast = FakeJob(done=False), FakeJob(done=True)
    q = FutureQueue(max_size=3)
    q.submit(slow)  # inserted first
    q.submit(fast)  # inserted second

    popped = q.pop()
    assert popped is fast
    assert len(q) == 1 and q.full() is False


@pytest.mark.timeout(5)
def test_pop_recognises_nested_jobs():
    """
    The queue must scan *nested* payloads (<= 2 levels, depth-first).

    We wrap a finished `FakeJob` inside a tuple along with arbitrary metadata
    and verify that `pop()` still detects completion.
    """
    done_job = FakeJob(done=True)
    nested_payload = (done_job, {"cfg": "dummy"})
    q = FutureQueue(max_size=2)
    q.submit(nested_payload)

    popped = q.pop()
    assert popped is nested_payload


@pytest.mark.timeout(5)
def test_pop_frees_capacity_for_further_submissions():
    """
    After `pop()` removes a finished payload the queue should no longer be full.

    * Fill a queue of size 1 with a *finished* job.
    * `pop()` should return immediately and leave the queue empty.
    * A subsequent `submit()` must succeed without raising.
    """
    q = FutureQueue(max_size=1)
    q.submit(FakeJob(done=True))  # queue is full

    q.pop()  # frees the slot
    assert len(q) == 0 and not q.full()

    # Should accept a new item now.
    try:
        q.submit(FakeJob())
    except RuntimeError:
        pytest.fail("Queue did not free capacity after pop()")


@pytest.mark.timeout(5)
def test_depth_two_nesting_is_detected():
    """
    The spec promises a *depth-first scan <= 2* levels deep.

    We wrap a finished job two layers down:  [ ( job ) ].  `pop()` must still
    recognise completion and unblock.
    """
    deep = [[FakeJob(done=True)]]
    q = FutureQueue(max_size=2)
    q.submit(deep)

    popped = q.pop()
    assert popped is deep  # exact payload returned


@pytest.mark.timeout(5)
def test_payload_with_multiple_jobs_mixed_status():
    """
    If any *member* job is finished the *whole* payload counts as finished.

    We craft a container with (unfinished, finished).  Even though the first
    element is *not* done, the second is -- so `pop()` must still dequeue it.
    """
    first, second = FakeJob(done=False), FakeJob(done=True)
    mixed = (first, second)
    q = FutureQueue(max_size=1)
    q.submit(mixed)

    popped = q.pop()
    assert popped is mixed
    assert len(q) == 0  # queue now empty


@pytest.mark.timeout(5)
def test_non_job_payload_left_in_queue_if_no_done_job_inside():
    """
    Submitting a payload *without* any `done()` method must *not* starve later
    finished jobs.

    We enqueue a plain dict (no job) first, followed by a finished `FakeJob`.
    `pop()` should skip over the dict, return the finished job, and leave the
    dict in the queue.
    """
    sentinel = {"msg": "not a job"}
    finished = FakeJob(done=True)

    q = FutureQueue(max_size=3)
    q.submit(sentinel)
    q.submit(finished)

    assert q.pop() is finished
    assert list(q) == [sentinel]  # dict still waiting (never satisfies pop)


@pytest.mark.timeout(5)
def test_iter_preserves_fifo_after_interleaved_pops():
    """
    `__iter__` must always reflect current FIFO order of *remaining* items.

    Scenario:
    1. enqueue A(done=False), B(done=True), C(done=False)
    2. `pop()` removes B
    3. Order should now be [A, C] -- verify with `list(q)`.
    """
    a, b, c = FakeJob(), FakeJob(done=True), FakeJob()
    q = FutureQueue(max_size=5)
    q.submit(a)
    q.submit(b)
    q.submit(c)

    q.pop()  # removes B
    assert [a, c] == list(q)


def _call_pop(fq: FutureQueue, out: queue.Queue):
    """Run fq.pop() in a separate thread and capture result / exception."""
    try:
        out.put(fq.pop())
    except Exception as exc:  # pragma: no cover
        out.put(exc)


def test_pop_returns_first_runtime_finish_not_fifo():
    """
    The job that *completes first in wall time* must be returned, regardless
    of insertion order.

    Queue order:  slow(2 s)  ->  fast(0.1 s)

    Expected: `pop()` blocks ~0.1 s, returns *fast*.
    """
    slow = DelayedJob(2.0)
    fast = DelayedJob(0.1)

    fq = FutureQueue(max_size=2)
    fq.submit(slow)  # FIFO head
    fq.submit(fast)  # finishes first

    out = queue.Queue()
    t = threading.Thread(target=_call_pop, args=(fq, out), daemon=True)
    t.start()
    t.join(timeout=5)

    assert not t.is_alive(), "pop() blocked too long"
    popped = out.get_nowait()
    assert popped is fast
    assert list(fq) == [slow]  # slow still pending


def test_blocking_pop_unblocks_when_only_job_finishes_later():
    """
    When **all** payloads are unfinished, `pop()` must wait until one finishes.

    We enqueue a single `DelayedJob(0.2)`, start `pop()` in a thread, and
    verify it returns within the safety window.
    """
    delayed = DelayedJob(0.2)
    fq = FutureQueue(max_size=1)
    fq.submit(delayed)

    out = queue.Queue()
    t = threading.Thread(target=_call_pop, args=(fq, out), daemon=True)
    t.start()
    t.join(timeout=5)

    assert not t.is_alive(), "pop() blocked >5 s"
    assert out.get_nowait() is delayed
    assert len(fq) == 0


def test_pop_skips_non_job_then_unblocks_on_future_finish():
    """
    Interleaving:  [dict, DelayedJob].

    `pop()` must *skip* the non-job payload, block until the job finishes,
    then return **the job** while leaving the dict untouched.
    """
    sentinel = {"cfg": "noop"}
    job = DelayedJob(0.1)

    fq = FutureQueue(3)
    fq.submit(sentinel)
    fq.submit(job)

    out = queue.Queue()
    t = threading.Thread(target=_call_pop, args=(fq, out), daemon=True)
    t.start()
    t.join(timeout=5)

    assert out.get_nowait() is job
    assert list(fq) == [sentinel]


def test_multiple_done_jobs_in_one_payload_returns_first_depth_first():
    """
    If a single payload contains *several* finished jobs, the queue should treat the first depth-first hit as the trigger.

    We build:  ( doneA , [ doneB ] )  -- `pop()` may choose either, but spec allows returning the *payload* not the individual job; ensure dequeueing occurs immediately.
    """
    a, b = FakeJob(done=True), FakeJob(done=True)
    nested = (a, [b])

    fq = FutureQueue(2)
    fq.submit(nested)

    assert fq.pop() is nested
    assert len(fq) == 0

>>>> jobkit/test_hooks.py
import atexit
import signal
import threading
import time

import beartype
import pytest

from .hooks import ExitHook


def _invoke_handler(handler, signum):
    """
    Call the given *handler* as the signal machinery would:
    - pass (signum, current-frame)   (frame may be None for our purposes)
    - swallow KeyboardInterrupt / SystemExit so the suite keeps running.
    """
    try:
        handler(signum, None)
    except (KeyboardInterrupt, SystemExit):
        pass


def test_register_returns_self_and_is_idempotent():
    """
    *Spec:* `register()` **must** return the same object so callers can write
    `reaper = ClaimReaper(...).register()`.

    It must also be safe to call repeatedly (harmless no-ops) because some launchers may defensively register twice.
    """

    sink = []
    hook = ExitHook(lambda claim: sink.append(claim))

    assert hook.register() is hook  # first call
    assert hook.register() is hook  # second call - still fine


def test_add_then_discard_accepts_generic_payload():
    """
    With the new signature `add(claim)`, the class must accept *any* hashable
    payload the caller chooses.  The easiest non-trivial smoke-test is a tuple.
    """

    hook = ExitHook(release_fn=lambda c: None).register()

    claim = ("cfg-xyz", "task-abc")
    hook.add(claim)  # should not raise
    hook.discard(claim)  # should not raise


def test_multiple_outstanding_claims_do_not_interfere():
    """
    Adding several distinct claims before discarding them should not trigger
    internal errors such as "set changed size during iteration".
    """

    hook = ExitHook(lambda c: None).register()
    claims = [f"job-{i}" for i in range(5)]

    for c in claims:
        hook.add(c)

    for c in claims:
        hook.discard(c)


def test_release_run_calls_injected_release_fn():
    """
    `release_run()` is documented as a *thin wrapper* that forwards to the
    injected `release_fn`.  Verify that the exact same claim object reaches the
    callback once--and only once.
    """

    hits = []
    hook = ExitHook(lambda claim: hits.append(claim))

    claim = ("id", 7)
    hook.release_run(claim)

    assert hits == [claim], "release_fn should have been invoked exactly once"


def test_add_requires_hashable_claim():
    """
    Non-hashable objects cannot be stored in the internal set that tracks
    outstanding claims.  A correct implementation therefore raises TypeError.
    """

    hook = ExitHook(lambda c: None)

    # list is non-hashable -> should blow up
    with pytest.raises((TypeError, beartype.roar.BeartypeCallHintParamViolation)):
        hook.add(["unhashable"])


def test_release_run_invokes_callback_exactly_each_time():
    """
    *Behavioural guarantee:* every explicit release_run() call must translate
    into one--and only one--invocation of the injected release_fn, regardless of
    whether the claim was previously added/discarded.
    """

    hits = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    claim = ("cfg-id-123", "task-foo")

    # Claim is active
    hook.add(claim)
    hook.release_run(claim)
    assert hits == [claim]  # called once

    # Claim is no longer tracked
    hook.discard(claim)
    hook.release_run(claim)
    assert hits == [claim, claim]  # called again, no extras


def test_sigint_releases_all_claims():
    hits = []
    claims = [f"claim-{i}" for i in range(3)]
    hook = ExitHook(lambda c: hits.append(c)).register()

    for c in claims:
        hook.add(c)

    old_handler = signal.getsignal(signal.SIGINT)
    try:
        with pytest.raises(KeyboardInterrupt):
            signal.raise_signal(signal.SIGINT)
    finally:
        signal.signal(signal.SIGINT, old_handler)  # restore for other tests

    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(claims)


def test_sigint_handler_releases_all_claims():
    """
    Core guarantee: when the SIGINT handler is invoked, every *current* claim
    is released exactly once.

    We call the handler directly rather than raising a real signal so the
    suite survives even if ClaimReaper forgot to install it.
    """

    hits, claims = [], [f"claim-{i}" for i in range(3)]
    hook = ExitHook(lambda c: hits.append(c)).register()
    for c in claims:
        hook.add(c)

    handler = signal.getsignal(signal.SIGINT)
    assert callable(handler) and handler not in (signal.SIG_DFL, signal.SIG_IGN), (
        "ClaimReaper failed to install a SIGINT handler"
    )

    _invoke_handler(handler, signal.SIGINT)

    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(claims)


def test_duplicate_add_is_idempotent_under_sigint():
    """
    Adding the same claim twice must not produce duplicate releases when the
    SIGINT handler fires.
    """

    hits, claim = [], ("cfg-42", "task-alpha")
    hook = ExitHook(lambda c: hits.append(c)).register()
    hook.add(claim)
    hook.add(claim)  # duplicate

    handler = signal.getsignal(signal.SIGINT)
    assert callable(handler), "missing SIGINT handler"
    _invoke_handler(handler, signal.SIGINT)

    assert hits == [claim], "duplicate add produced duplicate release"


def test_sigterm_handler_releases_only_current_claims():
    """
    Discarded claims should not be released by the SIGTERM handler.
    """

    hits = []
    live1, live2, discarded = "stay-1", "stay-2", "gone-x"
    hook = ExitHook(lambda c: hits.append(c)).register()

    for c in (live1, live2, discarded):
        hook.add(c)
    hook.discard(discarded)  # no longer live

    handler = signal.getsignal(signal.SIGTERM)
    assert callable(handler), "missing SIGTERM handler"
    _invoke_handler(handler, signal.SIGTERM)

    assert sorted(hits) == sorted([live1, live2])


def test_discard_unknown_claim_is_noop():
    """
    Discarding a claim that was never added should *not* raise.  This keeps
    launcher code simple because it can unconditionally discard in finally-
    blocks without first checking membership.
    """

    hook = ExitHook(lambda c: None).register()

    # Should silently ignore
    hook.discard("non-existent")


def test_register_calls_atexit(monkeypatch):
    """
    *Contract*: `register()` must install an atexit hook so that claims are released on normal interpreter shutdown.

    We patch `atexit.register` to capture the callback and assert that:
    1) it was invoked exactly once; 2) the registered object is callable.
    """

    captured: list[callable] = []

    def _fake_register(fn, *args, **kwargs):
        captured.append(fn)

    monkeypatch.setattr(atexit, "register", _fake_register)

    # construction shouldn't trigger the hook -- only .register()
    hook = ExitHook(lambda _: None)
    assert not captured

    hook.register()
    assert len(captured) == 1
    assert callable(captured[0])


def test_atexit_handler_releases_all_live_claims(monkeypatch):
    """
    Verify that the function registered with `atexit` releases *every* claim
    that is still active when the interpreter would exit.

    Strategy
    --------
    * Capture the cleanup callback via a patched `atexit.register`.
    * Add several claims.
    * Manually invoke the captured callback.
    * Confirm `release_fn` was called once per tracked claim (order irrelevant).
    """

    cleanup_fns = []

    def _capture(fn, *a, **kw):
        cleanup_fns.append(fn)

    monkeypatch.setattr(atexit, "register", _capture)

    hits = []
    hook = ExitHook(lambda c: hits.append(c)).register()
    claims = [("cfg-0", "task-0"), ("cfg-1", "task-1"), ("cfg-2", "task-2")]
    for claim in claims:
        hook.add(claim)

    # Simulate interpreter shutdown
    assert cleanup_fns, "No atexit hook registered"
    cleanup_fns[0]()  # invoke captured function

    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(claims)


def test_lock_prevents_set_mutation_during_massive_adds():
    """
    A classic failure mode is "set changed size during iteration" when the
    signal-handler walks `_claims` while another thread is adding claims.

    Strategy
    --------
    * Worker thread continuously adds new claims.
    * Main thread waits a short moment, then calls the SIGINT handler.
    * If locking is absent we'll almost certainly trigger the RuntimeError.
    * We also check that each claim is released **at most once**.
    """

    hits: list[str] = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    stop = threading.Event()

    def _producer():
        i = 0
        while not stop.is_set():
            hook.add(f"claim-{i}")
            i += 1

    t = threading.Thread(target=_producer)
    t.start()

    # Give producer a head-start so the set is being modified.
    time.sleep(0.05)

    handler = signal.getsignal(signal.SIGINT)
    _invoke_handler(handler, signal.SIGINT)

    stop.set()
    t.join()

    # No duplicates => each claim released only once (lock prevented races)
    assert len(hits) == len(set(hits))


def test_lock_prevents_set_mutation_during_discards():
    """
    Another race: thread discarding while handler iterates.
    """

    hits: list[str] = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    # Pre-populate many claims
    claims = [f"c{i}" for i in range(250)]
    for c in claims:
        hook.add(c)

    def _consumer():
        for c in claims:
            hook.discard(c)
            time.sleep(0.0005)  # keep the race window open

    t = threading.Thread(target=_consumer)
    t.start()

    # Let the consumer start discarding, then fire the handler
    time.sleep(0.02)
    handler = signal.getsignal(signal.SIGINT)
    _invoke_handler(handler, signal.SIGINT)

    t.join()

    # All *remaining* live claims were released once; any discarded before
    # the handler shouldn't re-appear, so no duplicates.
    assert len(hits) == len(set(hits))


def test_lock_serialises_multiple_concurrent_handlers():
    """
    If two threads invoke the handler almost simultaneously, the internal lock
    must guarantee:
      * no crashes,
      * each claim released <= 1 time,
      * after both finish `_claims` is empty (second call sees nothing).

    We mimic this by launching a second thread that calls the SIGTERM handler
    while the main thread does the same.
    """

    hits: list[tuple[str, str]] = []
    hook = ExitHook(lambda c: hits.append(c)).register()

    claims = [(f"cfg{i}", f"t{i}") for i in range(100)]
    for c in claims:
        hook.add(c)

    handler = signal.getsignal(signal.SIGTERM)

    barrier = threading.Barrier(2)

    def _invoke():
        barrier.wait()
        _invoke_handler(handler, signal.SIGTERM)

    t = threading.Thread(target=_invoke)
    t.start()

    barrier.wait()  # release both threads
    _invoke_handler(handler, signal.SIGTERM)
    t.join()

    # Every claim released exactly once
    assert sorted(hits) == sorted(claims)
    assert len(hits) == len(set(hits))


def test_handler_identity_shared_across_instances():
    """
    Registering two separate ClaimReaper objects must **not** replace the
    process-wide signal handler with two different callables.  Both `.register()`
    calls should leave exactly *one* shared handler installed.
    """

    old = signal.getsignal(signal.SIGINT)
    try:
        ExitHook(lambda _: None).register()
        handler1 = signal.getsignal(signal.SIGINT)

        ExitHook(lambda _: None).register()
        handler2 = signal.getsignal(signal.SIGINT)

        assert handler1 is handler2 is not signal.SIG_DFL
    finally:
        signal.signal(signal.SIGINT, old)


def test_multiple_hooks_each_release_their_own_claims():
    """
    When the shared handler runs, *every* live claim from *every* registered
    hook must be released exactly once.
    """

    hits1, hits2 = [], []
    r1 = ExitHook(lambda c: hits1.append(c)).register()
    r2 = ExitHook(lambda c: hits2.append(c)).register()

    claim1, claim2 = ("A", 1), ("B", 2)
    r1.add(claim1)
    r2.add(claim2)

    _invoke_handler(signal.getsignal(signal.SIGINT), signal.SIGINT)

    assert hits1 == [claim1]
    assert hits2 == [claim2]


def test_discarded_claims_on_one_hook_do_not_affect_others():
    """
    If hook-1 discards a claim before the signal arrives, only hook-2's
    live claim should be released.
    """

    h1, h2 = [], []
    r1 = ExitHook(lambda c: h1.append(c)).register()
    r2 = ExitHook(lambda c: h2.append(c)).register()

    gone = ("gone", 0)
    stay = ("stay", 1)

    r1.add(gone)
    r1.discard(gone)  # already finished
    r2.add(stay)

    _invoke_handler(signal.getsignal(signal.SIGINT), signal.SIGINT)

    assert h1 == []  # no spurious releases
    assert h2 == [stay]


def test_unregistered_hook_claims_are_not_released():
    """
    Claims tracked by a *non-registered* ClaimReaper instance must *not* be
    released when some other hook's handler fires.
    """

    ghost_hits, live_hits = [], []
    ghost = ExitHook(lambda c: ghost_hits.append(c))  # NOT registered
    live = ExitHook(lambda c: live_hits.append(c)).register()

    ghost.add(("ghost", 9))
    live.add(("live", 10))

    _invoke_handler(signal.getsignal(signal.SIGINT), signal.SIGINT)

    assert ghost_hits == []  # untouched
    assert live_hits == [("live", 10)]


def test_handler_is_reentrant_no_duplicate_releases():
    """
    After the first invocation empties the claim set, a *second* call should
    release nothing new.  This proves internal state was cleared.
    """

    hits = []
    r = ExitHook(lambda c: hits.append(c)).register()
    r.add(("cfg", "task"))

    handler = signal.getsignal(signal.SIGTERM)
    _invoke_handler(handler, signal.SIGTERM)  # first pass
    first_count = len(hits)

    _invoke_handler(handler, signal.SIGTERM)  # second pass
    assert len(hits) == first_count, "duplicate releases detected"


def test_can_add_new_claims_after_previous_release():
    """
    A hook should remain usable: after its claims are flushed by a signal,
    callers may add new claims and expect those to be released on the *next*
    signal.
    """

    hits = []
    r = ExitHook(lambda c: hits.append(c)).register()

    r.add("first")
    h = signal.getsignal(signal.SIGTERM)
    _invoke_handler(h, signal.SIGTERM)

    r.add("second")
    _invoke_handler(h, signal.SIGTERM)

    assert hits == ["first", "second"]


def test_second_signal_after_empty_claims_is_noop():
    """
    If no claims are outstanding, invoking the handler should simply return and
    **not** raise or append anything.
    """

    hits = []
    ExitHook(lambda c: hits.append(c)).register()

    handler = signal.getsignal(signal.SIGINT)
    _invoke_handler(handler, signal.SIGINT)  # nothing to release

    assert hits == []


def test_atexit_cleanup_is_idempotent():
    """
    The function registered with `atexit` must clear the claim set so that a second manual call is harmless (re-entrant) and releases nothing new.
    """

    captured = []

    def _capture(fn, *a, **kw):
        captured.append(fn)

    with pytest.MonkeyPatch().context() as m:
        m.setattr(atexit, "register", _capture)

        hits = []
        r = ExitHook(lambda c: hits.append(c)).register()
        r.add("x")

    # Simulate normal shutdown twice
    captured[0]()  # first call
    captured[0]()  # second call - should be a no-op

    assert hits == ["x"]

>>>> kabr/__init__.py
"""
# Kenyan Animal Behavior Recognition (KABR)

KABR is a video recognition task ([paper](https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/papers/Kholiavchenko_KABR_In-Situ_Dataset_for_Kenyan_Animal_Behavior_Recognition_From_Drone_WACVW_2024_paper.pdf), [website](https://kabrdata.xyz/), [Huggingface](https://huggingface.co/datasets/imageomics/KABR)) where the model predicts Kenyan animal behavior in short video segments.

This can be framed as a classification task: given a short video segment of a single animal, which behavior is most common within the segment?

While specialized architectures exist, we train a simple nearest-centroid classifier [which works well with few-shot tasks](https://arxiv.org/abs/1911.04623) over video representations.
We get video representations by embedding each frame of the video and taking the mean over the batch dimension.

## Data

To download the data, you need to use the dataset download script:

1. Copy-paste the [download script](https://huggingface.co/datasets/imageomics/KABR/raw/main/download.py) to your data directory, like `/scratch/KABR/download.py`.
2. Run `python download.py`. It doesn't have any requirements beyond the Python standard library.
"""

import csv
import dataclasses
import logging
import os

import beartype
import numpy as np
import polars as pl
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger(__name__)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Video:
    """A single video instance as a sequence of frames."""

    video_id: int
    """Video ID."""
    frames: list[str]
    """Paths to actual frame images."""
    labels: list[int]
    """Frame-level labels."""

    def __post_init__(self):
        err_msg = f"Video {self.video_id} has a different number of frames ({len(self.frames)} and labels ({len(self.labels)})."
        assert len(self.frames) == len(self.labels), err_msg


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """
    Clips of at most 90 frames in Charades format with each frame stored as an image.
    """

    def __init__(self, path, split: str, transform=None, seed: int = 42):
        self.path = path
        self.split = split
        self.transform = transform
        self.seed = seed

        self.rng = np.random.default_rng(seed=seed)

        self.n_frames = 16
        self.n_every = 5

        # Load videos
        #############

        frames: dict[int, list[str]] = {}
        labels: dict[int, list[int]] = {}

        if not os.path.exists(self.path) or not os.path.isdir(self.path):
            msg = f"Path '{self.path}' doesn't exist. Did you download the KABR dataset? See the docstring at the top of this file for instructions."
            raise RuntimeError(msg)

        with open(os.path.join(self.path, "annotation", f"{split}.csv")) as fd:
            reader = csv.reader(fd, delimiter=" ")
            next(reader)  # skip headers
            for _, video_id, frame_id, path, label in reader:
                video_id = int(video_id)
                frame_id = int(frame_id)
                label = int(label)

                if video_id not in frames:
                    frames[video_id] = []
                if video_id not in labels:
                    labels[video_id] = []

                if frame_id > len(frames[video_id]) + 1:
                    raise ValueError(f"Video {video_id} is missing a frame.")

                path = os.path.join(self.path, "dataset", "image", path)
                frames[video_id].append(path)
                labels[video_id].append(label)

        self.videos = [
            Video(video_id, frames[video_id], labels[video_id])
            for video_id in frames.keys()
            if len(frames[video_id]) >= self.n_frames
        ]

    def __getitem__(
        self, i: int
    ) -> tuple[list[Float[Tensor, "3 width height"]], list[int], str]:
        """
        Returns 16 frames and their labels sampled every 5 frames from a clip. The start of the clip is uniformly sampled. If there are fewer
        """
        n_every = self.n_every

        video = self.videos[i]

        while len(video.frames) < ((self.n_frames - 1) * n_every + 1):
            n_every -= 1

        if n_every <= 0:
            print(n_every, len(video.frames), ((self.n_frames - 1) * n_every + 1))
        assert n_every >= 1

        # margin is the number of extra frames on either size of the 16x5 sampled frames.
        margin = len(video.frames) - ((self.n_frames - 1) * n_every + 1)

        # Pick a random start, then pick n_frames frames every n_every frames.
        # (sam) This is likely not clear and there are probably better ways to express this in Python that is more clear to other video ML devs. Please open a PR if you know a better way!
        start = self.rng.integers(0, margin + 1)
        frames = video.frames[start:None:n_every][: self.n_frames]
        labels = video.labels[start:None:n_every][: self.n_frames]

        images = [Image.open(frame) for frame in frames]

        if self.transform is not None:
            images = [self.transform(image) for image in images]

        return images, labels, str(i)

    def __len__(self) -> int:
        return len(self.videos)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """Runs KABR benchmark."""
    # 1. Load model
    backbone = registry.load_vision_backbone(cfg.model)
    backbone = backbone.to(cfg.device)

    # 2. Load data.
    train_features = get_features(cfg, backbone, is_train=True)
    test_features = get_features(cfg, backbone, is_train=False)

    # 4. Do simpleshot.
    clf = helpers.init_logreg_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    # Return benchmark report.
    preds = [
        reporting.Prediction(
            str(video_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for video_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]
    return reporting.Report("kabr", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["kabr"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone)
    split = "train" if is_train else "val"

    dataset = Dataset(cfg.data.kabr, split, transform=img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=max(1, cfg.batch_size // 32),
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=True,
    )

    all_feats, all_labels, all_ids = [], [], []

    def probe(batch):
        frames, _, _ = batch
        frames = torch.stack(frames, dim=0)
        frames = frames.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
            n_frames, bsz, c, h, w = frames.shape
            frames = frames.view(bsz * n_frames, c, h, w)
            outputs = backbone.img_encode(frames)
            features = outputs.img_features.view(n_frames, bsz, -1)
            features = aggregate_frames(features)

    with helpers.auto_batch_size(dataloader, probe=probe, backoff=1):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc=f"kabr/{split}"):
            frames, labels, ids = next(it)
            frames = torch.stack(frames, dim=0)
            labels = torch.stack(labels, dim=0)
            frames = frames.to(cfg.device, non_blocking=True)

            with torch.amp.autocast(cfg.device):
                # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
                n_frames, bsz, c, h, w = frames.shape
                frames = frames.view(bsz * n_frames, c, h, w)
                outputs = backbone.img_encode(frames)
                features = outputs.img_features.view(n_frames, bsz, -1)

                features = aggregate_frames(features)
                all_feats.append(features.cpu())

            labels = aggregate_labels(labels)
            all_labels.append(labels.cpu())

            logger.debug("Embedded batch %d/%d", b + 1, total)
            all_ids.extend(ids)

    all_feats = torch.cat(all_feats, dim=0).cpu().numpy()
    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()
    all_ids = np.array(all_ids)

    return Features(all_feats, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
def aggregate_labels(
    labels: Int[Tensor, "n_frames n_examples"],
) -> Int[Tensor, " n_examples"]:
    """Aggregate per-frame labels to a per-video label. Uses the most common label (mode)."""
    return torch.mode(labels, dim=0).values


@jaxtyped(typechecker=beartype.beartype)
def aggregate_frames(
    features: Float[Tensor, "n_frames n_examples dim"],
) -> Float[Tensor, "n_examples dim"]:
    return torch.max(features, dim=0).values

>>>> kabr/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "beartype",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the Kenyan Animal Behavior Recognition (KABR) dataset.

Examples
--------
# bare-bones (all animals, default path):
python -m biobench.kabr.download

# custom output directory, keep zip archives:
python -m biobench.kabr.download --dir /scratch/KABR --keep-archives
"""

import collections.abc
import dataclasses
import glob
import hashlib
import os.path
import pathlib
import zipfile

import beartype
import requests
import tqdm
import tyro

# --------- #
# Constants #
# --------- #

BASE_URL = "https://huggingface.co/datasets/imageomics/KABR/resolve/main/KABR"
DATASET_PREFIX = "dataset/image/"

ANIMAL_PART_RANGE: dict[str, tuple[str, str]] = {
    "giraffes": ("aa", "ad"),
    "zebras_grevys": ("aa", "am"),
    "zebras_plains": ("aa", "al"),
}

STATIC_FILES: list[str] = [
    "README.txt",
    "annotation/classes.json",
    "annotation/distribution.xlsx",
    "annotation/train.csv",
    "annotation/val.csv",
    "configs/I3D.yaml",
    "configs/SLOWFAST.yaml",
    "configs/X3D.yaml",
    "dataset/image2video.py",
    "dataset/image2visual.py",
]

# ------- #
# Helpers #
# ------- #


@beartype.beartype
def generate_part_files(animal: str, start: str, end: str) -> list[str]:
    """Generate `dataset/image/{animal}_part_??` blocks inclusive of start/end."""
    start_a, start_b = map(ord, start)
    end_a, end_b = map(ord, end)
    return [
        f"{DATASET_PREFIX}{animal}_part_{chr(a)}{chr(b)}"
        for a in range(start_a, end_a + 1)
        for b in range(start_b, end_b + 1)
    ]


@beartype.beartype
def all_files_for_animals(animals: collections.abc.Iterable[str]) -> list[str]:
    files: list[str] = STATIC_FILES.copy()
    for animal in animals:
        files.append(f"{DATASET_PREFIX}{animal}_md5.txt")
        start, end = ANIMAL_PART_RANGE[animal]
        files.extend(generate_part_files(animal, start, end))
    return files


@beartype.beartype
def stream_download(url: str, dst: pathlib.Path, chunk_bytes: int) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    with requests.get(url, stream=True, timeout=30) as r:
        r.raise_for_status()
        total = int(r.headers.get("content-length", 0))
        with (
            open(dst, "wb") as fd,
            tqdm.tqdm(
                total=total,
                unit="B",
                unit_scale=True,
                unit_divisor=1024,
                desc=dst.name,
                leave=False,
            ) as pbar,
        ):
            for chunk in r.iter_content(chunk_size=chunk_bytes):
                fd.write(chunk)
                pbar.update(len(chunk))


@beartype.beartype
def md5_file(path: pathlib.Path, chunk_bytes: int = 8 * 1024 * 1024) -> str:
    h = hashlib.md5()
    with open(path, "rb") as fd:
        for block in iter(lambda: fd.read(chunk_bytes), b""):
            h.update(block)
    return h.hexdigest()


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    dir: pathlib.Path = pathlib.Path("KABR_files")
    """Where to place downloaded data."""
    animals: tuple[str, ...] = tuple(ANIMAL_PART_RANGE.keys())
    """Subset of animals to download."""
    chunk_size_kb: int = 1024
    """Stream chunk size in KB."""
    keep_archives: bool = False
    """Keep concatenated *.zip files & md5 after extraction."""
    skip_existing: bool = True
    """Skip download if file already present on disk."""


@beartype.beartype
def main(args: Args) -> None:
    files = all_files_for_animals(args.animals)
    chunk = args.chunk_size_kb * 1024

    print("Downloading KABR ...")
    for rel in tqdm.tqdm(files, unit="file"):
        dst = args.dir / rel
        if args.skip_existing and dst.exists():
            continue
        url = f"{BASE_URL}/{rel}"
        stream_download(url, dst, chunk)

    print("Concatenating split archives ...")
    for animal in args.animals:
        out_zip = args.dir / f"{DATASET_PREFIX}{animal}.zip"
        parts = sorted(
            glob.glob(str(args.dir / f"{DATASET_PREFIX}{animal}_part_*")),
            key=lambda p: pathlib.Path(p).name,
        )
        if out_zip.exists() or not parts:
            continue
        total_bytes = sum(os.path.getsize(p) for p in parts)
        with (
            open(out_zip, "wb") as dst,
            tqdm.tqdm(
                total=total_bytes,
                unit="B",
                unit_scale=True,
                unit_divisor=1024,
                desc=f"Concat {animal}",
                leave=False,
            ) as bar,
        ):
            for part in parts:
                with open(part, "rb") as src:
                    for chunk in iter(lambda: src.read(8 * 1024 * 1024), b""):
                        dst.write(chunk)
                        bar.update(len(chunk))
                pathlib.Path(part).unlink()

    print("Validating & extracting ...")
    for animal in tqdm.tqdm(args.animals, unit="animal"):
        md5_txt = args.dir / f"{DATASET_PREFIX}{animal}_md5.txt"
        zip_path = args.dir / f"{DATASET_PREFIX}{animal}.zip"
        if not md5_txt.exists() or not zip_path.exists():
            print(f"Skipping {animal} (missing files).")
            continue

        expected = md5_txt.read_text().strip().split()[0]
        got = md5_file(zip_path)
        if got != expected:
            raise RuntimeError(
                f"MD5 mismatch for {zip_path.name}: {got} (expected {expected})"
            )

        with zipfile.ZipFile(zip_path) as zf:
            zf.extractall(zip_path.parent)

        if not args.keep_archives:
            zip_path.unlink(missing_ok=True)
            md5_txt.unlink(missing_ok=True)

    print(f"Done. Data at: {args.dir}")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> kabr/test_download.py
import hashlib

from . import download as kd


def test_generate_part_files():
    out = kd.generate_part_files("giraffes", "aa", "ab")
    assert out == [
        "dataset/image/giraffes_part_aa",
        "dataset/image/giraffes_part_ab",
    ]


def test_all_files_for_animals_includes_static_and_md5():
    animals = ("giraffes",)
    files = kd.all_files_for_animals(animals)
    # static + md5 + first part file sanity
    assert "README.txt" in files
    assert "dataset/image/giraffes_md5.txt" in files
    assert "dataset/image/giraffes_part_aa" in files
    # no duplicates
    assert len(files) == len(set(files))


def test_md5_file(tmp_path):
    p = tmp_path / "blob.bin"
    data = b"abc" * 123
    p.write_bytes(data)
    expect = hashlib.md5(data).hexdigest()
    assert kd.md5_file(p) == expect


def test_stream_download_stub(tmp_path, monkeypatch):
    # prepare fake response
    payload = b"hello world"

    class FakeResp:
        headers = {"content-length": str(len(payload))}

        def iter_content(self, chunk_size):
            yield payload

        def raise_for_status(self): ...

        # context-manager methods
        def __enter__(self):
            return self

        def __exit__(self, *exc): ...

    monkeypatch.setattr(kd.requests, "get", lambda *a, **kw: FakeResp())
    # run
    dst = tmp_path / "file.bin"
    kd.stream_download("http://foo.bar", dst, 16)
    assert dst.read_bytes() == payload

>>>> kabr/test_kabr.py
import pytest

from . import Video


def test_video_length_check():
    Video(0, ["f1"], [0])
    with pytest.raises(AssertionError):
        Video(1, ["f1"], [0, 0])

>>>> linear_probing.py
import logging
import math

import beartype
import numpy as np
import sklearn.base
import sklearn.utils.validation
import torch
import torch.nn
import torch.utils.data

from . import helpers


@beartype.beartype
class LinearProbeClassifier(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):
    """
    LayerNorm + Linear soft-max head trained with AdamW on fixed feature vectors.
    """

    def __init__(
        self,
        n_steps: int = 10_000,
        batch_size: int = 2048,
        lr: float = 3e-4,
        weight_decay: float = 1e-2,
        device: str = "cpu",
    ):
        self.n_steps = n_steps
        self.batch_size = batch_size
        self.lr = lr
        self.weight_decay = weight_decay
        self.device = device
        self.logger = logging.getLogger("linear-probe-clf")

    # fit
    def fit(self, X, y):
        x, y = sklearn.utils.validation.check_X_y(X, y, dtype=np.float32, order="C")
        x_dim, n_cls = x.shape[1], int(np.max(y) + 1)

        # model
        self.model_ = torch.nn.Sequential(
            torch.nn.LayerNorm(x_dim, elementwise_affine=True),
            torch.nn.Linear(x_dim, n_cls, bias=True),
        ).to(self.device)
        self.model_.train()

        # param-wise wd: skip bias & LN γ,β
        decay, no_decay = [], []
        for n, p in self.model_.named_parameters():
            (decay if p.ndim > 1 else no_decay).append(p)
        opt = torch.optim.AdamW(
            [
                {"params": decay, "weight_decay": self.weight_decay},
                {"params": no_decay, "weight_decay": 0.0},
            ],
            lr=self.lr,
        )
        lr_scheduler = CosineWarmup(
            init=self.lr,
            max=self.lr,
            final=self.lr * 1e-2,
            n_warmup_steps=0,
            n_steps=self.n_steps,
        )

        # data
        dataset = torch.utils.data.TensorDataset(
            torch.from_numpy(x), torch.from_numpy(y).long()
        )
        loader = torch.utils.data.DataLoader(
            dataset, self.batch_size, shuffle=True, pin_memory=True, drop_last=False
        )

        scaler = torch.amp.GradScaler()

        # training loop
        it = helpers.infinite(loader)
        for step in helpers.progress(
            range(self.n_steps), every=self.n_steps // 100, desc="sgd"
        ):
            xb, yb = next(it)
            xb, yb = xb.to(self.device), yb.to(self.device)
            with torch.amp.autocast(self.device):
                logits = self.model_(xb)
                loss = torch.nn.functional.cross_entropy(logits, yb)
            scaler.scale(loss).backward()
            scaler.step(opt)
            scaler.update()
            lr = lr_scheduler.step()
            for param_group in opt.param_groups:
                param_group["lr"] = lr
            opt.zero_grad(set_to_none=True)

            if step % (self.n_steps // 100) == 0:
                self.logger.info(
                    "step: %d/%d (%.1f%%) loss: %.6f lr: %.3g",
                    step,
                    self.n_steps,
                    (step / self.n_steps * 100),
                    loss.item(),
                    lr,
                )

        self.model_.eval()
        return self

    # predict
    def predict(self, X):
        sklearn.utils.validation.check_is_fitted(self, ["model_"])
        x = sklearn.utils.validation.check_array(X, dtype=np.float32, order="C")
        xb = torch.from_numpy(x).to(self.device)
        with torch.no_grad():
            logits = self.model_(xb)
            preds = logits.argmax(dim=1).cpu().numpy()
        return preds

    # predict_proba
    def predict_proba(self, X):
        sklearn.utils.validation.check_is_fitted(self, ["model_"])
        x = sklearn.utils.validation.check_array(X, dtype=np.float32, order="C")
        xb = torch.from_numpy(x).to(self.device)
        with torch.no_grad():
            probs = torch.nn.functional.softmax(self.model_(xb), dim=1)
        return probs.cpu().numpy()


@beartype.beartype
class Scheduler:
    def step(self) -> float:
        err_msg = f"{self.__class__.__name__} must implement step()."
        raise NotImplementedError(err_msg)


@beartype.beartype
class Linear(Scheduler):
    def __init__(self, init: float, final: float, n_steps: int):
        self.init = init
        self.final = final
        self.n_steps = n_steps
        self._step = 0

    def step(self) -> float:
        self._step += 1
        return self.init + (self.final - self.init) * (self._step / self.n_steps)


@beartype.beartype
class CosineWarmup(Scheduler):
    def __init__(
        self,
        *,
        init: float,
        max: float,
        final: float,
        n_warmup_steps: int,
        n_steps: int,
    ):
        self.init = init
        self.max = max
        self.final = final
        self.n_warmup_steps = n_warmup_steps
        self.n_steps = n_steps
        self._step = 0

    def step(self) -> float:
        self._step += 1
        if self._step < self.n_warmup_steps:
            # Linear warmup
            return self.init + (self.max - self.init) * (
                self._step / self.n_warmup_steps
            )

        # Cosine decay.
        return self.final + 0.5 * (self.max - self.final) * (
            1
            + math.cos(
                (self._step - self.n_warmup_steps)
                / (self.n_steps - self.n_warmup_steps)
                * math.pi
            )
        )


def _plot_example_schedules():
    import matplotlib.pyplot as plt
    import numpy as np

    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 8))

    n_steps = 1000
    xs = np.arange(n_steps)

    schedule = Linear(0.1, 0.9, n_steps)
    ys = [schedule.step() for _ in xs]

    ax1.plot(xs, ys)
    ax1.set_title("Linear")

    schedule = CosineWarmup(0.1, 1.0, 0.0, 0, n_steps)
    ys = [schedule.step() for _ in xs]

    ax2.plot(xs, ys)
    ax2.set_title("CosineWarmup")

    fig.tight_layout()
    fig.savefig("schedules.png")


if __name__ == "__main__":
    _plot_example_schedules()

>>>> mammalnet/__init__.py
"""
# MammalNet

We frame behavior recognition as a classification task.
Given a short video segment, embed the video via some frame-sampling strategy and associate that embedding with a label.
We train a simple nearest-centroid classifier [which works well with few-shot tasks](https://arxiv.org/abs/1911.04623) over these representation-label pairs.

You must use torchcodec 0.2 with torch 2.6.
If you have torch 2.7, then use torchcodec 0.3.

If you use this benchmark, please cite the original work:

```
@inproceedings{chen2023mammalnet,
  title={Mammalnet: A large-scale video benchmark for mammal recognition and behavior understanding},
  author={Chen, Jun and Hu, Ming and Coker, Darren J and Berumen, Michael L and Costelloe, Blair and Beery, Sara and Rohrbach, Anna and Elhoseiny, Mohamed},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13052--13061},
  year={2023}
}
```
"""

import collections.abc
import csv
import dataclasses
import functools
import logging
import os
import os.path

import beartype
import numpy as np
import polars as pl
import torch
from jaxtyping import Float, Float16, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger(__name__)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    # 1. Load model
    backbone = registry.load_vision_backbone(cfg.model)
    backbone = backbone.to(cfg.device)

    # 2. Load data.
    train_features = get_features(cfg, backbone, is_train=True)
    test_features = get_features(cfg, backbone, is_train=False)
    torch.cuda.empty_cache()

    # 4. Do simpleshot.
    clf = helpers.init_logreg_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    true_labels = test_features.y
    pred_labels = clf.predict(test_features.x)

    # Return benchmark report.
    preds = [
        reporting.Prediction(
            str(video_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for video_id, pred, true in zip(test_features.ids, pred_labels, true_labels)
    ]
    return reporting.Report("mammalnet", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["mammalnet"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float16[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    """
    Gets all model features and true labels for all frames and all examples in the dataloader.

    Returns it as a pair of big tensors; other tasks use a dedicated class for this, but here it's just a tuple.

    Args:
        args: KABR task arguments.
        backbone: Vision backbone.
        is_train: Whether it's training data or not.

    Returns:
    """
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone)
    split = "train" if is_train else "test"

    dataset = Dataset(cfg.data.mammalnet, split=split, transform=img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=max(1, cfg.batch_size // 32),
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
    )

    def probe(batch):
        with torch.amp.autocast(cfg.device):
            frames, _, _ = batch
            frames = frames.to(cfg.device, non_blocking=True)
            bsz, n_frames, c, h, w = frames.shape
            frames = frames.view(bsz * n_frames, c, h, w)
            outputs = backbone.img_encode(frames)
            features = outputs.img_features.view(bsz, n_frames, -1)
            features = aggregate_frames(features.to(torch.float16))

    all_feats, all_labels, all_ids = [], [], []

    with helpers.auto_batch_size(dataloader, probe=probe, backoff=1):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), desc=f"mammal/{split}"):
            with torch.amp.autocast(cfg.device):
                frames, labels, ids = next(it)
                frames = frames.to(cfg.device, non_blocking=True)
                # conv2d doesn't support multiple batch dimensions, so we have to view() before and after the model.img_encode() call.
                bsz, n_frames, c, h, w = frames.shape
                frames = frames.view(bsz * n_frames, c, h, w)
                outputs = backbone.img_encode(frames)
                features = outputs.img_features.view(bsz, n_frames, -1)

                features = aggregate_frames(features.to(torch.float16))
                all_feats.append(features.cpu())

            all_labels.extend(labels)
            all_ids.extend(ids)

    all_feats = torch.cat(all_feats, dim=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)
    assert len(all_ids) == len(dataset)

    if is_train and cfg.n_train >= 0:
        assert len(all_ids) == len(dataset) == cfg.n_train

    return Features(all_feats, all_labels, all_ids)


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class Video:
    frame_fpaths: list[str]
    """Full filepaths to the frames."""
    video_id: str
    """Unique identifier for the video clip."""
    species_id: int
    """Numeric ID representing the animal species in the video."""
    behavior_id: int
    """Numeric ID representing the behavior category in the video."""


@beartype.beartype
def find_videos(
    root: str, *, split: str, composition: str = "composition", n_frames: int = 32
) -> collections.abc.Iterator[Video]:
    if not os.path.exists(root) or not os.path.isdir(root):
        msg = f"Path '{root}' doesn't exist. Did you download the MammalNet dataset?"
        raise RuntimeError(msg)

    with open(os.path.join(root, "annotation", composition, f"{split}.csv")) as fd:
        reader = csv.reader(fd, delimiter=" ")
        for rel_path, behavior_id, species_id in reader:
            # the CSV prefixes "trimmed_videos/..."
            _, fname = rel_path.split("/")
            video_id, ext = os.path.splitext(fname)
            assert ext == ".mp4"

            frames_dpath = os.path.join(root, "frames", video_id)
            if not os.path.isdir(frames_dpath):
                msg = ("Missing frames for clip '%s' in split '%s'; skipping",)
                logger.warn(msg, frames_dpath, split)
                continue

            frame_fpaths = [
                os.path.join(frames_dpath, f"frame_{f + 1:02}.jpg")
                for f in range(n_frames)
            ]
            frame_fpaths = [fpath for fpath in frame_fpaths if os.path.isfile(fpath)]

            if len(frame_fpaths) < n_frames:
                msg = "Missing %d frames for clip '%s' in split '%s; skipping"
                logger.warn(msg, n_frames - len(frame_fpaths), video_id, split)
                continue

            yield Video(frame_fpaths, video_id, int(species_id), int(behavior_id))


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    _videos: list[Video]

    def __init__(self, root: str, *, split: str, transform):
        self._videos = list(
            helpers.progress(
                find_videos(root, split=split), every=500, desc=f"videos/{split}"
            )
        )
        self._transform = transform
        logger.info("Loaded %d videos for split '%s'.", len(self), split)

    def __len__(self) -> int:
        return len(self._videos)

    def __getitem__(
        self, i
    ) -> tuple[Float[Tensor, "n_frames channels width height"], int, str]:
        video = self._videos[i]

        frames = [self._transform(Image.open(fpath)) for fpath in video.frame_fpaths]

        return torch.stack(frames, dim=0), video.behavior_id, video.video_id

    @functools.cached_property
    def labels(self) -> Int[np.ndarray, " n"]:
        return np.array([video.behavior_id for video in self._videos])


@jaxtyped(typechecker=beartype.beartype)
def aggregate_frames(
    features: Float16[Tensor, "batch n_frames dim"],
) -> Float16[Tensor, "batch dim"]:
    return torch.max(features, dim=1).values

>>>> mammalnet/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "beartype",
#     "ffmpeg-python",
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
Download the MammalNet benchmark and its annotations.
"""

import concurrent.futures
import dataclasses
import json
import logging
import math
import pathlib
import statistics
import tarfile

import beartype
import ffmpeg
import requests
import tqdm
import tyro

VIDEOS_URL = "https://mammalnet.s3.amazonaws.com/full_video.tar.gz"
ANNOTATIONS_URL = "https://mammalnet.s3.amazonaws.com/annotation.tar"

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("mammalnet.download")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """Where to save the downloaded archives and (optionally) extract them."""
    chunk_size_kb: int = 1024
    """Download chunk size (KB). 1024 KB ~ 1 MB."""
    download_videos: bool = False
    """Whether to download the video archive."""
    download_annotations: bool = False
    """Whether to download the annotation archive."""
    trim_videos: bool = False
    """Whether to create trimmed video clips based on annotations."""
    check_stats: bool = False
    """Whether to check video length statistics for both full and trimmed videos."""
    sample_frames: bool = False
    """Whether to extract stills from trimmed clips."""
    n_frames: int = 32
    """How many frames per clip."""
    n_workers: int = 16
    """Number of parallel `ffmpeg`s to spawn."""


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Annotation:
    label: str
    """The class label for this annotation segment."""
    start_s: float
    """Start time in seconds."""
    end_s: float
    """End time in seconds."""

    @property
    def duration_s(self) -> float:
        return self.end_s - self.start_s


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Detection:
    vid_id: str
    """Unique identifier for the video clip."""
    taxonomy: list[dict[str, str]]
    """Taxonomic classification information for the detected animal."""
    annotations: list[Annotation]
    """List of time segments with behavior annotations."""
    duration_s: int
    """Total duration of the video in seconds."""
    resolution: tuple[int, int]
    """Video resolution in pixels"""
    fps: int
    """Frames per second of the video."""
    subset: str
    """Dataset split this video belongs to (e.g., 'train', 'val', 'test')."""
    url: str
    """Original source URL for the video."""

    @classmethod
    def from_json(cls, vid_id, dct):
        annotations = [
            Annotation(
                label=ann["label"],
                start_s=float(ann["segment"][0]),
                end_s=float(ann["segment"][1]),
            )
            for ann in dct.pop("annotations")
        ]
        taxonomy = dct.pop("taxnomy")
        duration_s = dct.pop("duration")
        resolution = tuple(int(x) for x in dct.pop("resolution").split("x"))
        return cls(
            vid_id=vid_id,
            taxonomy=taxonomy,
            annotations=annotations,
            duration_s=duration_s,
            resolution=resolution,
            **dct,
        )


@beartype.beartype
def _download(url: str, dest: pathlib.Path, chunk_bytes: int) -> pathlib.Path:
    dest.parent.mkdir(parents=True, exist_ok=True)
    if dest.exists():
        print(f"{dest.name} already present, skipping download.")
        return dest

    r = requests.get(url, stream=True)
    r.raise_for_status()
    total = int(r.headers.get("content-length", 0))

    with (
        dest.open("wb") as f,
        tqdm.tqdm(
            total=total, unit="B", unit_scale=True, desc=f"Downloading {dest.name}"
        ) as bar,
    ):
        for chunk in r.iter_content(chunk_size=chunk_bytes):
            f.write(chunk)
            bar.update(len(chunk))
    return dest


@beartype.beartype
def _extract(archive: pathlib.Path, out_dir: pathlib.Path) -> None:
    with tarfile.open(archive, "r:*") as tar:
        for member in tqdm.tqdm(tar, desc=f"Extracting {archive.name}"):
            tar.extract(member, path=out_dir)


@beartype.beartype
def _duration(path: pathlib.Path) -> float:
    out = ffmpeg.probe(str(path))
    return float(out["format"]["duration"])


@beartype.beartype
def _n_frames(path: pathlib.Path, *, tol: float = 0.01) -> int:
    """
    Robust frame-count extractor.

    Priority:
        1.   stream.nb_frames                      (exact when present)
        2-4. derived counts that should agree within *tol*
             * duration_ts / time_base
             * duration * fps
             * coded_number_of_frames (if present)
    Returns the majority (or first) value that satisfies pair-wise agreement.
    Raises if no two estimates agree.

    tol: relative tolerance, e.g. 0.01 -> 1 % mismatch allowed.
    """
    meta = ffmpeg.probe(str(path))["streams"][0]

    def as_int(x: str | None) -> int | None:
        return int(x) if x and x.isdigit() else None

    candidates = {}

    # 1) nb_frames (often absent on H.264)
    if n := as_int(meta.get("nb_frames")):
        candidates["nb_frames"] = n

    # 2) duration_ts / time_base
    if meta.get("duration_ts") and meta.get("time_base"):
        num_ts = int(meta["duration_ts"])
        tb_num, tb_den = map(int, meta["time_base"].split("/"))
        if tb_num:
            candidates["duration_ts"] = round(num_ts * tb_den / tb_num)

    # 3) duration * fps
    fps_num, fps_den = map(int, meta["r_frame_rate"].split("/"))
    fps = fps_num / fps_den if fps_den else 0
    if meta.get("duration") and fps:
        candidates["duration*fps"] = round(float(meta["duration"]) * fps)

    # 4) coded_number_of_frames (for some codecs)
    if n := as_int(meta.get("coded_number_of_frames")):
        candidates["coded_frames"] = n

    # choose majority-agreeing integer
    votes = {}
    for _, v in candidates.items():
        votes[v] = votes.get(v, 0) + 1

    # allow near-duplicates
    for i_name, i_val in candidates.items():
        if any(
            math.isclose(i_val, j_val, rel_tol=tol)
            for j_val in votes.keys()
            if j_val != i_val
        ):
            votes[i_val] += 1

    best, count = max(votes.items(), key=lambda kv: kv[1])
    if count < 2 and len(candidates) > 1:  # no agreement
        raise RuntimeError(
            f"Inconsistent frame counts for {path.name}: "
            + ", ".join(f"{k}={v}" for k, v in candidates.items())
        )
    return best


@beartype.beartype
def _load_detections(base: pathlib.Path) -> list[Detection]:
    with open(base / "annotation" / "detection_annotations.json") as fd:
        detections = [
            Detection.from_json(key, value) for key, value in json.load(fd).items()
        ]

    return detections


@beartype.beartype
def _stats(base: pathlib.Path, n_workers: int = 8):
    detections = _load_detections(base)

    ###############
    # Full videos #
    ###############

    from_json = [det.duration_s for det in detections]
    mean_s_from_json = statistics.mean(from_json)

    vids = list(
        p for p in (base / "full_videos").iterdir() if p.suffix.lower() == ".mp4"
    )
    durations = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_duration, vid) for vid in vids]
        for fut in tqdm.tqdm(
            concurrent.futures.as_completed(futs),
            total=len(futs),
            desc="full video durations",
        ):
            durations.append(fut.result())
    mean_s_from_disk = statistics.mean(durations)

    print("From paper:")
    print(f"  Mean (s) : {106:6.1f}")
    print("From detection_annotations.json:")
    print(f"  Mean (s) : {mean_s_from_json:6.1f}")
    print(f"From {base / 'full_videos'}:")
    print(f"  Mean (s) : {mean_s_from_disk:6.1f}")

    ##################
    # Trimmed videos #
    ##################

    # Calculate expected durations from annotations
    from_json = [ann.duration_s for det in detections for ann in det.annotations]
    mean_s_from_json = statistics.mean(from_json)

    vids = list(
        p for p in (base / "trimmed_videos").iterdir() if p.suffix.lower() == ".mp4"
    )
    durations = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_duration, vid) for vid in vids]
        for fut in tqdm.tqdm(
            concurrent.futures.as_completed(futs),
            total=len(futs),
            desc="trimmed video durations",
        ):
            if err := fut.exception():
                logger.warning("Exception: %s", err)
                continue
            durations.append(fut.result())
    mean_s_from_disk = statistics.mean(durations)

    print("From paper:")
    print(f"  Mean (s) : {77:6.1f}")
    print("From detection_annotations.json:")
    print(f"  Mean (s) : {mean_s_from_json:6.1f}")
    print(f"From {base / 'trimmed_videos'}:")
    print(f"  Mean (s) : {mean_s_from_disk:6.1f}")


@beartype.beartype
def _trim(src: pathlib.Path, dst: pathlib.Path, start_s: float, end_s: float):
    """Copy-trim [start_s, end_s] from *src* into *dst* without re-encoding."""
    if start_s >= end_s:
        raise ValueError("start_s must be < end_s")

    duration = end_s - start_s
    (
        ffmpeg
        # fast seek to ~start (input-side -ss is key-frame aligned, faster)
        .input(str(src), ss=start_s)
        # output-side -t gives exact length; libx264/AAC re-encodes safely
        .output(
            str(dst),
            t=duration,
            vcodec="libx264",
            crf=23,
            preset="veryfast",
            movflags="+faststart",  # web-friendly moov placement
            loglevel="error",  # silence ffmpeg spam unless errors
            **{"an": None},  # <- -an  (strip audio)
        )
        .overwrite_output()
        .run()
    )


@beartype.beartype
def _trim_all(base: pathlib.Path, n_workers: int):
    (base / "trimmed_videos").mkdir(exist_ok=True)
    jobs = []
    with open(base / "annotation" / "detection_annotations.json") as fd:
        for key, value in json.load(fd).items():
            det = Detection.from_json(key, value)

            src = base / "full_videos" / f"{det.id}.mp4"
            if len(det.annotations) > 1:
                for k, ann in enumerate(det.annotations):
                    dst = base / "trimmed_videos" / f"{det.id}_{k + 1}.mp4"
                    jobs.append((src, dst, ann.start_s, ann.end_s))
            else:
                ann = det.annotations[0]
                dst = base / "trimmed_videos" / f"{det.id}.mp4"
                jobs.append((src, dst, ann.start_s, ann.end_s))

    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_trim, *job) for job in jobs]
        for fut in tqdm.tqdm(concurrent.futures.as_completed(futs), total=len(futs)):
            fut.result()  # re-raise on failure


@beartype.beartype
def _sample(src: pathlib.Path, dst_dir: pathlib.Path, *, n_frames: int):
    if dst_dir.exists() and any(dst_dir.iterdir()):
        return  # already done

    step = max(_n_frames(src) // n_frames, 1)  # stride in frame space

    dst_dir.mkdir(parents=True, exist_ok=True)

    (
        ffmpeg.input(str(src))
        .filter("select", f"not(mod(n,{step}))")
        .filter("setpts", "N/FRAME_RATE/TB")
        .output(
            str(dst_dir / "frame_%02d.jpg"),
            vframes=n_frames,
            vsync="vfr",
            qscale=2,
            loglevel="error",
            **{"an": None},
        )
        .overwrite_output()
        .run()
    )


@beartype.beartype
def _sample_frames(base: pathlib.Path, *, n_workers: int, n_frames: int):
    trimmed = base / "trimmed_videos"
    out = base / "frames"
    vids = [p for p in trimmed.iterdir() if p.suffix.lower() == ".mp4"]

    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:
        futs = [pool.submit(_sample, p, out / p.stem, n_frames=n_frames) for p in vids]
        for fut in tqdm.tqdm(concurrent.futures.as_completed(futs), total=len(futs)):
            fut.result()  # re-raise on failure


@beartype.beartype
def main(args: Args) -> None:
    base = pathlib.Path(args.dir).expanduser().resolve()
    chunk = args.chunk_size_kb * 1024

    if args.download_videos:
        target = base / pathlib.Path(VIDEOS_URL).name
        archive = _download(VIDEOS_URL, target, chunk)

        _extract(archive, base)
        print(f"Extracted videos into {base}")

    if args.download_annotations:
        target = base / pathlib.Path(ANNOTATIONS_URL).name
        archive = _download(ANNOTATIONS_URL, target, chunk)

        _extract(archive, base)
        print(f"Extracted annotations into {base}")

    if args.trim_videos:
        _trim_all(base, args.n_workers)

    if args.check_stats:
        _stats(base, args.n_workers)

    if args.sample_frames:
        _sample_frames(base, n_workers=args.n_workers, n_frames=args.n_frames)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> newt/__init__.py
"""
# NeWT: Natural World Tasks

NeWT is a collection of 164 binary classification tasks related to visual understanding of the natural world ([CVPR 2021 paper](https://arxiv.org/abs/2103.16483), [code](https://github.com/visipedia/newt/tree/main)).

We evaluate a vision model by extracting visual features for each image, fitting a linear SVM to the training examples, and evaluating on the test data.
We aggregate scores across all 164 tasks.

If you use this evaluation, be sure to cite the original work:

```
@inproceedings{van2021benchmarking,
  title={Benchmarking Representation Learning for Natural World Image Collections},
  author={Van Horn, Grant and Cole, Elijah and Beery, Sara and Wilber, Kimberly and Belongie, Serge and Mac Aodha, Oisin},
  booktitle={Computer Vision and Pattern Recognition},
  year={2021}
}
```
"""

import collections.abc
import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import scipy.stats
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.svm
import torch
from jaxtyping import Bool, Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("newt")


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    The NeWT benchmark.
    First, get features for all images.
    Second, select the subsets of features that correspond to different tasks and train an SVM.
    Third, evaluate the SVM and report results.
    """

    # Fit SVMs.
    all_preds = []
    for task in get_all_tasks(cfg):
        (x_train, y_train), (x_test, y_test) = task.splits

        x_mean = x_train.mean(axis=0, keepdims=True)

        x_train = x_train - x_mean
        x_train = l2_normalize(x_train)

        x_test = x_test - x_mean
        x_test = l2_normalize(x_test)

        svc = init_svc(cfg.n_train)

        svc.fit(x_train, y_train)
        y_pred = svc.predict(x_test)
        info = {
            "task": task.name,
            "cluster": task.cluster,
            "subcluster": task.subcluster,
        }
        preds = [
            reporting.Prediction(str(id), float(pred == true), info)
            for id, pred, true in zip(task.example_ids, y_pred, y_test)
        ]

        all_preds.extend(preds)

    return reporting.Report("newt", all_preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["newt"]

    n, *rest = df.group_by("model_ckpt").agg(n=pl.len()).get_column("n").to_list()
    assert all(n == i for i in rest)

    if b > 0:
        assert rng is not None, "must provide rng argument"
        i_bs = rng.integers(0, n, size=(b, n), dtype=np.int32)

    scores = {}

    scores_buf = np.empty((b, n), dtype=np.float32)

    for model_ckpt in df.get_column("model_ckpt").unique().sort().to_list():
        # pull y_true and y_pred for *one* model
        scores_ = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "score")
            .unique()
            .sort("img_id")
            .get_column("score")
            .cast(pl.Float32)
            .to_numpy()
        )

        if len(scores_) == 0:
            continue

        if b > 0:
            # bootstrap resample into pre-allocated buffers
            np.take(scores_, i_bs, axis=0, out=scores_buf)
            scores[model_ckpt] = scores_buf.mean(axis=1)
        else:
            scores[model_ckpt] = np.array([scores_.mean()])

    return scores


@jaxtyped(typechecker=beartype.beartype)
class Sample(typing.TypedDict):
    """A dictionary representing a single image sample with its metadata.

    Attributes:
        img_id: Unique identifier for the image.
        img: The image tensor with shape [3, width, height] (RGB channels first).
        label: Binary class label (0 or 1) for the image.
    """

    img_id: str
    img: Float[Tensor, "3 width height"]
    label: Int[Tensor, ""]


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """A dataset that returns ImageSample dictionaries."""

    def __init__(
        self,
        root: str,
        img_ids: Shaped[np.ndarray, " n"],
        labels: Int[np.ndarray, " n"],
        transform=None,
    ):
        """Initialize the dataset with image paths and labels.

        Args:
            root: Root directory containing the images.
            img_ids: Array of image IDs.
            labels: Array of binary labels corresponding to the images.
            transform: Optional transform to apply to the images.
        """
        self.transform = transform
        self.root = root
        self.img_ids = img_ids
        self.labels = labels

    def __getitem__(self, i: int) -> Sample:
        """Get a sample by its index.

        Args:
            i: Index of the sample to retrieve.

        Returns:
            A dictionary containing the image ID, image tensor, and label.
        """
        img_id = self.img_ids[i]
        img = Image.open(os.path.join(self.root, f"{img_id}.jpg"))
        if self.transform is not None:
            img = self.transform(img)
        label = self.labels[i]
        return {"img_id": img_id, "img": img, "label": label}

    def __len__(self) -> int:
        """Return the number of samples in the dataset.

        Returns:
            The number of samples.
        """
        return len(self.img_ids)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Task:
    """
    Task is a group of features and labels for an SVM + a train/test split.
    """

    name: str
    cluster: str
    subcluster: str | None
    features: Float[np.ndarray, "batch dim"]
    labels: Int[np.ndarray, " batch"]
    is_train: Bool[np.ndarray, " batch"]
    example_ids: Shaped[np.ndarray, " batch"]  # Should be String[...]

    def __repr__(self) -> str:
        return f"Task(task={self.name}, cluster={self.cluster}, features={self.features.shape})"

    @property
    def splits(
        self,
    ) -> tuple[
        tuple[Float[np.ndarray, "n_train dim"], Int[np.ndarray, " n_train"]],
        tuple[Float[np.ndarray, "n_test dim"], Int[np.ndarray, " n_test"]],
    ]:
        """
        The features and labels for train and test splits.

        Returned as `(x_train, y_train), (x_test, y_test)`.
        """
        x_train = self.features[self.is_train]
        y_train = self.labels[self.is_train]
        x_test = self.features[~self.is_train]
        y_test = self.labels[~self.is_train]

        return (x_train, y_train), (x_test, y_test)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_all_tasks(cfg: config.Experiment) -> collections.abc.Iterator[Task]:
    """ """
    rng = np.random.default_rng(seed=cfg.seed)

    # Load model
    backbone = registry.load_vision_backbone(cfg.model)
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(cfg.data.newt, labels_csv_name)
    imgs_dir_name = "newt2021_images"
    imgs_dir_path = os.path.join(cfg.data.newt, imgs_dir_name)

    if not os.path.isfile(labels_csv_path):
        msg = f"Path '{labels_csv_path}' doesn't exist. Did you download the Newt dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path with '--data'; see --help for more."
        raise RuntimeError(msg)

    # Read the CSV and add row indices
    df = pl.read_csv(labels_csv_path).with_row_index(name="original_index")

    # Sample balanced training data for each task
    df = sample(rng, df, cfg.n_train).with_row_index(name="sampled_index")

    # Get all image IDs and labels
    all_data = df.select("id", "label").to_numpy(structured=True)
    all_ids, all_labels = all_data["id"], all_data["label"]

    # Create dataset with all samples
    dataset = Dataset(
        imgs_dir_path,
        all_ids,
        all_labels,
        img_transform,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    def probe(batch):
        imgs = batch["img"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features  # forward only

    all_features, all_ids = [], []

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc="newt"):
            batch = next(it)
            imgs = batch["img"].to(cfg.device)

            with torch.amp.autocast("cuda"):
                features = backbone.img_encode(imgs).img_features
                features = torch.nn.functional.normalize(features, dim=-1)
                all_features.append(features.cpu())

            all_ids.extend(batch["img_id"])

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)

    for task in df.get_column("task").unique():
        task_df = df.filter(pl.col("task") == task)

        task_idx = task_df.get_column("sampled_index").to_numpy()
        features = all_features[task_idx].numpy()
        ids = all_ids[task_idx]

        labels = task_df.get_column("label").to_numpy()
        is_train = task_df.select(pl.col("split") == "train").get_column("split")

        cluster = task_df.item(row=0, column="task_cluster")
        subcluster = task_df.item(row=0, column="task_subcluster")
        yield Task(
            task, cluster, subcluster, features, labels, is_train.to_numpy(), ids
        )


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[np.ndarray, "batch dim"],
) -> Float[np.ndarray, "batch dim"]:
    """Normalizes a batch of vectors to have L2 unit norm."""
    norms = np.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


def init_svc(n_train: int):
    """Create a new, randomly initialized SVM with a random hyperparameter search over kernel, C and gamma. It uses only 16 jobs in parallel to prevent overloading the CPUs on a shared machine."""
    if n_train < 10:
        return sklearn.pipeline.make_pipeline(
            sklearn.svm.SVC(kernel="linear"),
        )

    return sklearn.model_selection.RandomizedSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.svm.SVC(C=1.0, kernel="rbf"),
        ),
        {
            "svc__C": scipy.stats.loguniform(a=1e-3, b=1e1),
            "svc__kernel": ["rbf", "linear", "sigmoid", "poly"],
            "svc__gamma": scipy.stats.loguniform(a=1e-4, b=1e-3),
        },
        n_iter=100,
        n_jobs=16,
        random_state=42,
    )


@jaxtyped(typechecker=beartype.beartype)
def sample(rng: np.random.Generator, df: pl.DataFrame, n_train: int) -> pl.DataFrame:
    """Sample a balanced subset of training data points for each task.

    Args:
        rng: Random number generator.
        df: NeWT dataframe.
        n_train: Number of training samples per task to return.

    Returns:
        A DataFrame with balanced training samples and all test samples.
    """
    if n_train <= 0:
        return df  # Return all data if n_train is not positive

    # Create a new dataframe to store the results
    result_dfs = []

    # Keep all test samples
    test_df = df.filter(pl.col("split") != "train")
    result_dfs.append(test_df)

    # Process each task separately
    for task in df.get_column("task").unique():
        task_df = df.filter((pl.col("task") == task) & (pl.col("split") == "train"))

        # Skip if the task has no training samples
        if task_df.height == 0:
            continue

        # Get samples for each class
        class0_df = task_df.filter(pl.col("label") == 0)
        class1_df = task_df.filter(pl.col("label") == 1)

        n0 = n_train // 2
        n1 = n_train - n0

        assert n0 > 0
        assert n1 > 0

        # Sample from each class
        if n0 < class0_df.height:
            indices0 = rng.choice(class0_df.height, size=n0, replace=False)
            result_dfs.append(
                class0_df.with_row_index(name="tmp")
                .filter(pl.col("tmp").is_in(indices0))
                .drop("tmp")
            )
        else:
            result_dfs.append(class0_df)

        if n1 < class1_df.height:
            indices1 = rng.choice(class1_df.height, size=n1, replace=False)
            result_dfs.append(
                class1_df.with_row_index(name="tmp")
                .filter(pl.col("tmp").is_in(indices1))
                .drop("tmp")
            )
        else:
            result_dfs.append(class1_df)

    # Combine all dataframes
    return pl.concat(result_dfs)

>>>> newt/download.py
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the NeWT dataset.

Run with:

1. `python biobench/newt/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.newt.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os.path
import tarfile

import requests
import tqdm
import tyro

images_url = (
    "https://ml-inat-competition-datasets.s3.amazonaws.com/newt/newt2021_images.tar.gz"
)
labels_url = "https://ml-inat-competition-datasets.s3.amazonaws.com/newt/newt2021_labels.csv.tar.gz"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""

    images: bool = True
    """Whether to download images [4.1GB]."""
    labels: bool = True
    """Whether to download labels."""


def main(args: Args):
    """Download NeWT."""
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    labels_tar_path = os.path.join(args.dir, "labels.tar")
    images_tar_path = os.path.join(args.dir, "images.tar")
    labels_csv_name = "newt2021_labels.csv"
    labels_csv_path = os.path.join(args.dir, labels_csv_name)
    images_dir_name = "newt2021_images"
    images_dir_path = os.path.join(args.dir, images_dir_name)

    if args.labels:
        # Download labels
        r = requests.get(labels_url, stream=True)
        r.raise_for_status()

        with open(labels_tar_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
        print(f"Downloaded labels: {labels_tar_path}.")

    if args.images:
        # Download images.
        r = requests.get(images_url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(images_tar_path, "wb") as fd:
            for chunk in tqdm.tqdm(
                r.iter_content(chunk_size=chunk_size),
                total=n_bytes / chunk_size,
                unit="b",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading images",
            ):
                fd.write(chunk)
        print(f"Downloaded images: {images_tar_path}.")

    with tarfile.open(labels_tar_path, "r") as tar:
        tar.extract(labels_csv_name, path=args.dir, filter="data")
    print(f"Extracted labels: {labels_csv_path}.")

    with open(labels_csv_path) as fd:
        n_images = len(fd.read().split("\n")) - 1

    with tarfile.open(images_tar_path, "r") as tar:
        for member in tqdm.tqdm(tar, desc="Extracting images", total=n_images):
            tar.extract(member, path=args.dir, filter="data")
    print(f"Extracted images: {images_dir_path}.")


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> openset.py
import logging

import beartype
import numpy as np
import sklearn.base
import sklearn.discriminant_analysis
import sklearn.utils.validation
from jaxtyping import Float, jaxtyped

from . import helpers

logger = logging.getLogger(__name__)


@beartype.beartype
class MahalanobisOpenSetClassifier(
    sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin
):
    """
    Wraps an arbitrary scikit-learn multiclass estimator with a Mahalanobis out-of-distribution detector.  Unknown samples are assigned `unknown_label`.

    @inproceedings{lee2018simple,
      title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
      author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
      year = 2018,
      booktitle = {Advances in Neural Information Processing Systems},
      publisher = {Curran Associates, Inc.},
      volume = 31,
      pages = {},
      url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf},
      editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
    }


    Parameters
    ----------
    base_estimator : scikit-learn estimator
        Must implement fit / predict (e.g. a Pipeline with a classifier).
    alpha : float, default=0.95
        Confidence level for the chi-squared cutoff; 1-alpha is the tail mass declared OOD.
    unknown_label : int | str, default=-1
        Label given to detections outside the known set.
    """

    def __init__(
        self, base_estimator, alpha: float = 0.95, unknown_label: int | str = -1
    ):
        self.base_estimator = base_estimator
        self.alpha = alpha
        self.unknown_label = unknown_label

    # ---------------- #
    # scikit-learn API #
    # ---------------- #
    def fit(self, X, y):
        X, y = sklearn.utils.validation.check_X_y(X, y, accept_sparse=False)
        self.classes_ = np.unique(y)

        self.clf_ = sklearn.base.clone(self.base_estimator).fit(X, y)
        logger.info("Fit base estimator.")

        self.lda_ = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(
            solver="eigen", shrinkage="auto", store_covariance=True
        )
        self.lda_.fit(X, y)
        logger.info("Fit LDA.")

        self.means_ = self.lda_.means_
        self.covariance_ = self.lda_.covariance_
        try:
            self.inv_covariance_ = np.linalg.inv(self.covariance_)
        except np.linalg.LinAlgError:
            self.inv_covariance_ = np.linalg.pinv(self.covariance_)
        logger.info("Inverted covariance matrix.")

        train_scores = min_mahalanobis_sq_batched(X, self.means_, self.inv_covariance_)
        self.tpr_ = np.percentile(train_scores, 95)

        logger.info("Fit.")
        return self

    def predict(self, X):
        scores = self.decision_function(X)

        pred_known = self.clf_.predict(X)
        pred = np.where(scores >= 0, pred_known, self.unknown_label)
        return pred

    def decision_function(self, X):
        sklearn.utils.validation.check_is_fitted(self, "clf_")
        X = sklearn.utils.validation.check_array(X, accept_sparse=False)

        d2 = min_mahalanobis_sq_batched(X, self.means_, self.inv_covariance_)
        return self.tpr_ - d2


@jaxtyped(typechecker=beartype.beartype)
def min_mahalanobis_sq(
    X: Float[np.ndarray, "n d"],
    mu: Float[np.ndarray, "classes d"],
    cov_inv: Float[np.ndarray, "d d"],
) -> Float[np.ndarray, " n"]:
    logger.info(
        "Calculating min_mahalanobis_sq. n=%d, d=%d, c=%d",
        X.shape[0],
        X.shape[1],
        mu.shape[0],
    )
    diff = X[:, None, :] - mu[None, :, :]
    logger.info("Got diff with shape %s", diff.shape)
    d2 = np.einsum("ncd,dd,ncd->nc", diff, cov_inv, diff)
    logger.info("Got d2 with shape %s", d2.shape)
    out = d2.min(axis=1)
    logger.info("Got out with shape %s", out.shape)
    return out.astype(np.float32)


@jaxtyped(typechecker=beartype.beartype)
def min_mahalanobis_sq_batched(
    X: Float[np.ndarray, "n d"],
    mu: Float[np.ndarray, "classes d"],
    cov_inv: Float[np.ndarray, "d d"],
    *,
    bsz: int = 256,
) -> Float[np.ndarray, " n"]:
    n, d = X.shape
    L = np.linalg.cholesky(cov_inv)
    logger.info("Calculated L.")

    Xw = X @ L  # (n, d)  whiten once
    muw = mu @ L  # (c, d)
    muw_norm_sq = np.square(muw).sum(axis=1).reshape(1, -1)  # (1, c)

    out = np.full(n, 0.0, dtype=np.float32)

    for start, end in helpers.progress(helpers.batched_idx(n, bsz), desc="mahalanobis"):
        Xwb = Xw[start:end]  # (bsz, d)
        Xwb_norm_sq = np.square(Xwb).sum(axis=1, keepdims=True)  # (bsz, 1)
        d_sq = Xwb_norm_sq + muw_norm_sq - 2.0 * Xwb @ muw.T  # (bsz,C)
        out[start:end] = d_sq.min(axis=1)

    return out.astype(np.float32)

>>>> plankton/__init__.py
"""
Classification of phytoplankton using ridge classifiers.
This task is particularly challenging because the image distribution is very different to typical pre-training datasets; it's all microscopic images in mono-channel (black and white).

If you use this task, please cite the original paper to propose this train/test split and the original datasets as well:

Paper:

```
@article{kaisa2022towards,
    author={Kraft, Kaisa  and Velhonoja, Otso  and Eerola, Tuomas  and Suikkanen, Sanna  and Tamminen, Timo  and Haraguchi, Lumi  and Ylöstalo, Pasi  and Kielosto, Sami  and Johansson, Milla  and Lensu, Lasse  and Kälviäinen, Heikki  and Haario, Heikki  and Seppälä, Jukka },
    title={Towards operational phytoplankton recognition with automated high-throughput imaging, near-real-time data processing, and convolutional neural networks},
    journal={Frontiers in Marine Science},
    volume={9},
    year={2022},
    url={https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2022.867695},
    doi={10.3389/fmars.2022.867695},
    issn={2296-7745},
}
```

Training data:

```
@misc{kaisa2022syke
    doi = {10.23728/B2SHARE.ABF913E5A6AD47E6BAA273AE0ED6617A},
    url = {https://b2share.eudat.eu/records/abf913e5a6ad47e6baa273ae0ed6617a},
    author = {Kraft, Kaisa and Velhonoja, Otso and Seppälä, Jukka and Hällfors, Heidi and Suikkanen, Sanna and Ylöstalo, Pasi and Anglès, Sílvia and Kielosto, Sami and Kuosa, Harri and Lehtinen, Sirpa and Oja, Johanna and Tamminen, Timo},
    keywords = {3.1.21 -> Biology -> Marine biology, phytoplankton image data set, imaging flow cytometry, Imaging FlowCytobot, IFCB, phytoplankton, Baltic Sea, image data, SYKE, Finnish Environment Institute, Marine Research Centre, Marine Ecological Research Laboratory, plankton image data, FINMARI},
    title = {SYKE-plankton_IFCB_2022},
    publisher = {https://b2share.eudat.eu},
    year = {2022},
    copyright = {open}
}
```

Evaluation data:

```
@misc{kaisa2021syke,
  doi = {10.23728/B2SHARE.7C273B6F409C47E98A868D6517BE3AE3},
  url = {https://b2share.eudat.eu/records/7c273b6f409c47e98a868d6517be3ae3},
  author = {Kraft, Kaisa and Haraguchi, Lumi and Velhonoja, Otso and Seppälä, Jukka},
  keywords = {3.1.21 -> Biology -> Marine biology, phytoplankton image data set, imaging flow cytometry, Imaging FlowCytobot, IFCB, Baltic Sea, image data, SYKE, Finnish Environment Institute, Marine Research Centre, Marine Ecological Research Laboratory, plankton image data, FINMARI, phytoplankton},
  title = {SYKE-plankton_IFCB_Utö_2021},
  publisher = {https://b2share.eudat.eu},
  year = {2022},
  copyright = {open}
}
```

This task was added because of interesting conversations with [Ekaterina Nepovinnykh](https://scholar.google.com/citations?user=lmYki4gAAAAJ) and [Heikki Kälviäinen](https://www.lut.fi/en/profiles/heikki-kalviainen).
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("plankton")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    y: Int[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using cross validation splits.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Get features
    train_features = get_features(cfg, backbone, is_train=True)
    val_features = get_features(cfg, backbone, is_train=False)

    torch.cuda.empty_cache()  # Be nice to others on the machine.

    # 2. Fit model.
    clf = helpers.init_logreg_clf(cfg)
    clf.fit(train_features.x, train_features.y)

    # 3. Predict.
    pred_labels = clf.predict(val_features.x)
    logger.info("Predicted classes for %d examples.", len(val_features.x))
    true_labels = val_features.y

    preds = [
        reporting.Prediction(
            str(image_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for image_id, pred, true in zip(val_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("plankton", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["plankton"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
class Sample(typing.TypedDict):
    """A dictionary representing a single image sample with its metadata.

    Attributes:
        img_id: Unique identifier for the image.
        img: The image tensor with shape [3, width, height] (RGB channels first).
        label: Binary class label (0 or 1) for the image.
    """

    img_id: str
    img: Float[Tensor, "3 width height"]
    label: Int[Tensor, ""]


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    transform: typing.Any | None
    """Optional function function that transforms an image into a format expected by a neural network."""
    samples: list[tuple[str, str, str]]
    """List of all image ids, image paths, and classnames."""

    def __init__(self, root: str, transform):
        self.transform = transform
        self.samples = []
        if not os.path.exists(root) or not os.path.isdir(root):
            msg = f"Path '{root}' doesn't exist. Did you download the plankton dataset? See the docstring at the top of this file for instructions."
            raise RuntimeError(msg)

        class_to_int = {}
        for dirname in sorted(os.listdir(root)):
            class_to_int[dirname] = len(class_to_int)

        for dirpath, dirnames, filenames in os.walk(root):
            img_class = os.path.relpath(dirpath, root)
            for filename in filenames:
                if not filename.endswith(".png"):
                    continue
                img_id = filename.removesuffix(".png")
                img_path = os.path.join(dirpath, filename)
                self.samples.append((img_id, img_path, class_to_int[img_class]))

    def __getitem__(self, i) -> Sample:
        img_id, img_path, label = self.samples[i]
        img = Image.open(img_path).convert("RGB")
        if self.transform is not None:
            img = self.transform(img)
        return {"img_id": img_id, "img": img, "label": label}

    def __len__(self) -> int:
        return len(self.samples)

    @property
    def labels(self) -> Int[np.ndarray, " n_samples"]:
        return np.array([label for _, _, label in self.samples])


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, is_train: bool
) -> Features:
    split = "train" if is_train else "val"
    images_dir_path = os.path.join(cfg.data.plankton, split)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    dataset = Dataset(images_dir_path, img_transform)

    if is_train and cfg.n_train > 0:
        i = helpers.balanced_random_sample(dataset.labels, cfg.n_train)
        assert len(i) == cfg.n_train
        dataset = torch.utils.data.Subset(dataset, i)

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    def probe(batch):
        imgs = batch["img"].to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            _ = backbone.img_encode(imgs).img_features  # forward only

    all_ids, all_features, all_labels = [], [], []

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"plk/{split}"):
            batch = next(it)
            imgs = batch["img"].to(cfg.device)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_ids.extend(batch["img_id"])

            all_labels.extend(batch["label"])

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)
    assert len(all_ids) == len(dataset)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)

>>>> plankton/download.py
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "requests",
#     "tqdm",
#     "tyro",
# ]
# ///
"""
A script to download the SYKE-plankton_IFCB_2022 dataset.

Run with:

1. `python biobench/plankton/download.py --help` if `biobench/` is in your $PWD.
2. `python -m biobench.plankton.download --help` if you have installed `biobench` as a package.
"""

import dataclasses
import os
import shutil
import zipfile

import requests
import tqdm
import tyro

train_url = "https://b2share.eudat.eu/api/files/63a79aff-4194-48c8-8055-0a73ecfcf183/phytoplankton_labeled.zip"
val_url = "https://b2share.eudat.eu/api/files/4a62bb1b-9bd0-4005-9217-7472ee6ed92c/phytoplankton_Ut%C3%B6_2021_labeled.zip"


@dataclasses.dataclass(frozen=True)
class Args:
    """Configure download options."""

    dir: str = "."
    """Where to save data."""

    chunk_size_kb: int = 1
    """How many KB to download at a time before writing to file."""


def main(args: Args):
    os.makedirs(args.dir, exist_ok=True)
    chunk_size = int(args.chunk_size_kb * 1024)
    train_zip = os.path.join(args.dir, "train.zip")
    val_zip = os.path.join(args.dir, "val.zip")

    for filepath, url in [(train_zip, train_url), (val_zip, val_url)]:
        r = requests.get(url, stream=True)
        r.raise_for_status()

        n_bytes = int(r.headers["content-length"])

        with open(filepath, "wb") as fd:
            # Need to specify a manual progress bar in order to get units and such working.
            t = tqdm.tqdm(
                total=n_bytes,
                unit="B",
                unit_scale=1,
                unit_divisor=1024,
                desc="Downloading images",
            )
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
            t.close()

        with zipfile.ZipFile(filepath, "r") as zip:
            for member in tqdm.tqdm(
                zip.infolist(), unit="img", desc="Extracting images"
            ):
                zip.extract(member, args.dir)

    # Move images to particular split-named folders.
    val_folder = "phytoplankton_Utö_2021_labeled"
    move(os.path.join(args.dir, val_folder), os.path.join(args.dir, "val"))
    train_folder = "labeled_20201020"
    move(os.path.join(args.dir, train_folder), os.path.join(args.dir, "train"))

    print(f"Downloaded, extracted and organized images in {args.dir}.")


def move(src: str, dst: str):
    """
    Moves _src_ to _dst_. If _dst_ exists, it will be overwritten.
    """
    if os.path.isdir(dst):
        shutil.rmtree(dst)
    os.rename(src, dst)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> plantnet/__init__.py
"""
Pl@ntNet is a "dataset with high label ambiguity and a long-tailed distribution" from NeurIPS 2021.
We fit a ridge classifier from scikit-learn to a backbone's embeddings and evaluate on the validation split.


There are two pieces that make Pl@ntNet more than a simple classification task:

1. Because of the long tail, we use `class_weight='balanced'` which adjusts weights based on class frequency.
2. We use macro F1 both to choose the alpha parameter and to evaluate the final classifier rather than accuracy due to the massive class imbalance.

If you use this task, please cite the original paper:

@inproceedings{plantnet-300k,
    author={Garcin, Camille and Joly, Alexis and Bonnet, Pierre and Lombardo, Jean-Christophe and Affouard, Antoine and Chouet, Mathias and Servajean, Maximilien and Lorieul, Titouan and Salmon, Joseph},
    booktitle={NeurIPS Datasets and Benchmarks 2021},
    title={{Pl@ntNet-300K}: a plant image dataset with high label ambiguity and a long-tailed distribution},
    year={2021},
}
"""

import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import polars as pl
import sklearn.experimental.enable_halving_search_cv
import sklearn.linear_model
import sklearn.model_selection
import sklearn.pipeline
import sklearn.preprocessing
import torch
from jaxtyping import Float, Shaped, jaxtyped
from PIL import Image
from torch import Tensor

from .. import config, helpers, registry, reporting

logger = logging.getLogger("plantnet")


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[np.ndarray, "n dim"]
    labels: Shaped[np.ndarray, " n"]
    ids: Shaped[np.ndarray, " n"]

    def y(self, encoder):
        return encoder.transform(self.labels.reshape(-1, 1)).reshape(-1)


@beartype.beartype
def benchmark(cfg: config.Experiment) -> reporting.Report:
    """
    Steps:
    1. Get features for all images.
    2. Select lambda using cross validation splits.
    3. Report score on test data.
    """
    backbone = registry.load_vision_backbone(cfg.model)

    # 1. Get features
    val_features = get_features(cfg, backbone, split="val")
    train_features = get_features(cfg, backbone, split="train")

    encoder = sklearn.preprocessing.OrdinalEncoder()
    all_labels = np.concatenate((val_features.labels, train_features.labels))
    encoder.fit(all_labels.reshape(-1, 1))

    # 2. Fit model.
    clf = init_clf(cfg)
    clf.fit(train_features.x, train_features.y(encoder))

    helpers.write_hparam_sweep_plot("plantnet", cfg.model.ckpt, clf)
    alpha = clf.best_params_["ridgeclassifier__alpha"].item()
    logger.info("alpha=%.2g scored %.3f.", alpha, clf.best_score_.item())

    true_labels = val_features.y(encoder)
    pred_labels = clf.predict(val_features.x)

    preds = [
        reporting.Prediction(
            str(img_id),
            float(pred == true),
            {"y_pred": pred.item(), "y_true": true.item()},
        )
        for img_id, pred, true in zip(val_features.ids, pred_labels, true_labels)
    ]

    return reporting.Report("plantnet", preds, cfg)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    assert df.get_column("task_name").unique().to_list() == ["plantnet"]
    return reporting.bootstrap_scores_macro_f1(df, b=b, rng=rng)


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    transform: typing.Any | None
    """Optional function function that transforms an image into a format expected by a neural network."""
    samples: list[tuple[str, str, str]]
    """List of all image ids, image paths, and classnames."""

    def __init__(self, root: str, transform):
        self.transform = transform
        self.samples = []
        if not os.path.exists(root) or not os.path.isdir(root):
            msg = f"Path '{root}' doesn't exist. Did you download the Pl@ntNet dataset? See the docstring at the top of this file for instructions. If you did download it, pass the path as --dataset-dir PATH"
            raise RuntimeError(msg)

        for dirpath, dirnames, filenames in os.walk(root):
            img_class = os.path.relpath(dirpath, root)
            for filename in filenames:
                img_id = filename.removesuffix(".jpg")
                img_path = os.path.join(dirpath, filename)
                self.samples.append((img_id, img_path, img_class))

    def __getitem__(self, i: int) -> tuple[str, Float[Tensor, "3 width height"], str]:
        img_id, img_path, img_class = self.samples[i]
        img = Image.open(img_path)
        if self.transform is not None:
            img = self.transform(img)
        return img_id, img, img_class

    def __len__(self) -> int:
        return len(self.samples)


@jaxtyped(typechecker=beartype.beartype)
@torch.no_grad()
def get_features(
    cfg: config.Experiment, backbone: registry.VisionBackbone, *, split: str
) -> Features:
    imgs_dir_path = os.path.join(cfg.data.plantnet, "images", split)

    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(cfg.device))

    dataset = Dataset(imgs_dir_path, img_transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        num_workers=cfg.n_workers,
        drop_last=False,
        shuffle=False,
        pin_memory=False,
        persistent_workers=False,
    )

    all_ids, all_features, all_labels = [], [], []

    def probe(batch):
        _, imgs, _ = batch
        imgs = imgs.to(cfg.device, non_blocking=True)
        with torch.amp.autocast(cfg.device):
            backbone.img_encode(imgs).img_features  # forward only

    with helpers.auto_batch_size(dataloader, probe=probe):
        total = len(dataloader) if not cfg.debug else 2
        it = iter(dataloader)
        for b in helpers.progress(range(total), every=10, desc=f"plnt/{split}"):
            ids, imgs, labels = next(it)
            imgs = imgs.to(cfg.device)

            with torch.amp.autocast(cfg.device):
                features = backbone.img_encode(imgs).img_features
                all_features.append(features.cpu())

            all_ids.extend(ids)
            all_labels.extend(labels)

    all_features = torch.cat(all_features, axis=0).cpu().numpy()
    all_labels = np.array(all_labels)
    all_ids = np.array(all_ids)

    assert len(all_ids) == len(dataset)
    if cfg.n_train >= 0:
        assert len(all_ids) == cfg.n_train
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@beartype.beartype
def init_clf(cfg: config.Experiment):
    alpha = np.pow(2.0, np.arange(-15, 11))
    if cfg.debug:
        alpha = np.pow(2.0, np.arange(-2, 2))

    return sklearn.model_selection.HalvingGridSearchCV(
        sklearn.pipeline.make_pipeline(
            sklearn.preprocessing.StandardScaler(),
            sklearn.linear_model.RidgeClassifier(1.0, class_weight="balanced"),
        ),
        {"ridgeclassifier__alpha": alpha},
        n_jobs=16,
        verbose=2,
        # This uses sklearn.metrics.f1_score with average="macro"
        scoring="f1_macro",
        factor=3,
    )

>>>> plantnet/download.py
import dataclasses
import os.path
import zipfile

import requests
import tqdm
import tyro

images_url = "https://zenodo.org/records/5645731/files/plantnet_300K.zip"


@dataclasses.dataclass(frozen=True)
class Args:
    dir: str = "."
    """where to save data."""
    chunk_size_kb: int = 1
    """how many KB to download at a time before writing to file."""
    download: bool = True
    """whether to download images [29.5GB]."""
    unzip: bool = True
    """whether to unzip images."""


def main(args: Args):
    os.makedirs(args.dir, exist_ok=True)

    chunk_size = int(args.chunk_size_kb * 1024)

    images_zip_path = os.path.join(args.dir, "plantnet_300K.zip")

    if args.download:
        # Download images.
        r = requests.get(images_url, stream=True)
        r.raise_for_status()

        t = tqdm.tqdm(
            total=int(r.headers["content-length"]),
            unit="B",
            unit_scale=1,
            unit_divisor=1024,
            desc="Downloading images",
        )
        with open(images_zip_path, "wb") as fd:
            for chunk in r.iter_content(chunk_size=chunk_size):
                fd.write(chunk)
                t.update(len(chunk))
        t.close()

        print(f"Downloaded images: {images_zip_path}.")

    if args.unzip:
        # Unzip images.
        zip_file = zipfile.ZipFile(images_zip_path)
        names = zip_file.namelist()
        for filename in tqdm.tqdm(names, desc="Unzipping images."):
            zip_file.extract(filename, path=args.dir)


if __name__ == "__main__":
    main(tyro.cli(Args))

>>>> rarespecies/__init__.py
import collections
import dataclasses
import logging
import math

import beartype
import datasets
import numpy as np
import sklearn.neighbors
import torch
from jaxtyping import Float, Int, Shaped, jaxtyped
from torch import Tensor

from biobench import config, helpers, registry, reporting

logger = logging.getLogger("rare-species")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    batch_size: int = 256
    """batch size for deep model."""
    n_workers: int = 4
    """number of dataloader worker processes."""
    log_every: int = 10
    """how often (number of batches) to log progress."""
    # Computed at runtime.
    device: str = "cuda"
    """(computed at runtime) which kind of accelerator to use."""
    debug: bool = False
    """(computed at runtime) whether to run in debug mode."""
    n_train: int = -1
    """(computed at runtime) number of maximum training samples. Negative number means use all of them."""


@beartype.beartype
def benchmark(cfg: config.Experiment) -> tuple[config.Model, reporting.Report]:
    backbone = registry.load_vision_backbone(cfg.model)
    features = get_features(cfg, backbone)

    train_i, test_i = make_split(features.y, k=1)

    scores = simpleshot(
        cfg,
        features.x[train_i],
        features.y[train_i],
        features.x[test_i],
        features.y[test_i],
    )
    examples = [
        reporting.Prediction(str(id), float(score), {})
        for id, score in zip(features.ids[test_i], scores.tolist())
    ]
    return cfg.model, reporting.Report("RareSpecies", examples)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Features:
    x: Float[Tensor, " n dim"]
    y: Int[Tensor, " n"]
    ids: Shaped[np.ndarray, " n"]


class Preprocess:
    def __init__(self, img_transform):
        self._img_transform = img_transform

    def __call__(self, example):
        example["image"] = example["image"].convert("RGB")
        example["image"] = self._img_transform(example["image"])
        example["label"] = "-".join(
            example[key]
            for key in [
                "kingdom",
                "phylum",
                "class",
                "order",
                "family",
                "genus",
                "species",
            ]
        )
        return example


@beartype.beartype
@torch.no_grad
def get_features(args: Args, backbone: registry.VisionBackbone) -> Features:
    img_transform = backbone.make_img_transform()
    backbone = torch.compile(backbone.to(args.device))

    dataset = (
        datasets.load_dataset("imageomics/rare-species", split="train")
        .to_iterable_dataset(num_shards=args.n_workers)
        .map(Preprocess(img_transform))
        .with_format("torch")
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=args.batch_size,
        num_workers=args.n_workers,
        drop_last=False,
        shuffle=False,  # We use dataset.shuffle instead
    )

    all_features, all_labels, all_ids = [], [], []

    total = math.ceil(11984 / args.batch_size) if not args.debug else 2
    it = iter(dataloader)
    logger.debug("Need to embed %d batches of %d images.", total, args.batch_size)
    for b in helpers.progress(range(total), every=args.log_every, desc="rarespecies"):
        batch = next(it)

        images = batch["image"].to(args.device)

        with torch.amp.autocast("cuda"):
            features = backbone.img_encode(images).img_features

        all_features.append(features.cpu())
        all_labels.extend(batch["label"])
        all_ids.extend(batch["rarespecies_id"])

    all_features = torch.cat(all_features, dim=0).cpu()
    all_ids = np.array(all_ids)
    all_labels = torch.tensor(all_labels)
    logger.info("Got features for %d images.", len(all_ids))

    return Features(all_features, all_labels, all_ids)


@jaxtyped(typechecker=beartype.beartype)
def simpleshot(
    args: Args,
    x_train: Float[Tensor, "n_train dim"],
    y_train: Int[Tensor, " n_train"],
    x_test: Float[Tensor, "n_test dim"],
    y_test: Int[Tensor, " n_test"],
) -> Float[Tensor, " n_test"]:
    """
    Applies simpleshot to the video clips. We assign each clip the majority label. Return the list of scores for x_test.
    """
    x_mean = x_train.mean(axis=0, keepdims=True)

    x_train = x_train - x_mean
    x_train = l2_normalize(x_train)

    x_test = x_test - x_mean
    x_test = l2_normalize(x_test)

    clf = sklearn.neighbors.NearestCentroid()
    clf.fit(x_train, y_train)

    # Do this next step on the GPU to make it fast.
    # Goes from 1 batch/sec to 77 batch/sec
    centroids = torch.from_numpy(clf.centroids_).to(args.device)
    x_test = x_test.to(args.device)
    y_test = y_test.to(args.device)

    scores = []
    for start, stop in batched_idx(len(x_test), args.batch_size):
        x_batch = x_test[start:stop]
        y_batch = y_test[start:stop]
        distances = torch.linalg.vector_norm(x_batch[:, None] - centroids, axis=2)
        preds = torch.argmin(distances, dim=1)

        scores.append((preds == y_batch).type(torch.float32))

    return torch.cat(scores, axis=0)


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[Tensor, "n_examples dim"],
) -> Float[Tensor, "n_examples dim"]:
    norms = torch.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@jaxtyped(typechecker=beartype.beartype)
def make_split(
    labels: Int[Tensor, " n_examples"], *, k: int
) -> tuple[Int[Tensor, " n_train"], Int[Tensor, " n_test"]]:
    classes = np.unique(labels)

    train_indices = np.array([], dtype=int)
    test_indices = np.array([], dtype=int)

    # Iterate through each class to select indices
    for cls in classes:
        # Indices corresponding to the current class
        cls_indices = np.where(labels == cls)[0]
        # Randomly shuffle the indices
        np.random.shuffle(cls_indices)
        # Select the first K indices for the train set
        cls_train_indices = cls_indices[:k]
        # The rest go into the test set
        cls_test_indices = cls_indices[k:]
        # Append the selected indices to the train/test arrays
        train_indices = np.concatenate((train_indices, cls_train_indices))
        test_indices = np.concatenate((test_indices, cls_test_indices))

    # Shuffle the indices to mix classes
    np.random.shuffle(train_indices)
    np.random.shuffle(test_indices)

    return torch.from_numpy(train_indices), torch.from_numpy(test_indices)

>>>> registry.py
"""
Stores all vision backbones.
Users can register new custom backbones from their code to evaluate on biobench using `register_vision_backbone`.
As long as it satisfies the `VisionBackbone` interface, it will work will all tasks.
"""

import dataclasses
import logging

import beartype
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import config

logger = logging.getLogger(__name__)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class EncodedImgBatch:
    """The output of a `VisionBackbone`'s `VisionBackbone.img_encode()` method."""

    img_features: Float[Tensor, "batch img_dim"]
    """Image-level features. Each image is represented by a single vector."""
    patch_features: Float[Tensor, "batch n_patches patch_dim"] | None
    """Patch-level features. Only ViTs have patch-level features. These features might be a different dimension that the image features because of projection heads or such."""


@jaxtyped(typechecker=beartype.beartype)
class VisionBackbone(torch.nn.Module):
    """
    A frozen vision model that embeds batches of images into batches of vectors.

    To add new models to the benchmark, you can simply create a new class that satisfies this interface and register it.
    See `biobench.registry` for a tutorial on adding new vision backbones.
    """

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> EncodedImgBatch:
        """Encode a batch of images."""
        err_msg = f"{self.__class__.__name__} must implemented img_encode()."
        raise NotImplementedError(err_msg)

    def make_img_transform(self):
        """
        Return whatever function the backbone wants for image preprocessing.
        This should be an evaluation transform, not a training transform, because we are using the output features of this backbone as data and not updating this backbone.
        """
        err_msg = f"{self.__class__.__name__} must implemented make_img_transform()."
        raise NotImplementedError(err_msg)


_global_backbone_registry: dict[str, type[VisionBackbone]] = {}


@beartype.beartype
def load_vision_backbone(model_cfg: config.Model) -> VisionBackbone:
    """
    Load a pretrained vision backbone.
    """
    if model_cfg.org not in _global_backbone_registry:
        raise ValueError(f"Org '{model_cfg.org}' not found.")

    cls = _global_backbone_registry[model_cfg.org]
    return cls(model_cfg.ckpt, drop_keys=model_cfg.drop_keys)


@beartype.beartype
def register_vision_backbone(model_org: str, cls: type[VisionBackbone]):
    """
    Register a new vision backbone class.
    """
    if model_org in _global_backbone_registry:
        logger.warning("Overwriting key '%s' in registry.", model_org)
    _global_backbone_registry[model_org] = cls


def list_vision_backbones() -> list[str]:
    """
    List all vision backbone model orgs.
    """
    return list(_global_backbone_registry.keys())

>>>> report.py
import collections.abc
import dataclasses
import functools
import importlib
import json
import logging
import os.path
import pathlib
import sqlite3
import subprocess
import time

import beartype
import numpy as np
import polars as pl
import statsmodels.stats.multitest
import tyro
import whenever

from biobench import helpers

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("report.py")


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Task:
    name: str
    display: str

    @functools.cached_property
    def bootstrap_scores_fn(self) -> collections.abc.Callable:
        mod = importlib.import_module(f"biobench.{self.name}")

        return mod.bootstrap_scores

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)


prior_work_tasks = [
    Task("imagenet1k", "ImageNet-1K"),
    Task("newt", "NeWT"),
]

benchmark_tasks = [
    Task("beluga", "Beluga"),
    Task("fishnet", "FishNet"),
    Task("fungiclef", "FungiCLEF"),
    Task("herbarium19", "Herbarium19"),
    Task("iwildcam", "iWildCam"),
    Task("kabr", "KABR"),
    Task("mammalnet", "MammalNet"),
    Task("plankton", "Plankton"),
    Task("plantnet", "Pl@ntNet"),
]

task_lookup = {task.name: task for task in prior_work_tasks + benchmark_tasks}


@beartype.beartype
def date_to_ms(year: int, month: int, day: int) -> int:
    return whenever.Instant.from_utc(year, month, day).timestamp_millis()


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Model:
    ckpt: str
    display: str
    family: str
    resolution: int
    params: int
    release_ms: int | None

    def to_dict(self) -> dict[str, object]:
        return dataclasses.asdict(self)


models = [
    Model(
        "ViT-B-32/openai",
        "CLIP ViT-B/32",
        "CLIP",
        224,
        87_849_216,
        date_to_ms(2021, 1, 5),
    ),
    Model(
        "ViT-B-16/openai",
        "CLIP ViT-B/16",
        "CLIP",
        224,
        86_192_640,
        date_to_ms(2021, 1, 5),
    ),
    Model(
        "ViT-L-14/openai",
        "CLIP ViT-L/14",
        "CLIP",
        224,
        303_966_208,
        date_to_ms(2021, 1, 5),
    ),
    Model(
        "ViT-L-14-336/openai",
        "CLIP ViT-L/14",
        "CLIP",
        336,
        304_293_888,
        date_to_ms(2021, 1, 5),
    ),
    Model(
        "ViT-B-16-SigLIP/webli",
        "SigLIP ViT-B/16",
        "SigLIP",
        224,
        92_884_224,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-B-16-SigLIP-256/webli",
        "SigLIP ViT-B/16",
        "SigLIP",
        256,
        92_930_304,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-B-16-SigLIP-384/webli",
        "SigLIP ViT-B/16",
        "SigLIP",
        384,
        93_176_064,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-B-16-SigLIP-512/webli",
        "SigLIP ViT-B/16",
        "SigLIP",
        512,
        93_520_128,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-L-16-SigLIP-256/webli",
        "SigLIP ViT-L/16",
        "SigLIP",
        256,
        315_956_224,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-L-16-SigLIP-384/webli",
        "SigLIP ViT-L/16",
        "SigLIP",
        384,
        316_283_904,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-SO400M-14-SigLIP/webli",
        "SigLIP SO400M/14",
        "SigLIP",
        224,
        427_680_704,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-SO400M-14-SigLIP-384/webli",
        "SigLIP SO400M/14",
        "SigLIP",
        384,
        428_225_600,
        date_to_ms(2023, 9, 27),
    ),
    Model(
        "ViT-B-32-SigLIP2-256/webli",
        "SigLIP2 ViT-B/32",
        "SigLIP2",
        256,
        94_552_320,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-B-16-SigLIP2/webli",
        "SigLIP2 ViT-B/16",
        "SigLIP2",
        224,
        92_884_224,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-B-16-SigLIP2-256/webli",
        "SigLIP2 ViT-B/16",
        "SigLIP2",
        256,
        92_930_304,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-B-16-SigLIP2-384/webli",
        "SigLIP2 ViT-B/16",
        "SigLIP2",
        384,
        93_176_064,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-B-16-SigLIP2-512/webli",
        "SigLIP2 ViT-B/16",
        "SigLIP2",
        512,
        93_520_128,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-L-16-SigLIP2-256/webli",
        "SigLIP2 ViT-L/16",
        "SigLIP2",
        256,
        315_956_224,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-L-16-SigLIP2-384/webli",
        "SigLIP2 ViT-L/16",
        "SigLIP2",
        384,
        316_283_904,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-L-16-SigLIP2-512/webli",
        "SigLIP2 ViT-L/16",
        "SigLIP2",
        512,
        316_742_656,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-SO400M-16-SigLIP2-256/webli",
        "SigLIP2 SO400M/16",
        "SigLIP2",
        256,
        427_888_064,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-SO400M-16-SigLIP2-384/webli",
        "SigLIP2 SO400M/16",
        "SigLIP2",
        384,
        428_256_704,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-SO400M-16-SigLIP2-512/webli",
        "SigLIP2 SO400M/16",
        "SigLIP2",
        512,
        428_772_800,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-gopt-16-SigLIP2-256/webli",
        "SigLIP2 ViT-1B/16",
        "SigLIP2",
        256,
        1_163_168_256,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "ViT-gopt-16-SigLIP2-384/webli",
        "SigLIP2 ViT-1B/16",
        "SigLIP2",
        384,
        1_163_659_776,
        date_to_ms(2025, 2, 20),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-tiny-patch16-224",
        "OpenVision ViT-T/16",
        "OpenVision",
        224,
        5_561_088,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-small-patch16-224",
        "OpenVision ViT-S/16",
        "OpenVision",
        224,
        21_812_736,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-base-patch16-224",
        "OpenVision ViT-B/16",
        "OpenVision",
        224,
        86_191_104,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-large-patch14-224",
        "OpenVision ViT-L/14",
        "OpenVision",
        224,
        303_964_160,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-so400m-patch14-224",
        "OpenVision SO400M/14",
        "OpenVision",
        224,
        413_770_608,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-so400m-patch14-384",
        "OpenVision SO400M/14",
        "OpenVision",
        384,
        414_315_504,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "hf-hub:UCSC-VLAA/openvision-vit-huge-patch14-224",
        "OpenVision ViT-H/14",
        "OpenVision",
        224,
        632_074_240,
        date_to_ms(2025, 5, 7),
    ),
    Model(
        "dinov2_vits14_reg",
        "DINOv2 ViT-S/14",
        "DINOv2",
        224,
        22_058_112,
        date_to_ms(2024, 4, 12),
    ),
    Model(
        "dinov2_vitb14_reg",
        "DINOv2 ViT-B/14",
        "DINOv2",
        224,
        86_583_552,
        date_to_ms(2024, 4, 12),
    ),
    Model(
        "dinov2_vitl14_reg",
        "DINOv2 ViT-L/14",
        "DINOv2",
        224,
        304_372_736,
        date_to_ms(2024, 4, 12),
    ),
    Model(
        "dinov2_vitg14_reg",
        "DINOv2 ViT-g/14",
        "DINOv2",
        224,
        1_136_486_912,
        date_to_ms(2024, 4, 12),
    ),
    Model(
        "apple/aimv2-large-patch14-224",
        "AIMv2 ViT-L/14",
        "AIMv2",
        224,
        309_197_824,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-large-patch14-336",
        "AIMv2 ViT-L/14",
        "AIMv2",
        336,
        309_525_504,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-large-patch14-448",
        "AIMv2 ViT-L/14",
        "AIMv2",
        448,
        309_984_256,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-huge-patch14-224",
        "AIMv2 ViT-H/14",
        "AIMv2",
        224,
        680_851_968,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-huge-patch14-336",
        "AIMv2 ViT-H/14",
        "AIMv2",
        336,
        681_343_488,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-huge-patch14-448",
        "AIMv2 ViT-H/14",
        "AIMv2",
        448,
        682_031_616,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-1B-patch14-224",
        "AIMv2 ViT-1B/14",
        "AIMv2",
        224,
        1_234_958_336,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-1B-patch14-336",
        "AIMv2 ViT-1B/14",
        "AIMv2",
        336,
        1_235_613_696,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-1B-patch14-448",
        "AIMv2 ViT-1B/14",
        "AIMv2",
        448,
        1_236_531_200,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-3B-patch14-224",
        "AIMv2 ViT-3B/14",
        "AIMv2",
        224,
        2_720_658_432,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-3B-patch14-336",
        "AIMv2 ViT-3B/14",
        "AIMv2",
        336,
        2_721_641_472,
        date_to_ms(2024, 11, 21),
    ),
    Model(
        "apple/aimv2-3B-patch14-448",
        "AIMv2 ViT-3B/14",
        "AIMv2",
        448,
        2_723_017_728,
        date_to_ms(2024, 11, 21),
    ),
    Model("efficientnet_b0.ra_in1k", "EfficientNet B0", "CNN", 224, 5_288_548, None),
    Model("efficientnet_b3.ra2_in1k", "EfficientNet B3", "CNN", 320, 12_233_232, None),
    Model("mobilenetv2_100.ra_in1k", "MobileNet V2", "CNN", 224, 3_504_872, None),
    Model(
        "mobilenetv3_large_100.ra_in1k", "MobileNet V3 L", "CNN", 224, 5_483_032, None
    ),
    Model("resnet18.a1_in1k", "ResNet-18", "CNN", 288, 11_689_512, None),
    Model("resnet18d.ra2_in1k", "ResNet-18", "CNN", 288, 25_576_264, None),
    Model("resnet50.a1_in1k", "ResNet-50", "CNN", 288, 25_557_032, None),
    Model("resnet50d.a1_in1k", "ResNet-50d", "CNN", 288, 25_576_264, None),
    Model("convnext_tiny.in12k", "ConvNext-T", "CNN", 224, 36_910_477, None),
    Model(
        "convnext_tiny.in12k_ft_in1k", "ConvNext-T (IN1K)", "CNN", 288, 28_589_128, None
    ),
    Model(
        "vit_base_patch16_224.augreg2_in21k_ft_in1k",
        "ViT-B/16 (IN1K)",
        "ViT",
        224,
        86_567_656,
        None,
    ),
    Model(
        "hf-hub:imageomics/bioclip",
        "BioCLIP ViT-B/16",
        "cv4ecology",
        224,
        86_192_640,
        date_to_ms(2024, 5, 14),
    ),
    Model(
        "hf-hub:imageomics/bioclip-2",
        "BioCLIP-2 ViT-L/14",
        "cv4ecology",
        224,
        303_966_208,
        date_to_ms(2025, 5, 29),
    ),
    Model(
        "hf-hub:BGLab/BioTrove-CLIP",
        "BioTrove ViT-B/16",
        "cv4ecology",
        224,
        86_192_640,
        date_to_ms(2025, 1, 27),
    ),
    Model(
        "hf-hub:BVRA/MegaDescriptor-L-384",
        "MegaDescriptor Swin-L/4",
        "cv4ecology",
        384,
        195_198_516,
        date_to_ms(2023, 12, 14),
    ),
    Model(
        "vitl16", "V-JEPA ViT-L/16", "V-JEPA", 224, 305_490_944, date_to_ms(2024, 2, 15)
    ),
    Model(
        "vith16", "V-JEPA ViT-H/16", "V-JEPA", 224, 633_655_040, date_to_ms(2024, 2, 15)
    ),
    Model(
        "sam2_hiera_tiny.fb_r896_2pt1",
        "SAM2 Hiera-T",
        "SAM2",
        896,
        26_851_008,
        date_to_ms(2024, 10, 28),
    ),
    Model(
        "sam2_hiera_small.fb_r896_2pt1",
        "SAM2 Hiera-S",
        "SAM2",
        896,
        33_948_864,
        date_to_ms(2024, 10, 28),
    ),
    Model(
        "sam2_hiera_base_plus.fb_r896_2pt1",
        "SAM2 Hiera-B+",
        "SAM2",
        896,
        68_677_504,
        date_to_ms(2024, 10, 28),
    ),
    Model(
        "sam2_hiera_large.fb_r1024_2pt1",
        "SAM2 Hiera-L",
        "SAM2",
        1024,
        212_151_600,
        date_to_ms(2024, 10, 28),
    ),
]


model_lookup = {model.ckpt: model for model in models}


@beartype.beartype
def get_git_hash() -> str:
    """Returns the hash of the current git commit.

    Returns:
        str: The hash of the current git commit, assuming we are in a git repo
    """
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("ascii").strip()


@beartype.beartype
def calc_scores(
    df: pl.DataFrame, *, n_bootstraps: int, alpha: float, seed: int
) -> tuple[pl.DataFrame, pl.DataFrame]:
    rng = np.random.default_rng(seed=seed)
    scores_rows, bests_rows = [], []
    for task in helpers.progress(
        prior_work_tasks + benchmark_tasks, every=1, desc="bootstraps"
    ):
        try:
            bootstrap_scores = task.bootstrap_scores_fn
        except AttributeError:
            logger.warning("No `bootstrap_scores` for %s", task.name)
            continue

        logger.info("Getting score for %s", task.name)

        sub = df.filter(
            (pl.col("task_name") == task.name)
            & (pl.col("model_ckpt").is_in(model_lookup))
        )

        if sub.height == 0:
            continue

        scores = bootstrap_scores(sub, b=n_bootstraps, rng=rng)
        # freeze model order once
        ckpts = sorted(scores)  # list[str]  length = m

        # stack into a (b, m) matrix that matches that order
        s = np.column_stack([scores[c] for c in ckpts])  # (b, m)

        # locate the empirical best (column index j*)
        best_j = s.mean(axis=0).argmax()
        best_c = ckpts[best_j]

        # one-sided p-values against the best
        p_raw = (s >= s[:, [best_j]]).mean(axis=0)  # vector length m
        rej, p_adj, *_ = statsmodels.stats.multitest.multipletests(
            p_raw, alpha=alpha, method="holm"
        )

        # list checkpoints that are *not rejected* -> bold
        ties = [c for c, r in zip(ckpts, rej) if not r]
        bests_rows.append({"task": task.name, "best": best_c, "ties": ties})

        # Calculate reference scores, then compute task-level results.
        for model_ckpt, ref_score in bootstrap_scores(sub).items():
            bootstrap_mean = scores[model_ckpt].mean()
            low = (0.5 - (1 - alpha) / 2) * 100
            high = (0.5 + (1 - alpha) / 2) * 100
            ci_low, ci_high = np.percentile(scores[model_ckpt], (low, high))

            scores_rows.append({
                "task": task.name,
                "model": model_ckpt,
                "mean": ref_score.item(),
                "bootstrap_mean": bootstrap_mean.item(),
                "ci_low": ci_low.item(),
                "ci_high": ci_high.item(),
            })

    return pl.DataFrame(scores_rows), pl.DataFrame(bests_rows)


@beartype.beartype
def main(
    db: pathlib.Path = pathlib.Path(
        os.path.expandvars("/local/scratch/$USER/experiments/biobench/reports.sqlite")
    ),
    out: pathlib.Path = pathlib.Path("docs/data/results.json"),
    seed: int = 17,
    alpha=0.05,
    n_bootstraps: int = 500,
):
    """Generate a JSON report of benchmark results with bootstrap confidence intervals.

    This function reads experiment results from a SQLite database, calculates bootstrap statistics for each task/model combination, and writes the results to a JSON file for visualization.

    Args:
        db: Path to the SQLite database containing experiment results.
        out: Path where the JSON report will be written.
        seed: Random seed for reproducible bootstrapping.
        alpha: Significance level for confidence intervals and hypothesis tests.
        n_bootstraps: Number of bootstrap samples to generate.
    """

    stmt = "SELECT experiments.task_name, experiments.model_ckpt, predictions.score, predictions.img_id, predictions.info FROM experiments JOIN predictions ON experiments.id = predictions.experiment_id WHERE n_train = -1"
    df = (
        pl.read_database(stmt, sqlite3.connect(db), infer_schema_length=100_000)
        .lazy()
        .filter(pl.col("task_name").is_in(task_lookup))
        .with_columns(
            pl.coalesce([
                pl.col("info").str.json_path_match("$.y_true"),
                pl.col("info").str.json_path_match("$.true_y"),
            ]).alias("y_true"),
            pl.coalesce([
                pl.col("info").str.json_path_match("$.y_pred"),
                pl.col("info").str.json_path_match("$.pred_y"),
            ]).alias("y_pred"),
        )
        .select("task_name", "model_ckpt", "img_id", "score", "y_true", "y_pred")
        .collect()
    )
    logger.info("Loaded %d predictions.", df.height)

    # Print any unknown checkpoints
    unknown_ckpts = set(df["model_ckpt"].unique()) - set(model_lookup.keys())
    if unknown_ckpts:
        logger.warning(
            "Found %d unknown checkpoints: %s",
            len(unknown_ckpts),
            sorted(unknown_ckpts),
        )

    scores_df, bests_df = calc_scores(
        df, n_bootstraps=n_bootstraps, alpha=alpha, seed=seed
    )

    data = {
        "meta": {
            "schema": 1,
            "generated": int(time.time() * 1000),
            "git_commit": get_git_hash(),
            "seed": seed,
            "alpha": alpha,
            "n_bootstraps": n_bootstraps,
        },
        "models": [model.to_dict() for model in models],
        "benchmark_tasks": [task.to_dict() for task in benchmark_tasks],
        "prior_work_tasks": [task.to_dict() for task in prior_work_tasks],
        "results": scores_df.to_dicts(),
        "bests": bests_df.to_dicts(),
    }
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "w") as fd:
        json.dump(data, fd, indent=4)


if __name__ == "__main__":
    tyro.cli(main)

>>>> reporting.py
import dataclasses
import json
import logging
import os.path
import pathlib
import socket
import sqlite3
import subprocess
import sys
import time

import beartype
import numpy as np
import polars as pl
import sklearn.metrics
from jaxtyping import Float, Int, jaxtyped

from . import config, helpers

logger = logging.getLogger(__name__)

schema_fpath = pathlib.Path(__file__).parent / "schema.sql"


@beartype.beartype
def get_db(cfg: config.Experiment) -> sqlite3.Connection:
    """Get a connection to the reports database.

    Args:
        cfg: Experiment configuration

    Returns:
        sqlite3.Connection: A connection to the SQLite database
    """
    os.makedirs(os.path.expandvars(cfg.report_to), exist_ok=True)
    helpers.warn_if_nfs(cfg.report_to)
    db_fpath = os.path.join(os.path.expandvars(cfg.report_to), "reports.sqlite")
    db = sqlite3.connect(db_fpath, autocommit=True)

    with open(schema_fpath) as fd:
        schema = fd.read()
    db.executescript(schema)
    db.autocommit = False

    return db


@beartype.beartype
def already_ran(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> bool:
    """Check if an experiment has already been run.

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to check

    Returns:
        bool: True if the experiment has already been run, False otherwise
    """
    query = """
    SELECT COUNT(*)
    FROM experiments
    WHERE task_name = ?
    AND model_org = ?
    AND model_ckpt = ?
    AND n_train = ?
    """
    values = (task_name, cfg.model.org, cfg.model.ckpt, cfg.n_train)

    (count,) = db.execute(query, values).fetchone()
    return count > 0


@beartype.beartype
def is_claimed(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> bool:
    """Check if a run is already claimed by another process.

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to check

    Returns:
        bool: True if the run is already claimed, False otherwise
    """
    query = """
    SELECT COUNT(*)
    FROM runs
    WHERE task_name = ?
    AND model_org = ?
    AND model_ckpt = ?
    AND n_train = ?
    """
    values = (task_name, cfg.model.org, cfg.model.ckpt, cfg.n_train)

    (count,) = db.execute(query, values).fetchone()
    return count > 0


@beartype.beartype
def claim_run(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> bool:
    """Try to claim (task_name, model, n_train).

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to claim

    Returns:
        bool: True if this process inserted the row and now "owns" the run,
              False if row already existed and another worker has it
    """

    stmt = """
    INSERT OR IGNORE INTO runs
    (task_name, model_org, model_ckpt, n_train, pid, posix)
    VALUES (?,?,?,?,?,?)
    """
    values = (
        task_name,
        cfg.model.org,
        cfg.model.ckpt,
        cfg.n_train,
        os.getpid(),
        time.time(),
    )

    try:
        cur = db.execute(stmt, values)
        db.commit()
        claimed = cur.rowcount == 1  # 1 row inserted -> we won
        if claimed:
            logger.info(
                "Claimed (%s, %s, %s, %d).",
                task_name,
                cfg.model.org,
                cfg.model.ckpt,
                cfg.n_train,
            )
        return claimed
    except Exception:
        db.rollback()
        raise


@beartype.beartype
def release_run(db: sqlite3.Connection, cfg: config.Experiment, task_name: str) -> None:
    """Delete the coordination row so others may claim again.

    Args:
        db: SQLite database connection
        cfg: Experiment configuration
        task_name: Name of the task to release
    """
    stmt = """
    DELETE FROM runs
    WHERE task_name=? AND model_org=? AND model_ckpt=? AND n_train=?
    """
    values = (task_name, cfg.model.org, cfg.model.ckpt, cfg.n_train)
    logger.info("Releasing claim on (%s, %s, %s, %d).", *values)

    try:
        db.execute(stmt, values)
        db.commit()
        logger.info("Released claim on (%s, %s, %s, %d).", *values)
    except Exception:
        db.rollback()
        raise


@beartype.beartype
def clear_stale_claims(db: sqlite3.Connection, *, max_age_hours: int = 72) -> int:
    """
    Delete rows in `runs` whose POSIX timestamp is older than `max_age_hours`.

    Returns
    -------
    int
        Number of rows deleted.
    """
    if max_age_hours <= 0:
        raise ValueError("max_age_hours must be positive")

    cutoff = time.time() - max_age_hours * 3600
    try:
        cur = db.execute("DELETE FROM runs WHERE posix < ?", (cutoff,))
        db.commit()
        return cur.rowcount
    except Exception:
        db.rollback()
        raise


@beartype.beartype
def get_git_hash() -> str:
    """Returns the hash of the current git commit.

    Returns:
        str: The hash of the current git commit, assuming we are in a git repo
    """
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("ascii").strip()


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Prediction:
    """An individual test prediction."""

    id: str
    """Whatever kind of ID; used to find the original image/example."""
    score: float
    """Test score; typically 0 or 1 for classification tasks."""
    info: dict[str, object]
    """Any additional information included. This might be the original class, the true label, etc."""


def get_gpu_name() -> str:
    import torch

    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).name
    else:
        return ""


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class Report:
    """
    The result of running a benchmark task.
    """

    # Actual details of the report
    task_name: str
    """The benchmark name."""
    predictions: list[Prediction]
    """A list of (example_id, score, info) objects"""
    cfg: config.Experiment
    """Experimental config."""
    _: dataclasses.KW_ONLY
    splits: dict[str, float] = dataclasses.field(default_factory=dict)
    """Other scores that you would like to report. These do not have confidence intervals."""

    # Stuff for trying to reproduce this result. These are filled in by default.
    argv: list[str] = dataclasses.field(default_factory=lambda: sys.argv)
    """Command used to get this report."""
    git_commit: str = get_git_hash()
    """Git commit for this current report."""
    posix: float = dataclasses.field(default_factory=time.time)
    """Time when this report was constructed."""
    gpu_name: str = dataclasses.field(default_factory=get_gpu_name)
    """Name of the GPU that ran this experiment."""
    hostname: str = dataclasses.field(default_factory=socket.gethostname)
    """Machine hostname that ran this experiment."""

    def __repr__(self):
        return f"Report({self.task_name} with {len(self.predictions)} predictions)"

    def __str__(self):
        return repr(self)

    @beartype.beartype
    def write(self) -> None:
        """Saves the report to disk in a machine-readable SQLite format."""
        db = get_db(self.cfg)

        preds_stmt = "INSERT INTO predictions(img_id, score, info, experiment_id) VALUES(?, ?, ?, ?)"
        exp_stmt = "INSERT INTO experiments(task_name, model_org, model_ckpt, n_train, exp_cfg, argv, git_commit, posix, gpu_name, hostname) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        try:
            cursor = db.cursor()

            exp_values = (
                self.task_name.lower(),
                self.cfg.model.org,
                self.cfg.model.ckpt,
                self.cfg.n_train,
                json.dumps(self.cfg.to_dict()),
                json.dumps(self.argv),
                self.git_commit,
                self.posix,
                self.gpu_name,
                self.hostname,
            )
            cursor.execute(exp_stmt, exp_values)
            exp_id = cursor.lastrowid
            preds_values = [
                (pred.id, pred.score, json.dumps(pred.info), exp_id)
                for pred in self.predictions
            ]
            cursor.executemany(preds_stmt, preds_values)

            # Commit the transaction if all statements succeed
            db.commit()
        except sqlite3.Error as err:
            # Roll back the transaction in case of error
            db.rollback()
            logger.critical("Error writing report for '%s': %s", self.task_name, err)
            raise


@beartype.beartype
def micro_acc(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.accuracy_score(y_true, y_pred)


@beartype.beartype
def macro_acc(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.balanced_accuracy_score(y_true, y_pred)


@beartype.beartype
def micro_f1(preds: list[Prediction]) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])
    return sklearn.metrics.f1_score(y_true, y_pred, average="micro")


@beartype.beartype
def macro_f1(preds: list[Prediction], *, labels: list[int] | None = None) -> float:
    y_pred = np.array([
        next(p.info[key] for key in ("y_pred", "pred_y") if key in p.info)
        for p in preds
    ])
    y_true = np.array([p.info.get("y_true", p.info.get("true_y")) for p in preds])

    if labels is None:
        labels = np.unique(np.stack([y_true, y_pred]))
        labels = np.arange(labels.max() + 1)
    else:
        labels = np.array(labels)

    assert (np.arange(labels.size) == labels).all()

    return sklearn.metrics.f1_score(
        y_true, y_pred, average="macro", labels=labels, zero_division=0.0
    )


@beartype.beartype
def micro_acc_batch(
    y_true: Int[np.ndarray, "*batch n"], y_pred: Int[np.ndarray, "*batch n"]
) -> Float[np.ndarray, "*batch"]:
    """
    Vectorised **micro-accuracy** (overall proportion of correct predictions).

    * Works on any leading `*batch` prefix; the final axis `n` is the number of examples.
    * Complexity O(B·n) time, O(1) extra memory.

    Parameters
    ----------
    y_true, y_pred
        Integer class labels / predictions >= 0.  All leading dimensions
        (`*batch`) must match; `n` is the sample count.

    Returns
    -------
    acc : np.ndarray
        Shape `*batch`; accuracy for every element of the batch prefix.
    """
    acc = (y_true == y_pred).mean(axis=-1, dtype=float)
    return acc


@jaxtyped(typechecker=beartype.beartype)
def macro_f1_batch(
    y_true: Int[np.ndarray, "*batch n"],
    y_pred: Int[np.ndarray, "*batch n"],
    *,
    labels: Int[np.ndarray, " c"] | None = None,
) -> Float[np.ndarray, "*batch"]:
    """
    Vectorised macro-F1 for large class counts.

    Accepts any leading `*batch` prefix; the last axis `n` is the number
    of examples.  Runs in O(B·n) time and O(B·C) memory, where
    B = prod(*batch) and C = #classes.

    All elements in y_true, y_pred, and labels must be integers >= 0. (Negative IDs would break the offset arithmetic; floats break np.bincount.)
    """

    # flatten batch prefix
    *batch_shape, n = y_true.shape
    b = int(np.prod(batch_shape))  # total batches

    y_true = y_true.reshape(b, n)
    y_pred = y_pred.reshape(b, n)

    # label remapping to dense 0...C-1
    if labels is None:
        labels = np.unique(np.stack([y_true, y_pred]))

    labels = np.arange(labels.max() + 1)
    c = labels.size
    assert (np.arange(c) == labels).all()

    # offsets for per-batch bincounts
    offset = np.arange(b, dtype=np.int64)[:, None] * c  # (b, 1)
    yz_true = y_true + offset  # (b, n)
    yz_pred = y_pred + offset  # (b, n)

    # counts for all examples
    true_cnt = np.bincount(yz_true.ravel(), minlength=(b * c)).reshape(b, c)
    pred_cnt = np.bincount(yz_pred.ravel(), minlength=(b * c)).reshape(b, c)

    # true positives
    tp_mask = y_true == y_pred  # (B, n)
    tp_off = yz_true[tp_mask]  # 1-D
    tp_cnt = np.bincount(tp_off, minlength=(b * c)).reshape(b, c)

    # F1 per class, then macro average
    fp = pred_cnt - tp_cnt
    fn = true_cnt - tp_cnt
    denom = 2 * tp_cnt + fp + fn  # (B, C)

    f1_c = np.zeros_like(denom, dtype=float)
    np.divide(2 * tp_cnt, denom, out=f1_c, where=denom != 0)

    macro_f1 = f1_c.mean(axis=-1)  # (B,)
    return macro_f1.reshape(batch_shape)


@jaxtyped(typechecker=beartype.beartype)
def bootstrap_scores_macro_f1(
    df: pl.DataFrame, *, b: int = 0, rng: np.random.Generator | None = None
) -> dict[str, Float[np.ndarray, " b"]]:
    """
    Polars dataframe with schema

    Schema({'task_name': String, 'model_ckpt': String, 'img_id': String, 'score': Float64, 'y_true': String, 'y_pred': String})
    """

    n, *rest = df.group_by("model_ckpt").agg(n=pl.len()).get_column("n").to_list()
    if not all(n == i for i in rest):
        breakpoint()

    if b > 0:
        assert rng is not None, "must provide rng argument"
        i_bs = rng.integers(0, n, size=(b, n), dtype=np.int32)

    scores = {}

    y_pred_buf = np.empty((b, n), dtype=np.int32)
    y_true_buf = np.empty((b, n), dtype=np.int32)

    for model_ckpt in helpers.progress(
        df.get_column("model_ckpt").unique().sort().to_list(),
        desc="bootstrap/macro-f1",
        every=3,
    ):
        # pull y_true and y_pred for *one* model
        y_pred = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_pred")
            .unique()
            .sort("img_id")
            .get_column("y_pred")
            .cast(pl.Float32)
            .cast(pl.Int32)
            .to_numpy()
        )

        if len(y_pred) == 0:
            continue

        y_true = (
            df.filter(pl.col("model_ckpt") == model_ckpt)
            .select("img_id", "y_true")
            .unique()
            .sort("img_id")
            .get_column("y_true")
            .cast(pl.Float32)
            .cast(pl.Int32)
            .to_numpy()
        )
        assert y_true.size == y_pred.size

        if b > 0:
            # bootstrap resample into pre-allocated buffers
            np.take(y_pred, i_bs, axis=0, out=y_pred_buf)
            np.take(y_true, i_bs, axis=0, out=y_true_buf)
            scores[model_ckpt] = macro_f1_batch(y_true_buf, y_pred_buf)
        else:
            scores[model_ckpt] = np.array([macro_f1_batch(y_true, y_pred)])

    return scores


##########
# COLORS #
##########


# https://coolors.co/palette/001219-005f73-0a9396-94d2bd-e9d8a6-ee9b00-ca6702-bb3e03-ae2012-9b2226

BLACK_HEX = "001219"
BLACK_RGB = (0, 18, 25)
BLACK_RGB01 = tuple(c / 256 for c in BLACK_RGB)

BLUE_HEX = "005f73"
BLUE_RGB = (0, 95, 115)
BLUE_RGB01 = tuple(c / 256 for c in BLUE_RGB)

CYAN_HEX = "0a9396"
CYAN_RGB = (10, 147, 150)
CYAN_RGB01 = tuple(c / 256 for c in CYAN_RGB)

SEA_HEX = "94d2bd"
SEA_RGB = (148, 210, 189)
SEA_RGB01 = tuple(c / 256 for c in SEA_RGB)

CREAM_HEX = "e9d8a6"
CREAM_RGB = (233, 216, 166)
CREAM_RGB01 = tuple(c / 256 for c in CREAM_RGB)

GOLD_HEX = "ee9b00"
GOLD_RGB = (238, 155, 0)
GOLD_RGB01 = tuple(c / 256 for c in GOLD_RGB)

ORANGE_HEX = "ca6702"
ORANGE_RGB = (202, 103, 2)
ORANGE_RGB01 = tuple(c / 256 for c in ORANGE_RGB)

RUST_HEX = "bb3e03"
RUST_RGB = (187, 62, 3)
RUST_RGB01 = tuple(c / 256 for c in RUST_RGB)

SCARLET_HEX = "ae2012"
SCARLET_RGB = (174, 32, 18)
SCARLET_RGB01 = tuple(c / 256 for c in SCARLET_RGB)

RED_HEX = "9b2226"
RED_RGB = (155, 34, 38)
RED_RGB01 = tuple(c / 256 for c in RED_RGB)

>>>> schema.sql
PRAGMA journal_mode = WAL;    -- Concurrent reads/writes
PRAGMA synchronous = NORMAL;  -- Good balance speed/safety
PRAGMA foreign_keys = ON;     -- Enforce FK constraints
PRAGMA busy_timeout = 30000;  -- Wait up to 30s before throwing timeout errors
PRAGMA strict = ON;           -- Enforce strict type checking (SQLite ≥ 3.37)
PRAGMA encoding = 'UTF-8';    -- Consistent text encoding


CREATE TABLE IF NOT EXISTS experiments (
    id INTEGER PRIMARY KEY AUTOINCREMENT,

    -- Task information
    task_name TEXT NOT NULL,

    -- Model information
    model_org TEXT NOT NULL,
    model_ckpt TEXT NOT NULL,

    -- Number of requested training samples.
    n_train INTEGER NOT NULL,

    exp_cfg TEXT NOT NULL,  -- JSON blob with full experiment configuration

    -- Metadata fields
    argv TEXT NOT NULL,  -- Command used to get this report (JSON array)
    git_commit TEXT NOT NULL,  -- Git commit hash
    posix INTEGER NOT NULL,  -- POSIX timestamp
    gpu_name TEXT,  -- Name of the GPU that ran this experiment
    hostname TEXT NOT NULL  -- Machine hostname that ran this experiment
);

CREATE TABLE IF NOT EXISTS predictions (
    img_id TEXT NOT NULL,  -- ID used to find the original image/example
    score REAL NOT NULL,  -- Test score; typically 0 or 1 for classification tasks
    info TEXT NOT NULL,  -- JSON blob with additional information (original class, true label, etc.)

    -- Foreign key to link to the experiments table
    experiment_id INTEGER NOT NULL,

    PRIMARY KEY (img_id, experiment_id),
    FOREIGN KEY (experiment_id) REFERENCES experiments(id)  ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS runs (
    task_name TEXT NOT NULL,
    model_org TEXT NOT NULL,
    model_ckpt TEXT NOT NULL,
    n_train INTEGER NOT NULL,

    -- For debugging
    posix INTEGER NOT NULL,  -- POSIX timestamp
    pid INTEGER NOT NULL,  -- os.getpid() of the worker
    PRIMARY KEY (task_name, model_org, model_ckpt, n_train)
);

>>>> simpleshot.py
"""
Implements normalized nearest-centroid classifiers, as described in [this paper](https://arxiv.org/abs/1911.04623).

If you use this work, be sure to cite the original work:

```
@article{wang2019simpleshot,
  title={Simpleshot: Revisiting nearest-neighbor classification for few-shot learning},
  author={Wang, Yan and Chao, Wei-Lun and Weinberger, Kilian Q and Van Der Maaten, Laurens},
  journal={arXiv preprint arXiv:1911.04623},
  year={2019}
}
```
"""

import beartype
import numpy as np
import sklearn.base
import sklearn.neighbors
import sklearn.utils.validation
import torch
from jaxtyping import Float, jaxtyped

from . import helpers


@beartype.beartype
class SimpleShotClassifier(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):
    """
    scikit-learn wrapper for the "normalized nearest-centroid" classifier (a.k.a. SimpleShot, Wang et al. ICCV 2019).

    Parameters
    ----------
    device : {'cpu','cuda'}, default='cpu'
        Used only during `predict`; centroids are pushed to this device for fast batched distance computation.
    """

    def __init__(self, batch_size: int = 512, device: str = "cpu"):
        self.batch_size = batch_size
        self.device = device

    def fit(self, X, y):
        x, y = sklearn.utils.validation.check_X_y(X, y, dtype=np.float32, order="C")
        # centre the cloud
        self.x_mean_ = x.mean(axis=0, keepdims=True)

        x = x - self.x_mean_
        x = l2_normalize(x)

        self.clf_ = sklearn.neighbors.NearestCentroid()
        self.clf_.fit(x, y)
        return self

    def predict(self, X):
        sklearn.utils.validation.check_is_fitted(self, ["clf_", "x_mean_"])
        x = sklearn.utils.validation.check_array(X, dtype=np.float32, order="C")

        x = x - self.x_mean_
        x = l2_normalize(x)

        # Do this next step on the GPU to make it fast.
        centroids = torch.from_numpy(self.clf_.centroids_).to(
            self.device, non_blocking=True
        )
        x = torch.from_numpy(x).to(self.device, non_blocking=True)

        preds = []
        for start, stop in helpers.batched_idx(len(x), self.batch_size):
            x_batch = x[start:stop]
            distances = torch.linalg.vector_norm(x_batch[:, None] - centroids, axis=2)
            preds.append(torch.argmin(distances, dim=1).cpu())

        return torch.cat(preds, dim=0).numpy()


@jaxtyped(typechecker=beartype.beartype)
def l2_normalize(
    features: Float[np.ndarray, "n_examples dim"],
) -> Float[np.ndarray, "n_examples dim"]:
    """L2-normalize a batch of features.

    Args:
        features: batch of $d$-dimensional vectors.

    Returns:
        batch of $d$-dimensional vectors with unit L2 norm.
    """
    norms = np.linalg.norm(features, ord=2, axis=1, keepdims=True)
    return features / norms

>>>> test_aimv2.py
import pytest
import torch
import transformers

from . import config, helpers, registry

CKPTS = [
    "apple/aimv2-large-patch14-224",
    "apple/aimv2-large-patch14-224-distilled",
    "apple/aimv2-1B-patch14-224",
    "apple/aimv2-large-patch14-448",
]
DTYPE = torch.float32
ATOL, RTOL = 1e-5, 1e-4


@pytest.fixture(scope="session", params=CKPTS)
def models(request):
    ckpt = request.param
    hf = transformers.AutoModel.from_pretrained(
        ckpt, trust_remote_code=True, cache_dir=helpers.get_cache_dir()
    ).eval()
    bio = registry.load_vision_backbone(config.Model("aimv2", ckpt)).eval().to(DTYPE)
    return hf, bio


def _rand(batch: int = 1):
    torch.manual_seed(0)
    return torch.rand(batch, 3, 224, 224, dtype=DTYPE)


def test_same_shape_single(models):
    hf, bio = models
    batch = _rand()
    h = hf(batch).last_hidden_state
    b = bio.img_encode(batch).patch_features
    assert h.shape == b.shape


def test_values_close_single(models):
    hf, bio = models
    batch = _rand()
    h = hf(batch).last_hidden_state
    b = bio.img_encode(batch).patch_features
    assert torch.allclose(h, b, atol=ATOL, rtol=RTOL)


def test_values_close_batch(models):
    hf, bio = models
    batch = _rand(batch=4)
    h = hf(batch).last_hidden_state
    b = bio.img_encode(batch).patch_features
    assert torch.allclose(h, b, atol=ATOL, rtol=RTOL)

>>>> test_auto_batch_size.py
import itertools
import math
import threading
import typing

import pytest
import torch
import torch.utils.data

from . import helpers


def make_dataloader(size: int):
    data = torch.rand(size, 3, 32, 32)
    ds = torch.utils.data.TensorDataset(data)
    return torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, num_workers=0)


def make_probe(*, max_ok: int, style: typing.Literal["oom", "sdpa"] = "oom"):
    """
    Probe that raises a CUDA-style OOM if the *batch* is larger than `max_ok`. Works on CPU; keeps test independent of real GPU RAM.

    -1 indicates to never OOM.
    """

    def probe(batch):
        imgs = batch[0] if isinstance(batch, (list, tuple)) else batch
        if max_ok > 0 and imgs.shape[0] > max_ok:
            if style == "oom":
                raise RuntimeError("CUDA out of memory.")  # substring preserved
            elif style == "sdpa":
                raise RuntimeError("CUDA error: invalid configuration argument")
            else:
                typing.assert_never(style)
        return imgs.mean()  # cheap op

    return probe


def test_ctx_runs_and_restores():
    dataloader = make_dataloader(128)
    orig = dataloader.batch_sampler.batch_size
    probe = make_probe(max_ok=8)  # fail when >8

    with helpers.auto_batch_size(dataloader, probe=probe, schedule=(2, 4, 8, 16)):
        # context executes arbitrary user code w/out throwing
        for _ in itertools.islice(dataloader, 3):
            pass
        # inside ctx largest successful should be 8
        assert dataloader.batch_sampler.batch_size == 8

    # after exit, original value restored
    assert dataloader.batch_sampler.batch_size == orig


@pytest.mark.skipif(
    not torch.cuda.is_available(), reason="only meaningful on a GPU box"
)
def test_with_gpu():
    """Smoke-test true GPU execution (no artificial OOM)."""

    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(dataloader, probe=lambda x: x * 1, schedule=(2, 4)):
        next(iter(dataloader))


def test_len_adjusts_with_batch_size():
    """
    `len(dataloader)` reflects the tuned batch size inside the CM and restores afterward.
    """
    dataloader = make_dataloader(30)
    n_samples = len(dataloader.dataset)  # 30
    orig_bs = dataloader.batch_sampler.batch_size  # 2
    assert len(dataloader) == math.ceil(n_samples / orig_bs)  # 15

    probe = make_probe(max_ok=8)  # tuning will settle on 8

    with helpers.auto_batch_size(dataloader, probe=probe, schedule=(2, 4, 8, 16)):
        tuned_bs = dataloader.batch_sampler.batch_size
        assert tuned_bs == 8

        inside_len = len(dataloader)
        assert inside_len == math.ceil(n_samples / tuned_bs)  # 4

    # back to original
    assert dataloader.batch_sampler.batch_size == orig_bs
    assert len(dataloader) == math.ceil(n_samples / orig_bs)  # 15


def test_invalid_cfg_error_handled():
    """Helper treats 'invalid configuration argument' the same as OOM."""
    dataloader = make_dataloader(128)
    orig = dataloader.batch_sampler.batch_size
    probe = make_probe(max_ok=4, style="sdpa")  # fail when >4

    with helpers.auto_batch_size(dataloader, probe=probe, schedule=(2, 4, 8)):
        # should settle on 4
        assert dataloader.batch_sampler.batch_size == 4

    # restored afterwards
    assert dataloader.batch_sampler.batch_size == orig


def test_terminates_on_short_dataset():
    """If dataset has < tried batch-size, auto_batch_size must still terminate."""

    dataloader = make_dataloader(12)

    def run():
        with helpers.auto_batch_size(dataloader, probe=make_probe(max_ok=-1)):
            pass  # nothing

    t = threading.Thread(target=run, daemon=True)
    t.start()
    t.join(timeout=2.0)  # 2-second cap
    assert not t.is_alive(), "auto_batch_size never terminated on tiny dataset"


def test_caps_at_dataset_len_generic():
    class Sample(typing.NamedTuple):
        img: torch.Tensor
        meta: dict[str, int]

    def make_dataset_namedtuple(n):
        data = torch.rand(n, 3, 32, 32)
        ds = [Sample(img=x, meta={"idx": i}) for i, x in enumerate(data)]
        return ds

    ds = make_dataset_namedtuple(12)
    loader = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, num_workers=0)

    with helpers.auto_batch_size(
        loader, probe=lambda b: b.img.mean(), schedule=(4, 8, 16, 32)
    ):
        assert loader.batch_sampler.batch_size == len(ds)  # 12


def test_upper_caps_batch_size():
    """
    With upper=32 the helper must stop at 32 even though larger sizes fit.
    """

    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(
        dataloader,
        probe=make_probe(max_ok=-1),
        schedule=(2, 4, 8, 16, 32, 64, 128),
        upper=32,
    ):
        assert dataloader.batch_sampler.batch_size == 32, "should cap at upper"


def test_upper_below_start_allowed():
    """
    If upper < initial batch size, helper should *lower* to upper immediately.
    """
    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(
        dataloader, probe=make_probe(max_ok=-1), schedule=(2, 4, 8), upper=1
    ):
        assert dataloader.batch_sampler.batch_size == 1


def _probe_flaky(flake_at: int):
    """Calls above flake_at work once, but not again."""
    seen = set()

    def inner(batch):
        bsz = helpers.infer_batch_size(batch)
        if bsz not in seen:
            seen.add(bsz)

        if bsz not in seen or bsz < flake_at:
            return batch[0].mean()

        raise RuntimeError("CUDA out of memory.")

    return inner


def test_backoff_after_flaky_probe():
    # 8 succeeds then OOMs; helper should fall back to 6.
    dataloader = make_dataloader(128)
    with helpers.auto_batch_size(
        dataloader, probe=_probe_flaky(flake_at=8), schedule=(2, 3, 4, 6, 8, 12, 16)
    ):
        assert dataloader.batch_sampler.batch_size == 6


def test_backoff_zero_keeps_largest():
    """
    backoff=0 must reproduce the "largest that works" policy.
    """
    loader = make_dataloader(64)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=16), schedule=(2, 4, 8, 16, 32), backoff=0
    ):
        assert loader.batch_sampler.batch_size == 16


def test_backoff_one_steps_back_once():
    """Largest success is 16, back-off by one => 8."""
    loader = make_dataloader(64)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=16), schedule=(2, 4, 8, 16, 32), backoff=1
    ):
        assert loader.batch_sampler.batch_size == 8


def test_backoff_clamped_to_smallest():
    """Only 3 successes (2,4,8). backoff=10 should clamp to 2."""
    loader = make_dataloader(64)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=8), schedule=(2, 4, 8, 16), backoff=10
    ):
        assert loader.batch_sampler.batch_size == 2


def test_negative_backoff_error():
    """A negative value is invalid."""
    loader = make_dataloader(32)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=8), schedule=(2, 4, 8), backoff=-1
        ):
            pass


def test_flaky_probe_backoff_zero():
    """Probe OOMs *after* first success at 8. Helper should settle on 8 with backoff=0."""
    loader = make_dataloader(128)
    with helpers.auto_batch_size(
        loader, probe=_probe_flaky(flake_at=8), schedule=(2, 4, 6, 8, 10), backoff=0
    ):
        assert loader.batch_sampler.batch_size == 6


def test_flaky_probe_backoff_one_skips_flake():
    """8 becomes flaky; second-largest stable is 6."""
    loader = make_dataloader(128)
    with helpers.auto_batch_size(
        loader, probe=_probe_flaky(flake_at=8), schedule=(2, 4, 6, 8, 10), backoff=1
    ):
        assert loader.batch_sampler.batch_size == 4


def test_unsorted_schedule_raises():
    """Schedule 4,16,8... has a downward step (16 -> 8) => should raise."""
    loader = make_dataloader(64)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=32), schedule=(4, 16, 8, 32)
        ):
            pass


def test_duplicate_size_raises():
    """Duplicate 8 breaks the strict-increasing rule (8 -> 8)."""
    loader = make_dataloader(64)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=32), schedule=(4, 8, 8, 16)
        ):
            pass


def test_iterator_schedule_checked():
    """Even if an iterator lazily yields an out-of-order value (8 then 4), the helper must detect it."""

    def gen():
        yield 2
        yield 8
        yield 4  # drop -> should raise

    loader = make_dataloader(64)
    with pytest.raises(ValueError):
        with helpers.auto_batch_size(
            loader, probe=make_probe(max_ok=16), schedule=gen()
        ):
            pass


def test_single_success_backoff_clamps():
    """Only size 2 succeeds. backoff=3 must still give 2."""
    loader = make_dataloader(16)
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=2), schedule=(2, 4, 8), backoff=3
    ):
        assert loader.batch_sampler.batch_size == 2


def test_dataset_cap_and_backoff():
    """Dataset of 12 samples, schedule goes beyond 12. Largest feasible <=12 is 12; backoff=2 => 4."""
    loader = make_dataloader(12)  # initial bs=2
    with helpers.auto_batch_size(
        loader,
        probe=make_probe(max_ok=-1),  # never OOM
        schedule=(4, 8, 12, 16, 32),
        backoff=2,
    ):
        assert loader.batch_sampler.batch_size == 4
        assert loader.batch_sampler.batch_size <= len(loader.dataset)


@pytest.mark.timeout(1.0)
def test_schedule_not_materialised():
    """Passing an *infinite* strictly-increasing generator must *not* hang. If the implementation ever did `list(schedule)` or similar, this test would exceed the 1-second timeout and fail."""

    def infinite_pow2():
        n = 2
        while True:
            yield n  # 2, 4, 8, 16, ...
            n *= 2

    loader = make_dataloader(64)  # dataset len = 64
    # probe never OOMs -> helper should stop when it hits dataset size
    with helpers.auto_batch_size(
        loader, probe=make_probe(max_ok=-1), schedule=infinite_pow2()
    ):
        # final batch_size should equal dataset length (64)
        assert loader.batch_sampler.batch_size == len(loader.dataset)

>>>> test_config.py
import itertools
import pathlib
import textwrap

from . import config


def _write_toml(tmp_path: pathlib.Path) -> pathlib.Path:
    tmp_path.mkdir(exist_ok=True)
    tmp_file = tmp_path / "grid.toml"
    tmp_file.write_text(
        textwrap.dedent(
            """
            models = [
              { org = "timm",      ckpt = "resnet50" },
              { org = "open-clip", ckpt = "ViT-B-16/openai" },
            ]

            debug   = [ true, false ]
            n_train = [ 10, 100 ]

            [data]
            newt = "/data/newt"
            """
        )
    )
    return tmp_file


def test_load_returns_full_cartesian_product(tmp_path):
    toml_path = _write_toml(tmp_path)
    exps = config.load(str(toml_path))

    # --- expected triples ---
    expected = set(
        itertools.product(
            ["timm", "open-clip"],  # model_org
            [True, False],  # debug
            [10, 100],  # n_train
        )
    )

    # --- actual triples ---
    got = {(exp.model.org, exp.debug, exp.n_train) for exp in exps}

    assert got == expected, (
        f"Missing/extra combinations: expected {expected}, got {got}"
    )

    # data-block propagated
    for exp in exps:
        assert exp.data.newt == "/data/newt"

>>>> test_helpers.py
import collections

import beartype
import hypothesis.extra.numpy as npst
import numpy as np
from hypothesis import assume, given
from hypothesis import strategies as st
from jaxtyping import Int, jaxtyped

from . import helpers


@st.composite
def _labels_and_n(draw):
    """
    Helper strategy: (labels array, n) with n <= len(labels) and n >= #classes
    """
    labels_list = draw(
        st.lists(st.integers(min_value=0, max_value=50), min_size=1, max_size=300)
    )
    labels = np.array(labels_list, dtype=int)
    n_classes = len(np.unique(labels))
    # choose n in [n_classes, len(labels)]
    n = draw(st.integers(min_value=n_classes, max_value=len(labels)))
    return labels, n


@jaxtyped(typechecker=beartype.beartype)
def _measure_balance(
    labels: Int[np.ndarray, " n_labels"], indices: Int[np.ndarray, " n"]
) -> float:
    """
    Calculate a balance metric (coefficient of variation, lower is better) for the selected samples (labels[indices]).

    Returns 0 for perfect balance, higher for more imbalance.
    """
    if len(indices) == 0:
        return 0.0

    # Get the distribution of classes in the selected samples
    selected_labels = labels[indices]
    class_counts = collections.Counter(selected_labels)

    # Get all unique classes in the original dataset
    all_classes = set(labels)

    # Check if it was possible to include at least one of each class but didn't
    if len(indices) >= len(all_classes) and len(class_counts) < len(all_classes):
        return float("inf")

    # Calculate coefficient of variation (standard deviation / mean)
    counts = np.array(list(class_counts.values()))

    # If only one class is present, return a high value to indicate imbalance
    if len(counts) == 1:
        return float("inf")

    mean = np.mean(counts)
    std = np.std(counts, ddof=1)  # Using sample standard deviation

    # Return coefficient of variation (0 for perfect balance)
    return std / mean if mean > 0 else 0.0


@given(
    st.text(
        # printable ASCII (includes '/' ':' we want to scrub)
        alphabet=st.characters(min_codepoint=32, max_codepoint=126),
        max_size=100,
    )
)
def test_fs_safe_idempotent_and_clean(random_text):
    cleaned = helpers.fs_safe(random_text)

    # 1) Forbidden characters removed
    assert ":" not in cleaned and "/" not in cleaned

    # 2) Idempotent
    assert helpers.fs_safe(cleaned) == cleaned


@given(
    total=st.integers(min_value=0, max_value=1_000),
    batch=st.integers(min_value=1, max_value=400),
)
def test_batched_idx_covers_range_without_overlap(total, batch):
    """batched_idx must partition [0,total) into consecutive, non-overlapping spans, each of length <= batch."""
    spans = list(helpers.batched_idx(total, batch))

    # edge-case: nothing to iterate
    if total == 0:
        assert spans == []
        return

    # verify each span and overall coverage
    covered = []
    expected_start = 0
    for start, stop in spans:
        # bounds & width checks
        assert 0 <= start < stop <= total
        assert (stop - start) <= batch
        # consecutiveness (no gaps/overlap)
        assert start == expected_start
        expected_start = stop
        covered.extend(range(start, stop))

    # spans collectively cover exactly [0, total)
    assert covered == list(range(total))


@given(
    labels=npst.arrays(
        dtype=np.int32,
        shape=st.integers(min_value=1000, max_value=10000),
        elements=st.integers(min_value=0, max_value=100),
    ),
    n=st.integers(min_value=10, max_value=1000),
)
def test_correct_sample_size(labels, n):
    """Test that the function returns exactly n samples (or all if n > len(labels))"""
    assume(len(np.unique(labels)) > 1)  # Ensure we have at least 2 classes

    indices = helpers.balanced_random_sample(labels, n)

    # Check that the number of samples is correct
    n_expected = min(n, len(labels))
    assert len(indices) == n_expected

    # Check that all indices are valid
    assert np.all(indices < len(labels)), "Some indices are out of bounds"

    # Check that there are no duplicate indices
    assert len(indices) == len(np.unique(indices)), "Duplicate indices found"


# Test case 2: Class balance property
@given(
    labels=npst.arrays(
        dtype=np.int32,
        shape=st.integers(min_value=1000, max_value=10000),
        elements=st.integers(min_value=0, max_value=20),
    ),
    n=st.integers(min_value=100, max_value=1000),
)
def test_class_balance(labels, n):
    """
    Test that the class distribution in the sample is more balanced than random sampling would be.
    """
    unique_classes = np.unique(labels)
    assume(len(unique_classes) > 1)  # Ensure we have at least 2 classes
    assume(n >= len(unique_classes))  # Ensure we request at least one sample per class

    # Get balanced samples
    balanced_indices = helpers.balanced_random_sample(labels, n)
    balanced_balance = _measure_balance(labels, balanced_indices)

    # Get a normal random sample for comparison
    random_indices = np.random.choice(len(labels), min(n, len(labels)), replace=False)
    random_balance = _measure_balance(labels, random_indices)

    # Check if our balanced sampling is generally better than random
    # Note: This might occasionally fail due to randomness, but should pass most of the time
    assert balanced_balance <= random_balance * 1.5, (
        f"Balance metric: balanced={balanced_balance}, random={random_balance}"
    )


def test_single_class_sampling():
    """Test sampling when all samples are from the same class"""
    labels = np.array([1, 1, 1, 1, 1], dtype=int)
    indices = helpers.balanced_random_sample(labels, 3)
    assert len(indices) == 3
    assert len(np.unique(indices)) == 3


def test_small_sample_size():
    """Test sampling with a very small n"""
    labels = np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=int)
    indices = helpers.balanced_random_sample(labels, 2)
    assert len(indices) == 2


def test_sample_size_larger_than_dataset():
    """Test when requested sample size exceeds dataset size"""
    labels = np.array([0, 1, 2], dtype=int)
    indices = helpers.balanced_random_sample(labels, 10)
    assert len(indices) == 3  # Should return all samples
    assert set(indices) == {0, 1, 2}


def test_empty_dataset():
    """Test sampling from an empty dataset"""
    labels = np.array([], dtype=int)
    indices = helpers.balanced_random_sample(labels, 5)
    assert len(indices) == 0


def test_zero_samples_requested():
    """Test when zero samples are requested"""
    labels = np.array([0, 1, 2, 3], dtype=int)
    indices = helpers.balanced_random_sample(labels, 0)
    assert len(indices) == 0


@given(_labels_and_n())
def test_balanced_random_sample_includes_each_class_when_possible(data):
    labels, n = data
    idx = helpers.balanced_random_sample(labels, n)
    assert set(labels[idx]) == set(np.unique(labels))


@given(
    labels=st.lists(st.integers(min_value=0, max_value=100), min_size=10, max_size=500),
    n=st.integers(min_value=1, max_value=500),
)
def test_balanced_random_sample_never_duplicates_indices(labels, n):
    """Returned index list must have no duplicates."""
    labels = np.array(labels, dtype=int)
    n = min(n, len(labels))  # cap n to dataset size

    idx = helpers.balanced_random_sample(labels, n)

    # uniqueness & length
    assert len(idx) == len(set(idx))
    assert len(idx) == n or len(idx) == len(labels)  # handles n>len(labels)

>>>> test_openset.py
"""Unit tests for `openset.MahalanobisOpenSetClassifier`."""

import numpy as np
import pytest
import sklearn.linear_model
from hypothesis import given, settings
from hypothesis import strategies as st

from . import openset

# --------#
# Helpers #
# --------#


def _simple_classifier():
    return openset.MahalanobisOpenSetClassifier(
        base_estimator=sklearn.linear_model.RidgeClassifier(),
        alpha=0.95,
        unknown_label=-1,
    )


def _toy_data():
    """Return a trivially separable 1-D two-class dataset."""
    x = np.array([-2.0, -1.5, -1.0, 1.0, 1.5, 2.0]).reshape(-1, 1)
    y = np.array([0, 0, 0, 1, 1, 1])
    return x, y


# ------------------ #
# Estimator API test #
# ------------------ #


def test_estimator_api():
    clf = _simple_classifier()
    x, y = _toy_data()

    # scikit-learn compliance: fit / predict / get_params / set_params
    assert hasattr(clf, "get_params") and hasattr(clf, "set_params")

    clf.fit(x, y)
    preds = clf.predict(x)

    # Attributes created by fit
    for attr in ("clf_", "means_", "inv_covariance_", "tpr_"):
        assert hasattr(clf, attr), f"missing attribute {attr} after fit()"

    # Perfect prediction on training data
    assert np.array_equal(preds, y)


# ----------------------------------------------- #
# Hypothesis: fuzz for exceptions (fit & predict) #
# ----------------------------------------------- #


@given(
    n_samples=st.integers(min_value=30, max_value=200),
    n_features=st.integers(min_value=2, max_value=60),
    n_classes=st.integers(min_value=2, max_value=40),
)
@settings(deadline=400)
def test_fuzz_no_exceptions(n_samples, n_features, n_classes):
    rng = np.random.default_rng()
    x = rng.normal(size=(n_samples, n_features)).astype(np.float32)
    y = rng.integers(0, n_classes, size=n_samples)

    clf = _simple_classifier()
    clf.fit(x, y)  # should not raise
    preds = clf.predict(x)

    assert preds.shape[0] == n_samples


# -------------------------------- #
# Hand-written deterministic cases #
# -------------------------------- #


@pytest.mark.parametrize(
    "test_pt, expected",
    [
        (np.array([[-1.0]]), 0),  # near class-0 centroid
        (np.array([[1.2]]), 1),  # near class-1 centroid
        (np.array([[100.0]]), -1),  # far away -> unknown
        (np.array([[-50.0]]), -1),  # far other side -> unknown
    ],
)
def test_known_vs_unknown(test_pt, expected):
    x, y = _toy_data()
    clf = _simple_classifier().fit(x, y)
    pred = clf.predict(test_pt)[0]
    assert pred == expected


# --------------------------------------------------------------#
# Property-based tests that performance is *better than random* #
# --------------------------------------------------------------#


def _make_cluster_data(n_classes=3, pts_per_class=30, d=4, noise=0.05):
    """Generate tight Gaussian clusters plus a distant OOD cluster."""
    rng = np.random.default_rng(0)
    centers = rng.normal(scale=3.0, size=(n_classes, d))

    x_known = []
    y_known = []
    for k, c in enumerate(centers):
        x_known.append(rng.normal(loc=c, scale=noise, size=(pts_per_class, d)))
        y_known.append(np.full(pts_per_class, k))
    x_known = np.vstack(x_known)
    y_known = np.concatenate(y_known)

    # OOD data around a far corner
    ood_center = np.full(d, 10.0)
    x_ood = rng.normal(loc=ood_center, scale=noise, size=(pts_per_class, d))
    y_ood = np.full(pts_per_class, -1)  # unknown label for evaluation

    return (x_known, y_known), (x_ood, y_ood)


@pytest.mark.parametrize("seed", [0, 42])
def test_high_known_accuracy(seed):
    (x_k, y_k), (x_ood, _) = _make_cluster_data()
    rng = np.random.default_rng(seed)

    idx = rng.choice(len(x_k), size=60, replace=False)
    x_train, y_train = x_k[idx], y_k[idx]
    x_test = np.vstack([x_k, x_ood])
    y_test = np.concatenate([y_k, np.full(len(x_ood), -1, dtype=y_k.dtype)])

    clf = _simple_classifier().fit(x_train, y_train)
    preds = clf.predict(x_test)

    # Basic expectations:
    acc = (preds == y_test).mean()
    assert acc > 0.8, f"Expected >80 % overall accuracy, got {acc:.2f}"


def test_roc_like_property():
    (x_k, y_k), (x_ood, _) = _make_cluster_data(n_classes=2)
    x_train, y_train = x_k[:40], y_k[:40]
    clf = _simple_classifier().fit(x_train, y_train)

    scores = clf.decision_function(np.vstack([x_k, x_ood]))

    # Simple AUC approximation: score separation
    separation = scores[: len(x_k)].mean() - scores[len(x_k) :].mean()
    assert separation > 0.5, "Mahalanobis scores should separate ID from OOD"


@st.composite
def random_spd_inv(draw, d: int):
    """Random symmetric-positive-definite inverse covariance."""
    A = draw(
        st.lists(
            st.floats(-5, 5, allow_nan=False, allow_infinity=False),
            min_size=d * d,
            max_size=d * d,
        )
    )
    A = np.asarray(A, dtype=np.float64).reshape(d, d)
    cov = A @ A.T + 1e-3 * np.eye(d)
    return np.linalg.inv(cov)


@st.composite
def maha_inputs(draw):
    # dimensions
    n = draw(st.integers(min_value=1, max_value=64))
    d = draw(st.integers(min_value=1, max_value=32))
    C = draw(st.integers(min_value=1, max_value=16))

    # feature matrix
    X = draw(
        st.lists(
            st.floats(-10, 10, allow_nan=False, allow_infinity=False),
            min_size=n * d,
            max_size=n * d,
        )
    )
    X = np.asarray(X, dtype=np.float64).reshape(n, d)

    # class means
    mu = draw(
        st.lists(
            st.floats(-10, 10, allow_nan=False, allow_infinity=False),
            min_size=C * d,
            max_size=C * d,
        )
    )
    mu = np.asarray(mu, dtype=np.float64).reshape(C, d)

    # shared precision (inverse covariance)
    cov_inv = draw(random_spd_inv(d))

    return X, mu, cov_inv


@given(maha_inputs())
def test_min_mahalanobis_sq_no_error(data):
    X, mu, cov_inv = data
    out = openset.min_mahalanobis_sq(X, mu, cov_inv)

    # basic sanity: shape and finiteness
    assert out.shape == (X.shape[0],)
    assert np.isfinite(out).all()
    assert (out >= 0).all()


@given(maha_inputs())
def test_min_mahalanobis_sq_batched_no_error(data):
    X, mu, cov_inv = data
    out = openset.min_mahalanobis_sq_batched(X, mu, cov_inv)

    # basic sanity: shape and finiteness
    assert out.shape == (X.shape[0],)
    assert np.isfinite(out).all()
    assert (out >= 0).all()


@given(maha_inputs())
def test_min_mahalanobis_sq_batched_equal(data):
    X, mu, cov_inv = data
    expected = openset.min_mahalanobis_sq(X, mu, cov_inv)
    actual = openset.min_mahalanobis_sq_batched(X, mu, cov_inv)

    assert expected.shape == actual.shape
    np.testing.assert_allclose(actual, expected, atol=1e-5, rtol=1e-5)

>>>> test_reporting.py
import json
import math
import multiprocessing
import pathlib
import sqlite3
import time

import beartype
import numpy as np
import pytest
from hypothesis import given
from hypothesis import strategies as st

from . import config, reporting

multiprocessing.set_start_method("spawn", force=True)


@st.composite
def _prediction_list(draw):
    """Generate an arbitrary non-empty list[Prediction] for single-label multiclass."""

    n = draw(st.integers(min_value=1, max_value=256))
    # Allow up to 50 distinct class IDs (0-50) - plenty for the property test.
    y_true = draw(
        st.lists(st.integers(min_value=0, max_value=50), min_size=n, max_size=n)
    )
    y_pred = draw(
        st.lists(st.integers(min_value=0, max_value=50), min_size=n, max_size=n)
    )

    preds = [
        reporting.Prediction(
            id=str(i),
            score=float(y_pred[i] == y_true[i]),
            info={"y_true": y_true[i], "y_pred": y_pred[i]},
        )
        for i in range(n)
    ]
    return preds


@st.composite
def _prediction_batch(draw):
    """Generate a *batch* (B >= 1) of equal-length prediction lists."""
    B = draw(st.integers(1, 2))  # batch size
    n = draw(st.integers(1, 4))  # common sample count
    batch = []
    for _ in range(B):
        y_true = draw(st.lists(st.integers(0, 50), min_size=n, max_size=n))
        y_pred = draw(st.lists(st.integers(0, 50), min_size=n, max_size=n))
        preds = [
            reporting.Prediction(
                id=str(i),
                score=float(y_pred[i] == y_true[i]),
                info={"y_true": y_true[i], "y_pred": y_pred[i]},
            )
            for i in range(n)
        ]
        batch.append(preds)
    return batch


@given(preds=_prediction_list())
def test_micro_f1_equals_micro_accuracy(preds):
    """Micro-averaged F1 must equal micro accuracy for single-label data."""

    acc = reporting.micro_acc(preds)
    f1 = reporting.micro_f1(preds)

    # Floating math can introduce tiny error, so compare with tolerance
    assert math.isclose(acc, f1, rel_tol=1e-12, abs_tol=1e-12)


@given(preds=_prediction_list())
def test_macro_f1_batch_matches_macro_f1_bsz0(preds):
    """Vectorised implementation must equal our non-batched macro-F1."""
    y_true = np.fromiter((p.info["y_true"] for p in preds), dtype=int)
    y_pred = np.fromiter((p.info["y_pred"] for p in preds), dtype=int)

    ours = reporting.macro_f1_batch(y_true, y_pred)
    ref = reporting.macro_f1(preds)

    assert math.isclose(ours, ref, rel_tol=1e-12, abs_tol=1e-12)


@given(preds=_prediction_list())
def test_macro_f1_batch_matches_macro_f1_bsz1(preds):
    """Vectorised implementation must equal our non-batched macro-F1."""
    y_true = np.fromiter((p.info["y_true"] for p in preds), dtype=int)
    y_pred = np.fromiter((p.info["y_pred"] for p in preds), dtype=int)

    ours = reporting.macro_f1_batch(y_true[None, :], y_pred[None, :])[0]
    ref = reporting.macro_f1(preds)

    assert math.isclose(ours, ref, rel_tol=1e-12, abs_tol=1e-12)


@given(batch=_prediction_batch())
def test_macro_f1_batch_matches_macro_f1_bsz_n(batch):
    """For a true batch (B > 1 possible), vectorised `macro_f1_batch` must equal looping over the legacy `macro_f1`."""
    # stack into (B, n)
    y_true = np.stack([
        np.fromiter((p.info["y_true"] for p in preds), dtype=int) for preds in batch
    ])
    y_pred = np.stack([
        np.fromiter((p.info["y_pred"] for p in preds), dtype=int) for preds in batch
    ])

    labels = np.unique(np.stack([y_true, y_pred]))
    labels = np.arange(labels.max() + 1)

    ours = reporting.macro_f1_batch(y_true, y_pred)
    ref = np.array([
        reporting.macro_f1(preds, labels=labels.tolist()) for preds in batch
    ])

    assert np.allclose(ours, ref, rtol=1e-12, atol=1e-12)


f1_macro_edgecases = [
    [[reporting.Prediction(id="0", score=1.0, info={"y_true": 2, "y_pred": 2})]],
    # [[reporting.Prediction(id="0", score=1.0, info={"y_true": 0, "y_pred": 0})]],
    # [
    #     [
    #         reporting.Prediction(id="0", score=1.0, info={"y_true": 0, "y_pred": 0}),
    #         reporting.Prediction(id="1", score=0.0, info={"y_true": 1, "y_pred": 0}),
    #     ],
    # ],
    # [
    #     [
    #         reporting.Prediction(id="0", score=1.0, info={"y_true": 0, "y_pred": 0}),
    #         reporting.Prediction(id="1", score=0.0, info={"y_true": 0, "y_pred": 1}),
    #     ]
    # ],
    # [
    #     [
    #         reporting.Prediction(id="0", score=1.0, info={"y_true": 0, "y_pred": 0}),
    #         reporting.Prediction(id="1", score=0.0, info={"y_true": 1, "y_pred": 0}),
    #     ],
    #     [
    #         reporting.Prediction(id="1", score=0.0, info={"y_true": 1, "y_pred": 0}),
    #         reporting.Prediction(id="1", score=0.0, info={"y_true": 1, "y_pred": 0}),
    #     ],
    # ],
]


@pytest.mark.parametrize("batch", f1_macro_edgecases)
def test_macro_f1_batch_matches_macro_f1_edgecases(batch):
    """For a true batch (B > 1 possible), vectorised `macro_f1_batch` must equal looping over the legacy `macro_f1`."""
    # stack into (B, n)
    y_true = np.stack([
        np.fromiter((p.info["y_true"] for p in preds), dtype=int) for preds in batch
    ])
    y_pred = np.stack([
        np.fromiter((p.info["y_pred"] for p in preds), dtype=int) for preds in batch
    ])

    ours = reporting.macro_f1_batch(y_true, y_pred)  # shape (B,)
    ref = np.array([reporting.macro_f1(preds) for preds in batch])

    assert np.allclose(ours, ref, rtol=1e-12, atol=1e-12)


##############################
# Tests for SQLite Reporting #
##############################

NOW = 1_700_000_000.0  # fixed "current" time so tests are deterministic
HOUR = 3600


@beartype.beartype
def make_cfg(tmp_path: pathlib.Path) -> config.Experiment:
    """Return a minimal Experiment that points its report DB inside tmp_path."""
    return config.Experiment(
        model=config.Model(org="openai", ckpt="ViT-B/16"),
        n_train=-1,
        report_to=str(tmp_path),
        log_to=str(tmp_path / "logs"),
    )


@beartype.beartype
def _insert_experiment(db: sqlite3.Connection, cfg: config.Experiment, task: str):
    """Directly insert a row into experiments (used to pre-populate scenario 1)."""
    values = (
        task,
        cfg.model.org,
        cfg.model.ckpt,
        cfg.n_train,
        json.dumps(cfg.to_dict()),
        "[]",
        "deadbeef",
        time.time(),
        "",
        "pytest",
    )
    stmt = """
    INSERT INTO experiments
    (task_name, model_org, model_ckpt, n_train,
     exp_cfg, argv, git_commit, posix, gpu_name, hostname)
    VALUES (?,?,?,?,?,?,?,?,?,?)
    """
    db.execute(stmt, values)
    db.commit()


@beartype.beartype
def _insert_claim(
    db: sqlite3.Connection, cfg: config.Experiment, task: str, *, age_h: float | int
):
    """Insert a claim with `age_h` hours of staleness via claim_run."""
    # fake the clock *during* the INSERT
    t0 = time.time
    time.time = lambda: NOW - age_h * HOUR
    try:
        assert reporting.claim_run(db, cfg, task)  # must succeed
    finally:
        time.time = t0


def test_skip_when_experiment_exists(tmp_path):
    cfg = make_cfg(tmp_path)
    task = "plantnet"

    db = reporting.get_db(cfg)
    _insert_experiment(db, cfg, task)

    assert reporting.already_ran(db, cfg, task) is True


BUSY_TIMEOUT = 30
WAIT = BUSY_TIMEOUT + 5


def _worker(cfg: config.Experiment, task: str, q, succeed: bool):
    db = reporting.get_db(cfg)
    if reporting.claim_run(db, cfg, task):
        time.sleep(20)
        if succeed:
            _insert_experiment(db, cfg, task)
            reporting.release_run(db, cfg, task)
            q.put("winner")
        else:
            reporting.release_run(db, cfg, task)
            q.put("failed")
    else:
        q.put("skip")


def test_one_winner_many_launchers(tmp_path):
    cfg = make_cfg(tmp_path)
    task = "kabr"
    q = multiprocessing.Queue()

    n_workers = 4

    procs = [
        multiprocessing.Process(target=_worker, args=(cfg, task, q, True))
        for _ in range(n_workers)
    ]
    for p in procs:
        p.start()
    for p in procs:
        p.join(timeout=WAIT)

    results = [q.get(timeout=WAIT) for _ in range(n_workers)]
    assert results.count("winner") == 1
    assert results.count("skip") == n_workers - 1

    db = reporting.get_db(cfg)
    # experiments row present
    assert reporting.already_ran(db, cfg, task)
    # runs table cleaned up
    assert db.execute("SELECT COUNT(*) FROM runs").fetchone()[0] == 0


def test_reclaim_after_failure(tmp_path):
    cfg = make_cfg(tmp_path)
    task = "kabr"
    q = multiprocessing.Queue()

    n_workers = 4

    # wave 1: one process fails
    procs1 = [multiprocessing.Process(target=_worker, args=(cfg, task, q, False))]
    procs1.extend([
        multiprocessing.Process(target=_worker, args=(cfg, task, q, True))
        for i in range(n_workers - 1)
    ])
    for p in procs1:
        p.start()
        time.sleep(0.5)
    for p in procs1:
        p.join(timeout=40)

    results1 = [q.get(timeout=40) for _ in range(n_workers)]
    assert results1.count("failed") == 1

    # wave 2: slot should be claimable again
    procs2 = [
        multiprocessing.Process(target=_worker, args=(cfg, task, q, True))
        for _ in range(n_workers)
    ]
    for p in procs2:
        p.start()
    for p in procs2:
        p.join(timeout=40)

    results2 = [q.get(timeout=40) for _ in range(n_workers)]
    assert results2.count("winner") == 1

    db = reporting.get_db(cfg)
    assert reporting.already_ran(db, cfg, task)
    assert db.execute("SELECT COUNT(*) FROM runs").fetchone()[0] == 0


def test_empty_db(tmp_path):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    assert reporting.clear_stale_claims(db, max_age_hours=72) == 0


def test_fresh_kept(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=5)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 0
    assert reporting.is_claimed(db, cfg, "task")


def test_stale_removed_and_reclaimable(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=100)

    # can't claim while it's stale & present
    assert reporting.is_claimed(db, cfg, "task")

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 1
    # now slot is free again
    assert not reporting.is_claimed(db, cfg, "task")
    assert reporting.claim_run(db, cfg, "task")


def test_mixed_rows(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task1", age_h=10)
    _insert_claim(db, cfg, "task2", age_h=90)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db, max_age_hours=72) == 1
    assert reporting.is_claimed(db, cfg, "task1")
    assert not reporting.is_claimed(db, cfg, "task2")


@pytest.mark.parametrize("hrs", [71.99, 72.0])
def test_exact_cutoff_not_deleted(tmp_path, monkeypatch, hrs):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=hrs)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 0
    assert reporting.is_claimed(db, cfg, "task")


def test_custom_threshold(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=50)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db, max_age_hours=45) == 1


@pytest.mark.parametrize("bad", [0, -5])
def test_bad_threshold_raises(tmp_path, bad):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    with pytest.raises(ValueError):
        reporting.clear_stale_claims(db, max_age_hours=bad)


def test_idempotent(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task", age_h=150)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 1
    # second pass should remove nothing
    assert reporting.clear_stale_claims(db) == 0


def test_rowcount_exact(tmp_path, monkeypatch):
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    _insert_claim(db, cfg, "task1", age_h=120)
    _insert_claim(db, cfg, "task2", age_h=130)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 2


def test_future_schema_extra_column(tmp_path, monkeypatch):
    # add extra column to ensure helper ignores unknown columns
    cfg = make_cfg(tmp_path)
    db = reporting.get_db(cfg)
    db.execute("ALTER TABLE runs ADD COLUMN note TEXT")
    _insert_claim(db, cfg, "task", age_h=100)

    monkeypatch.setattr(time, "time", lambda: NOW)
    assert reporting.clear_stale_claims(db) == 1

>>>> test_smoke.py
def test_smoke_imports():
    """Test that all modules can be imported without errors."""
    import biobench
    import biobench.aimv2
    import biobench.beluga
    import biobench.beluga.download
    import biobench.config
    import biobench.fishnet
    import biobench.fishnet.download
    import biobench.fungiclef
    import biobench.fungiclef.download
    import biobench.fungiclef.metrics
    import biobench.helpers
    import biobench.herbarium19
    import biobench.herbarium19.download
    import biobench.imagenet1k
    import biobench.inat21
    import biobench.inat21.download
    import biobench.iwildcam
    import biobench.iwildcam.download
    import biobench.newt
    import biobench.newt.download
    import biobench.openset
    import biobench.plankton
    import biobench.plankton.download
    import biobench.plantnet
    import biobench.plantnet.download
    import biobench.rarespecies
    import biobench.registry
    import biobench.reporting
    import biobench.simpleshot
    import biobench.third_party_models
    import biobench.vjepa

    # Check that key classes/functions exist
    assert hasattr(biobench.registry, "VisionBackbone")
    assert hasattr(biobench.config, "Experiment")
    assert hasattr(biobench.fungiclef, "benchmark")


def test_basic_instantiation():
    """Test that basic objects can be instantiated."""
    from biobench.config import Data, Experiment, Model

    # Create minimal valid objects
    model = Model(org="test", ckpt="test")
    data = Data()
    experiment = Experiment(model=model, data=data, seed=42)

    # Verify they have expected properties
    assert experiment.model.org == "test"
    assert experiment.seed == 42

>>>> test_vjepa.py
import pytest
import torch

from . import config, registry

DTYPE = torch.float32
ATOL, RTOL = 1e-5, 1e-4

CKPTS = ["vitl16", "vith16", "vith16-384"]


@pytest.fixture(scope="session", params=CKPTS, ids=lambda x: x[0])
def model(request):
    model_ckpt = request.param
    return registry.load_vision_backbone(config.Model("vjepa", model_ckpt))


def rand(b: int = 2):
    torch.manual_seed(0)
    return torch.rand(b, 3, 224, 224, dtype=DTYPE)


def test_is_nn(model):
    assert isinstance(model, torch.nn.Module)


def test_smoke(model):
    x = rand()
    model.img_encode(x.squeeze(1))

>>>> testing.md
# Testing

**Session‑scoped, parametrised fixture** is the centrepiece of out tests.

## TL;DR

We turn the checkpoint list into a **parametrised fixture** so that pytest automatically generates an independent test item for every `(test‑function, checkpoint)` pair, caches the expensive model‑load once per checkpoint for the whole session, and gives us clean, filterable IDs in the test report. This is the idiomatic way to fan‑out identical assertions over multiple datasets or configurations, and it avoids manual loops or repeated downloads.

## Separate test items & readable reports
`@pytest.fixture(..., params=CKPTS)` makes pytest expand each consuming test into *N* items (one per checkpoint). The resulting IDs appear as `test_same_shape_single[apple/aimv2‑large…]`, which pinpoints failures and lets you run a single case with `-k`.

## Automatic caching
With `scope="session"` the fixture is created once per parameter and reused everywhere. Heavy objects (two models and their weights) load exactly one time, saving minutes of startup.

## First‑class pytest features
Because each checkpoint is a real test item you can mark or skip it (`pytest.param(..., marks=...)`) or xfail just one checkpoint. This is impossible if you write an inner `for`‑loop instead.

## Composability
Other fixtures (e.g. a random image generator) can depend on `models`; they inherit the same parameter seamlessly without extra wiring.

## Why *session* scope?

* **Most expensive setup wins**: loading two ViT‑L checkpoints dwarfs per‑test compute, so widest scope gives biggest pay‑off.
* **Stateless models**: evaluation‑mode transformers are read‑only, so sharing them across tests is safe; there’s no gradient accumulation or RNG mutation.
* **Disk cache friendliness**: the `cache_dir` argument points to your global HF cache, so downloads happen once even across separate pytest sessions.


## Quick mental model

1. **Fixture creation**
   * Pytest iterates over `CKPTS`; for each, runs the body once, caches the `(hf,bio)` pair.
2. **Test expansion**
   * For every test that requests `models`, pytest clones it per checkpoint, injecting the cached pair.
3. **Execution order**
   * Fixture builds happen before the first test needing them. If ten tests all use `models`, the models load only once per checkpoint.
4. **Random helper `_rand()`**
   * Generates deterministic tensors (seed=0) so the value comparison is reproducible.

Keep this in mind and the tests should still make sense even after a long hiatus.

>>>> third_party_models.py
import logging
import os

import beartype
import einops
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import helpers, registry

logger = logging.getLogger("third_party")


@beartype.beartype
def get_ssl() -> bool:
    """
    Checks whether BIOBENCH_DISABLE_SSL is present in the environment.

    We use environment variables rather than a boolean argument because

    1. This is only needed on some systems, like OSC.
    2. Every benchmark needs it in exactly the same way, so it would show up in every benchmark script as more "noise".
    3. It is not manipulated throughout the running of the program. It's a global variable that's set at the start of the jobs.

    But in general, we should not use environment variables to manage program state.

    Returns:
        A boolean that's true if we should use SSL and false if not.
    """
    disable = os.environ.get("BIOBENCH_DISABLE_SSL", None)
    return not disable


@jaxtyped(typechecker=beartype.beartype)
class OpenClip(registry.VisionBackbone):
    """
    Loads checkpoints from [open_clip](https://github.com/mlfoundations/open_clip), an open-source reproduction of the original [CLIP](https://arxiv.org/abs/2103.00020) paper.

    Checkpoints are in the format `<ARCH>/<CKPT>`.
    Look at the [results file](https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv) for the pretrained models.
    For example, to load a ViT-B/16 train on Apple's Data Filtering Networks dataset, you would use `ViT-B-16/dfn2b`.
    """

    def __init__(self, ckpt: str, drop_keys: list[str] | None = None, **_):
        super().__init__()
        import open_clip

        if not get_ssl():
            logger.warning("Ignoring SSL certs. Try not to do this!")
            # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761
            # Ideally we don't have to disable SSL but we are only downloading weights.
            import ssl

            ssl._create_default_https_context = ssl._create_unverified_context

        if ckpt.startswith("hf-hub:"):
            clip, self.img_transform = open_clip.create_model_from_pretrained(ckpt)
        elif ckpt.startswith("local:"):
            # Format: "local:ARCH/PATH_TO_CHECKPOINT"
            ckpt = ckpt.removeprefix("local:")
            arch, local_path = ckpt.split("/", 1)
            clip, self.img_transform = self._load_clip_skeleton(arch)
            state_dict = self._patch_state_dict(local_path, drop_keys or [])
            clip.load_state_dict(state_dict, strict=True)
        else:
            arch, ckpt = ckpt.split("/")
            clip, self.img_transform = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )

        self.model = clip.visual
        self.model.output_tokens = True  # type: ignore

    @staticmethod
    def _load_clip_skeleton(arch: str):
        import open_clip

        # returns (model, eval_transform)
        model, _, preprocess = open_clip.create_model_and_transforms(
            arch, pretrained=None
        )
        return model, preprocess

    @staticmethod
    def _patch_state_dict(path: str, drop_keys: list[str]) -> dict:
        state_dict = torch.load(path, map_location="cpu", weights_only=False)
        # open_clip stores state_dict, optimizer, etc. We want the state_dict.
        state_dict = state_dict.get("state_dict", state_dict)
        # Often a DDP-'module.' prefix.
        state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        for k in drop_keys:
            state_dict.pop(k, None)
        return state_dict

    def make_img_transform(self):
        return self.img_transform

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        result = self.model(batch)
        # Sometimes the model does not return patch features if it has none.
        if isinstance(result, tuple):
            img, patches = result
            return registry.EncodedImgBatch(img, patches)
        else:
            return registry.EncodedImgBatch(result, None)


@jaxtyped(typechecker=beartype.beartype)
class Timm(registry.VisionBackbone):
    """
    Wrapper for models from the Timm (PyTorch Image Models) library.

    This class provides an interface to use any model from the Timm library
    as a vision backbone in the biobench framework.
    """

    # TODO: docs + describe the ckpt format.
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import timm

        self.ckpt = ckpt

        self.model = timm.create_model(ckpt, pretrained=True)

        data_cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)
        self.img_transform = timm.data.create_transform(**data_cfg)

    def make_img_transform(self):
        return self.img_transform

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        bsz_orig, _, _, _ = batch.shape
        feats = self.model.forward_features(batch)
        if feats.ndim == 4:
            # This could be a convnet of some kind with (batch, dim, width, height) or a ViT with (batch, width, height, dim).
            # We do some shape checking to figure it out.
            bsz, d1, d2, d3 = feats.shape

            if bsz_orig != bsz:
                msg = f"Batch size changed from {bsz_orig} to {bsz} in {self.ckpt}.forward_features()"
                raise ValueError(msg)

            if d1 == d2 and d3 > d1 and d3 > d2:
                bsz, w, h, d = feats.shape
                patches = einops.rearrange(feats, "b w h d -> b (w h) d")
            elif d2 == d3 and d1 > d2 and d1 > d3:
                bsz, d, w, h = feats.shape
                patches = einops.rearrange(feats, "b d w h -> b (w h) d")
            else:
                msg = f"Can't interpret shape {feats.shape} for model '{self.ckpt}'."
                raise ValueError(msg)

            # Validate the shape of the features
            if not (d > w and d > h):
                msg = f"Expected feature dimensions (d={d}) to be larger than spatial dimensions (w={w}, h={h}). This suggests the tensor dimensions may be in an unexpected order."
                raise ValueError(msg)

            if w != h:
                msg = f"Expected equal spatial dimensions, but got width={w} and height={h}. Unequal spatial dimensions indicate a mistake in our understanding of the output features from model '{self.ckpt}'."
                raise ValueError(msg)

            # TODO: should we only use max pooling?
            img = patches.max(dim=1).values  # Global max pooling
        elif feats.ndim == 3:
            # This is probably a ViT with (batch, patches, dim)
            bsz, num_patches, d = feats.shape
            patches = feats

            # For ViT models, we typically use the class token ([CLS]) as the image representation
            # if it exists, otherwise we do mean pooling over patches
            if self.model.num_prefix_tokens > 0:
                img = patches[:, 0]  # Use [CLS] token
                # Remove prefix tokens (like [CLS]) from patches
                patches = patches[:, self.model.num_prefix_tokens :]
            else:
                img = patches.max(dim=1).values  # Max pooling if no [CLS] token
        else:
            raise ValueError(
                f"Unexpected feature dimension: {feats.ndim}. Expected either 3 (ViT models) or 4 (ConvNet models). Check if the model architecture {self.ckpt} is supported."
            )

        return registry.EncodedImgBatch(img, patches)


@jaxtyped(typechecker=beartype.beartype)
class DinoV2(registry.VisionBackbone):
    def __init__(self, ckpt: str, **kwargs):
        super().__init__()

        self.model = torch.hub.load("facebookresearch/dinov2", ckpt)

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        dct = self.model.forward_features(batch)

        return registry.EncodedImgBatch(
            dct["x_norm_clstoken"], dct["x_norm_patchtokens"]
        )

    def make_img_transform(self):
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=(256, 256)),
            v2.CenterCrop(size=(224, 224)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])


@jaxtyped(typechecker=beartype.beartype)
class SAM2(registry.VisionBackbone):
    """
    A very small wrapper around the SAM-2 Hiera backbones exposed by `timm`.

    Design choices:

    * We rely 100 % on `timm` for model construction & weight loading (`timm.create_model("hf_hub:timm/<model_name>", pretrained=True)`).
    * The image transform is the exact one timm used during pre-training: `data.create_transform(**resolve_data_config(model.pretrained_cfg))`.
    """

    def __init__(self, ckpt: str, **kwargs):
        super().__init__()
        import timm

        self.ckpt = ckpt

        self.model = timm.create_model(f"hf_hub:timm/{ckpt}", pretrained=True)

        self.data_cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)

    def make_img_transform(self):
        import timm

        return timm.data.create_transform(**self.data_cfg)

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        x = self.model.forward_features(batch)
        x = einops.rearrange(x, "b w h d -> b (w h) d")
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

>>>> vjepa.py
"""
# V-JEPA *frozen-feature* backbone port

This file is a self-contained re-implementation of the encoder used in *V-JEPA* (Meta FAIR, 2024) for frozen downstream evaluation. The design follows the public facebookresearch/jepa reference code but strips it down to the minimum needed for image-only tasks.

Key implementation choices
--------------------------
1. Most modules are verbatim (or lightly edited for typing/PEP-8) copies from the upstream repo so that we do *not* depend on the unpublished PyPI package:

2. The official frozen image classification script (https://github.com/facebookresearch/jepa/blob/51c59d518fc63c08464af6de585f78ac0c7ed4d5/evals/image_classification_frozen/eval.py#L451-L455) repeats a still image along the temporal axis before feeding it to the video-ViT. We reproduce that behaviour with `x = einops.repeat(batch, "b c h w -> b c f h w", f=self.n_frames)` so the model sees a 16-frame clip of identical images.

3. Checkpoints live at `https://dl.fbaipublicfiles.com/jepa/<ckpt>/<ckpt>.pth.tar`. We download them into an `$CACHE/vjepa` sub-folder (`download()` helper) to avoid git-annex or HF dependencies.

4. Only the EMA target encoder (`state["target_encoder"]`) is loaded, mirroring the authors' evaluation code.  The usual `module.` prefix is stripped so that the state dict matches our local module names.

5. `img_encode()` returns both the full patch grid and a **max-pooled** global descriptor (`x.max(dim=1).values`).  The attentive classifier used in the paper is *not* re-implemented here; you can bolt your own head on top of the returned per-patch features.

## Limitations / divergences from FAIR reference

* No mixed-precision, distributed training or attentive probe head. This module is encoder-only.
* Patch/Tubelet shapes are fixed (16x16x2) and `num_frames` is pinned to 16; if you need other variants, expose them through the constructor.
* Only the three public checkpoints (`vitl16`, `vith16`, `vith16-384`) are supported, but extending to future releases is one line in `__init__`.
"""

import functools
import math
import os
import pathlib

import beartype
import einops
import numpy as np
import requests
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import helpers, registry


@beartype.beartype
def get_3d_sincos_pos_embed(
    embed_dim, grid_size, grid_depth, cls_token=False, uniform_power=False
):
    """
    grid_size: int of the grid height and width
    grid_depth: int of the grid depth
    returns:
        pos_embed: [grid_depth*grid_size*grid_size, embed_dim] (w/o cls_token)
                or [1+grid_depth*grid_size*grid_size, embed_dim] (w/ cls_token)
    """
    grid_d = np.arange(grid_depth, dtype=float)
    grid_h = np.arange(grid_size, dtype=float)
    grid_w = np.arange(grid_size, dtype=float)
    grid_h, grid_d, grid_w = np.meshgrid(
        grid_h, grid_d, grid_w
    )  # order of meshgrid is very important for indexing as [d,h,w]

    if not uniform_power:
        h_embed_dim = embed_dim // 4
        w_embed_dim = embed_dim // 4
        d_embed_dim = embed_dim // 2
    else:
        h_embed_dim = w_embed_dim = d_embed_dim = int(np.ceil(embed_dim / 6) * 2)

    emb_h = get_1d_sincos_pos_embed_from_grid(h_embed_dim, grid_h)  # (T*H*W, D1)
    emb_w = get_1d_sincos_pos_embed_from_grid(w_embed_dim, grid_w)  # (T*H*W, D2)
    emb_d = get_1d_sincos_pos_embed_from_grid(d_embed_dim, grid_d)  # (T*H*W, D3)
    pos_embed = np.concatenate([emb_d, emb_h, emb_w], axis=1)
    pos_embed = pos_embed[:, :embed_dim]
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


@beartype.beartype
def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    returns:
        pos_embed: [grid_size*grid_size, embed_dim] (w/o cls_token)
                or [1+grid_size*grid_size, embed_dim] (w/ cls_token)
    """
    grid_h = np.arange(grid_size, dtype=float)
    grid_w = np.arange(grid_size, dtype=float)
    grid_w, grid_h = np.meshgrid(
        grid_w, grid_h
    )  # order of meshgrid is very important for indexing as [h, w]

    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_h)  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_w)  # (H*W, D/2)
    pos_embed = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    embed_dim: output dimension for each position
    grid_size: int of the grid length
    returns:
        pos_embed: [grid_size, embed_dim] (w/o cls_token)
                or [1+grid_size, embed_dim] (w/ cls_token)
    """
    grid = np.arange(grid_size, dtype=float)
    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    returns: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=float)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum("m,d->md", pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out)  # (M, D/2)
    emb_cos = np.cos(out)  # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


@beartype.beartype
def trunc_normal_(
    tensor: Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0
) -> Tensor:
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


@beartype.beartype
class PatchEmbed(torch.nn.Module):
    """
    Image to Patch Embedding
    """

    def __init__(self, patch_size: int = 16, in_chans: int = 3, embed_dim: int = 768):
        super().__init__()
        self.patch_size = patch_size
        self.proj = torch.nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


@beartype.beartype
class PatchEmbed3D(torch.nn.Module):
    """
    Image to Patch Embedding
    """

    def __init__(
        self,
        patch_size: int = 16,
        tubelet_size: int = 2,
        in_chans: int = 3,
        embed_dim: int = 768,
    ):
        super().__init__()
        self.patch_size = patch_size
        self.tubelet_size = tubelet_size

        self.proj = torch.nn.Conv3d(
            in_channels=in_chans,
            out_channels=embed_dim,
            kernel_size=(tubelet_size, patch_size, patch_size),
            stride=(tubelet_size, patch_size, patch_size),
        )

    def forward(self, x):
        B, C, T, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


@beartype.beartype
class MLP(torch.nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=torch.nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = torch.nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = torch.nn.Linear(hidden_features, out_features)
        self.drop = torch.nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


@beartype.beartype
class Attention(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        qk_scale=None,
        attn_drop: float = 0.0,
        proj_drop: float = 0.0,
        use_sdpa: bool = True,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5
        self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = torch.nn.Dropout(attn_drop)
        self.proj = torch.nn.Linear(dim, dim)
        self.proj_drop_prob = proj_drop
        self.proj_drop = torch.nn.Dropout(proj_drop)
        self.use_sdpa = use_sdpa

    def forward(self, x, mask=None):
        B, N, C = x.shape
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, C // self.num_heads)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, num_heads, N, D]

        if self.use_sdpa:
            x = torch.nn.functional.scaled_dot_product_attention(
                q, k, v, dropout_p=self.proj_drop_prob
            )
            attn = None
        else:
            attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, D, D]
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn


@beartype.beartype
class Block(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias=False,
        qk_scale=None,
        drop: float = 0.0,
        attn_drop: float = 0.0,
        act_layer=torch.nn.GELU,
        norm_layer=torch.nn.LayerNorm,
        grid_size=None,
        grid_depth=None,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
        )

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
        )

    def forward(self, x, return_attention=False, mask=None):
        y, attn = self.attn(self.norm1(x), mask=mask)
        if return_attention:
            return attn
        x = x + y
        x = x + self.mlp(self.norm2(x))
        return x


@beartype.beartype
class VisionTransformer(torch.nn.Module):
    """Vision Transformer"""

    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        num_frames: int = 1,
        tubelet_size: int = 2,
        in_chans: int = 3,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        qk_scale=None,
        drop_rate: float = 0.0,
        attn_drop_rate: float = 0.0,
        norm_layer=torch.nn.LayerNorm,
        init_std: float = 0.02,
        out_layers=None,
        uniform_power=False,
        **kwargs,
    ):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.out_layers = out_layers

        self.input_size = img_size
        self.patch_size = patch_size

        self.num_frames = num_frames
        self.tubelet_size = tubelet_size
        self.is_video = num_frames > 1

        grid_size = self.input_size // self.patch_size
        grid_depth = self.num_frames // self.tubelet_size

        # Tokenize pixels with convolution
        if self.is_video:
            self.patch_embed = PatchEmbed3D(
                patch_size=patch_size,
                tubelet_size=tubelet_size,
                in_chans=in_chans,
                embed_dim=embed_dim,
            )
            self.num_patches = (
                (num_frames // tubelet_size)
                * (img_size // patch_size)
                * (img_size // patch_size)
            )
        else:
            self.patch_embed = PatchEmbed(
                patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim
            )
            self.num_patches = (img_size // patch_size) * (img_size // patch_size)

        # Position embedding
        self.uniform_power = uniform_power
        self.pos_embed = None
        self.pos_embed = torch.nn.Parameter(
            torch.zeros(1, self.num_patches, embed_dim), requires_grad=False
        )

        # Attention Blocks
        self.blocks = torch.nn.ModuleList([
            Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                act_layer=torch.nn.GELU,
                grid_size=grid_size,
                grid_depth=grid_depth,
                attn_drop=attn_drop_rate,
                norm_layer=norm_layer,
            )
            for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)

        # ------ initialize weights
        if self.pos_embed is not None:
            self._init_pos_embed(self.pos_embed.data)  # sincos pos-embed
        self.init_std = init_std
        self.apply(self._init_weights)
        self._rescale_blocks()

    def _init_pos_embed(self, pos_embed):
        embed_dim = pos_embed.size(-1)
        grid_size = self.input_size // self.patch_size
        if self.is_video:
            grid_depth = self.num_frames // self.tubelet_size
            sincos = get_3d_sincos_pos_embed(
                embed_dim,
                grid_size,
                grid_depth,
                cls_token=False,
                uniform_power=self.uniform_power,
            )
        else:
            sincos = get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False)
        pos_embed.copy_(torch.from_numpy(sincos).float().unsqueeze(0))

    def _init_weights(self, m):
        if isinstance(m, torch.nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, torch.nn.Linear) and m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.LayerNorm):
            torch.nn.init.constant_(m.bias, 0)
            torch.nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, torch.nn.Conv2d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.Conv3d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)

    def _rescale_blocks(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def get_num_layers(self):
        return len(self.blocks)

    def no_weight_decay(self):
        return {}

    def forward(self, x):
        """
        :param x: input image/video
        """

        # Tokenize input
        pos_embed = self.pos_embed
        if pos_embed is not None:
            pos_embed = self.interpolate_pos_encoding(x, pos_embed)
        x = self.patch_embed(x)
        if pos_embed is not None:
            x += pos_embed
        B, N, D = x.shape

        # Fwd prop
        outs = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if self.out_layers is not None and i in self.out_layers:
                outs.append(self.norm(x))

        if self.out_layers is not None:
            return outs

        if self.norm is not None:
            x = self.norm(x)

        return x

    def interpolate_pos_encoding(self, x, pos_embed):
        _, N, dim = pos_embed.shape

        if self.is_video:
            # If pos_embed already corret size, just return
            _, _, T, H, W = x.shape
            if H == self.input_size and W == self.input_size and T == self.num_frames:
                return pos_embed

            # Convert depth, height, width of input to be measured in patches
            # instead of pixels/frames
            T = T // self.tubelet_size
            H = H // self.patch_size
            W = W // self.patch_size

            # Compute the initialized shape of the positional embedding measured
            # in patches
            N_t = self.num_frames // self.tubelet_size
            N_h = N_w = self.input_size // self.patch_size
            assert N_h * N_w * N_t == N, "Positional embedding initialized incorrectly"

            # Compute scale factor for spatio-temporal interpolation
            scale_factor = (T / N_t, H / N_h, W / N_w)

            pos_embed = torch.nn.functional.interpolate(
                pos_embed.reshape(1, N_t, N_h, N_w, dim).permute(0, 4, 1, 2, 3),
                scale_factor=scale_factor,
                mode="trilinear",
            )
            pos_embed = pos_embed.permute(0, 2, 3, 4, 1).view(1, -1, dim)
            return pos_embed

        else:
            # If pos_embed already corret size, just return
            _, _, H, W = x.shape
            if H == self.input_size and W == self.input_size:
                return pos_embed

            # Compute scale factor for spatial interpolation
            npatch = (H // self.patch_size) * (W // self.patch_size)
            scale_factor = math.sqrt(npatch / N)

            pos_embed = torch.nn.functional.interpolate(
                pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                    0, 3, 1, 2
                ),
                scale_factor=scale_factor,
                mode="bicubic",
            )
            pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
            return pos_embed


def vit_large(patch_size: int = 16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4.0,
        qkv_bias=True,
        norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model


def vit_huge(patch_size: int = 16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size,
        embed_dim=1280,
        depth=32,
        num_heads=16,
        mlp_ratio=4.0,
        qkv_bias=True,
        norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model


@jaxtyped(typechecker=beartype.beartype)
class VJEPA(registry.VisionBackbone):
    def __init__(self, ckpt: str, **_):
        super().__init__()
        self.n_frames = 16
        if ckpt == "vitl16":
            size = 224
            vit = vit_large(img_size=size, num_frames=self.n_frames)
        elif ckpt == "vith16":
            size = 224
            vit = vit_huge(img_size=size, num_frames=self.n_frames)
            size = 224
        elif ckpt == "vith16-384":
            size = 384
            vit = vit_huge(img_size=size, num_frames=self.n_frames)
        else:
            raise ValueError(f"ckpt '{ckpt}' not recognized.")
        self.backbone = vit
        self.size = size

        ckpt_url = f"https://dl.fbaipublicfiles.com/jepa/{ckpt}/{ckpt}.pth.tar"
        state = torch.load(download(ckpt_url), map_location="cpu")
        state = {
            k.replace("module.", ""): v for k, v in state["target_encoder"].items()
        }
        self.load_state_dict(state)

    def img_encode(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> registry.EncodedImgBatch:
        x = einops.repeat(batch, "b c w h -> b c f w h", f=self.n_frames)
        x = self.backbone(x)

        # Reshape to (b, D, N, C) then average over D=8
        depth = self.n_frames // 2  # 8
        x = einops.rearrange(x, "b (d n) c -> b d n c", d=depth)
        x = x.mean(dim=1)

        # Return image features.
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

    def make_img_transform(self):
        import torch
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=self.size),
            v2.CenterCrop(self.size),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])


@beartype.beartype
def download(url: str, *, force: bool = False) -> pathlib.Path:
    root = pathlib.Path(helpers.get_cache_dir()) / "vjepa"
    root.mkdir(parents=True, exist_ok=True)
    fname = root / os.path.basename(url)
    if not fname.exists() or force:
        with requests.get(url, stream=True) as r, open(fname, "wb") as f:
            r.raise_for_status()
            for chunk in r.iter_content(1 << 20):
                f.write(chunk)
    return fname

