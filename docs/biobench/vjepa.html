<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>biobench.vjepa API documentation</title>
<meta name="description" content="V‑JEPA *frozen‑feature* backbone port …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>biobench.vjepa</code></h1>
</header>
<section id="section-intro">
<h1 id="vjepa-frozenfeature-backbone-port">V‑JEPA <em>frozen‑feature</em> backbone port</h1>
<p>This file is a self‑contained re‑implementation of the encoder used in <em>V‑JEPA</em> (Meta FAIR, 2024) for frozen downstream evaluation.
The design follows the public facebookresearch/jepa reference code but strips it down to the minimum needed for image‑only tasks.</p>
<h2 id="key-implementation-choices">Key Implementation Choices</h2>
<ol>
<li>Most modules are verbatim (or lightly edited for typing/PEP‑8) copies from the upstream repo so that we do <em>not</em> depend on the unpublished PyPI package:</li>
<li>"Fake‑video" input pathway exactly like FAIR’s frozen‑image script
The official frozen image classification script (<a href="https://github.com/facebookresearch/jepa/blob/51c59d518fc63c08464af6de585f78ac0c7ed4d5/evals/image_classification_frozen/eval.py#L451-L455">https://github.com/facebookresearch/jepa/blob/51c59d518fc63c08464af6de585f78ac0c7ed4d5/evals/image_classification_frozen/eval.py#L451-L455</a>) repeats a still image along the temporal axis before feeding it to the video‑ViT. We reproduce that behaviour with</li>
</ol>
<p><code>py
x = einops.repeat(batch, "b c h w -&gt; b c f h w", f=self.n_frames)
# (b,3,16,h,w)</code></p>
<p>so the model sees a 16‑frame clip of identical images.</p>
<ol>
<li>Checkpoints live at <code>https://dl.fbaipublicfiles.com/jepa/&lt;ckpt&gt;/&lt;ckpt&gt;.pth.tar</code>. We download them into an <code>$CACHE/vjepa</code> sub‑folder (<code><a title="biobench.vjepa.download" href="#biobench.vjepa.download">download()</a></code> helper) to avoid git‑annex or HF dependencies.</li>
<li>
<p>Only the EMA target encoder (<code>state["target_encoder"]</code>) is loaded, mirroring the authors’ evaluation code.
The usual <code>module.</code> prefix is stripped so that the state dict matches our local module names.</p>
</li>
<li>
<p><code>img_encode()</code> returns both the full patch grid and a <strong>max‑pooled</strong> global descriptor (<code>x.max(dim=1).values</code>).
The attentive classifier used in the paper is <em>not</em> re‑implemented here; you can bolt your own head on top of the returned per‑patch features.</p>
</li>
</ol>
<h2 id="limitations-divergences-from-fair-reference">Limitations / divergences from FAIR reference</h2>
<ul>
<li>No mixed‑precision, distributed training or attentive probe head. This module is encoder‑only.</li>
<li>Patch/Tubelet shapes are fixed (16x16x2) and <code>num_frames</code> is pinned to 16; if you need other variants, expose them through the constructor.</li>
<li>Only the three public checkpoints (<code>vitl16</code>, <code>vith16</code>, <code>vith16-384</code>) are supported, but extending to future releases is one line in <code>__init__</code>.</li>
</ul>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="biobench.vjepa.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>url: str, *, force: bool = False) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def download(url: str, *, force: bool = False) -&gt; pathlib.Path:
    root = pathlib.Path(helpers.get_cache_dir()) / &#34;vjepa&#34;
    root.mkdir(parents=True, exist_ok=True)
    fname = root / os.path.basename(url)
    if not fname.exists() or force:
        with requests.get(url, stream=True) as r, open(fname, &#34;wb&#34;) as f:
            r.raise_for_status()
            for chunk in r.iter_content(1 &lt;&lt; 20):
                f.write(chunk)
    return fname</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="biobench.vjepa.get_1d_sincos_pos_embed"><code class="name flex">
<span>def <span class="ident">get_1d_sincos_pos_embed</span></span>(<span>embed_dim, grid_size, cls_token=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    &#34;&#34;&#34;
    embed_dim: output dimension for each position
    grid_size: int of the grid length
    returns:
        pos_embed: [grid_size, embed_dim] (w/o cls_token)
                or [1+grid_size, embed_dim] (w/ cls_token)
    &#34;&#34;&#34;
    grid = np.arange(grid_size, dtype=float)
    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed</code></pre>
</details>
<div class="desc"><p>embed_dim: output dimension for each position
grid_size: int of the grid length
returns:
pos_embed: [grid_size, embed_dim] (w/o cls_token)
or [1+grid_size, embed_dim] (w/ cls_token)</p></div>
</dd>
<dt id="biobench.vjepa.get_1d_sincos_pos_embed_from_grid"><code class="name flex">
<span>def <span class="ident">get_1d_sincos_pos_embed_from_grid</span></span>(<span>embed_dim, pos)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    &#34;&#34;&#34;
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    returns: (M, D)
    &#34;&#34;&#34;
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=float)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum(&#34;m,d-&gt;md&#34;, pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out)  # (M, D/2)
    emb_cos = np.cos(out)  # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb</code></pre>
</details>
<div class="desc"><p>embed_dim: output dimension for each position
pos: a list of positions to be encoded: size (M,)
returns: (M, D)</p></div>
</dd>
<dt id="biobench.vjepa.get_2d_sincos_pos_embed"><code class="name flex">
<span>def <span class="ident">get_2d_sincos_pos_embed</span></span>(<span>embed_dim, grid_size, cls_token=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    &#34;&#34;&#34;
    grid_size: int of the grid height and width
    returns:
        pos_embed: [grid_size*grid_size, embed_dim] (w/o cls_token)
                or [1+grid_size*grid_size, embed_dim] (w/ cls_token)
    &#34;&#34;&#34;
    grid_h = np.arange(grid_size, dtype=float)
    grid_w = np.arange(grid_size, dtype=float)
    grid_w, grid_h = np.meshgrid(
        grid_w, grid_h
    )  # order of meshgrid is very important for indexing as [h, w]

    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_h)  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid_w)  # (H*W, D/2)
    pos_embed = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed</code></pre>
</details>
<div class="desc"><p>grid_size: int of the grid height and width
returns:
pos_embed: [grid_size<em>grid_size, embed_dim] (w/o cls_token)
or [1+grid_size</em>grid_size, embed_dim] (w/ cls_token)</p></div>
</dd>
<dt id="biobench.vjepa.get_3d_sincos_pos_embed"><code class="name flex">
<span>def <span class="ident">get_3d_sincos_pos_embed</span></span>(<span>embed_dim, grid_size, grid_depth, cls_token=False, uniform_power=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def get_3d_sincos_pos_embed(
    embed_dim, grid_size, grid_depth, cls_token=False, uniform_power=False
):
    &#34;&#34;&#34;
    grid_size: int of the grid height and width
    grid_depth: int of the grid depth
    returns:
        pos_embed: [grid_depth*grid_size*grid_size, embed_dim] (w/o cls_token)
                or [1+grid_depth*grid_size*grid_size, embed_dim] (w/ cls_token)
    &#34;&#34;&#34;
    grid_d = np.arange(grid_depth, dtype=float)
    grid_h = np.arange(grid_size, dtype=float)
    grid_w = np.arange(grid_size, dtype=float)
    grid_h, grid_d, grid_w = np.meshgrid(
        grid_h, grid_d, grid_w
    )  # order of meshgrid is very important for indexing as [d,h,w]

    if not uniform_power:
        h_embed_dim = embed_dim // 4
        w_embed_dim = embed_dim // 4
        d_embed_dim = embed_dim // 2
    else:
        h_embed_dim = w_embed_dim = d_embed_dim = int(np.ceil(embed_dim / 6) * 2)

    emb_h = get_1d_sincos_pos_embed_from_grid(h_embed_dim, grid_h)  # (T*H*W, D1)
    emb_w = get_1d_sincos_pos_embed_from_grid(w_embed_dim, grid_w)  # (T*H*W, D2)
    emb_d = get_1d_sincos_pos_embed_from_grid(d_embed_dim, grid_d)  # (T*H*W, D3)
    pos_embed = np.concatenate([emb_d, emb_h, emb_w], axis=1)
    pos_embed = pos_embed[:, :embed_dim]
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed</code></pre>
</details>
<div class="desc"><p>grid_size: int of the grid height and width
grid_depth: int of the grid depth
returns:
pos_embed: [grid_depth<em>grid_size</em>grid_size, embed_dim] (w/o cls_token)
or [1+grid_depth<em>grid_size</em>grid_size, embed_dim] (w/ cls_token)</p></div>
</dd>
<dt id="biobench.vjepa.trunc_normal_"><code class="name flex">
<span>def <span class="ident">trunc_normal_</span></span>(<span>tensor: torch.Tensor,<br>mean: float = 0.0,<br>std: float = 1.0,<br>a: float = -2.0,<br>b: float = 2.0) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def trunc_normal_(
    tensor: Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0
) -&gt; Tensor:
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="biobench.vjepa.vit_huge"><code class="name flex">
<span>def <span class="ident">vit_huge</span></span>(<span>patch_size: int = 16, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vit_huge(patch_size: int = 16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size,
        embed_dim=1280,
        depth=32,
        num_heads=16,
        mlp_ratio=4.0,
        qkv_bias=True,
        norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="biobench.vjepa.vit_large"><code class="name flex">
<span>def <span class="ident">vit_large</span></span>(<span>patch_size: int = 16, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vit_large(patch_size: int = 16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4.0,
        qkv_bias=True,
        norm_layer=functools.partial(torch.nn.LayerNorm, eps=1e-6),
        **kwargs,
    )
    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="biobench.vjepa.Attention"><code class="flex name class">
<span>class <span class="ident">Attention</span></span>
<span>(</span><span>dim: int,<br>num_heads: int = 8,<br>qkv_bias: bool = False,<br>qk_scale=None,<br>attn_drop: float = 0.0,<br>proj_drop: float = 0.0,<br>use_sdpa: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class Attention(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        qk_scale=None,
        attn_drop: float = 0.0,
        proj_drop: float = 0.0,
        use_sdpa: bool = True,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5
        self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = torch.nn.Dropout(attn_drop)
        self.proj = torch.nn.Linear(dim, dim)
        self.proj_drop_prob = proj_drop
        self.proj_drop = torch.nn.Dropout(proj_drop)
        self.use_sdpa = use_sdpa

    def forward(self, x, mask=None):
        B, N, C = x.shape
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, C // self.num_heads)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, num_heads, N, D]

        if self.use_sdpa:
            with torch.backends.cuda.sdp_kernel():
                x = torch.nn.functional.scaled_dot_product_attention(
                    q, k, v, dropout_p=self.proj_drop_prob
                )
                attn = None
        else:
            attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, D, D]
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="biobench.vjepa.Attention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, mask=None) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, mask=None):
    B, N, C = x.shape
    qkv = (
        self.qkv(x)
        .reshape(B, N, 3, self.num_heads, C // self.num_heads)
        .permute(2, 0, 3, 1, 4)
    )
    q, k, v = qkv[0], qkv[1], qkv[2]  # [B, num_heads, N, D]

    if self.use_sdpa:
        with torch.backends.cuda.sdp_kernel():
            x = torch.nn.functional.scaled_dot_product_attention(
                q, k, v, dropout_p=self.proj_drop_prob
            )
            attn = None
    else:
        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, D, D]
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = attn @ v
    x = x.transpose(1, 2).reshape(B, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x, attn</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="biobench.vjepa.Block"><code class="flex name class">
<span>class <span class="ident">Block</span></span>
<span>(</span><span>dim: int,<br>num_heads: int,<br>mlp_ratio: float = 4.0,<br>qkv_bias=False,<br>qk_scale=None,<br>drop: float = 0.0,<br>attn_drop: float = 0.0,<br>act_layer=torch.nn.modules.activation.GELU,<br>norm_layer=torch.nn.modules.normalization.LayerNorm,<br>grid_size=None,<br>grid_depth=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class Block(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias=False,
        qk_scale=None,
        drop: float = 0.0,
        attn_drop: float = 0.0,
        act_layer=torch.nn.GELU,
        norm_layer=torch.nn.LayerNorm,
        grid_size=None,
        grid_depth=None,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
        )

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
        )

    def forward(self, x, return_attention=False, mask=None):
        y, attn = self.attn(self.norm1(x), mask=mask)
        if return_attention:
            return attn
        x = x + y
        x = x + self.mlp(self.norm2(x))
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="biobench.vjepa.Block.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, return_attention=False, mask=None) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, return_attention=False, mask=None):
    y, attn = self.attn(self.norm1(x), mask=mask)
    if return_attention:
        return attn
    x = x + y
    x = x + self.mlp(self.norm2(x))
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="biobench.vjepa.MLP"><code class="flex name class">
<span>class <span class="ident">MLP</span></span>
<span>(</span><span>in_features,<br>hidden_features=None,<br>out_features=None,<br>act_layer=torch.nn.modules.activation.GELU,<br>drop=0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class MLP(torch.nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=torch.nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = torch.nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = torch.nn.Linear(hidden_features, out_features)
        self.drop = torch.nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="biobench.vjepa.MLP.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop(x)
    x = self.fc2(x)
    x = self.drop(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="biobench.vjepa.PatchEmbed"><code class="flex name class">
<span>class <span class="ident">PatchEmbed</span></span>
<span>(</span><span>patch_size: int = 16, in_chans: int = 3, embed_dim: int = 768)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class PatchEmbed(torch.nn.Module):
    &#34;&#34;&#34;
    Image to Patch Embedding
    &#34;&#34;&#34;

    def __init__(self, patch_size: int = 16, in_chans: int = 3, embed_dim: int = 768):
        super().__init__()
        self.patch_size = patch_size
        self.proj = torch.nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x</code></pre>
</details>
<div class="desc"><p>Image to Patch Embedding</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="biobench.vjepa.PatchEmbed.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    B, C, H, W = x.shape
    x = self.proj(x).flatten(2).transpose(1, 2)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="biobench.vjepa.PatchEmbed3D"><code class="flex name class">
<span>class <span class="ident">PatchEmbed3D</span></span>
<span>(</span><span>patch_size: int = 16,<br>tubelet_size: int = 2,<br>in_chans: int = 3,<br>embed_dim: int = 768)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class PatchEmbed3D(torch.nn.Module):
    &#34;&#34;&#34;
    Image to Patch Embedding
    &#34;&#34;&#34;

    def __init__(
        self,
        patch_size: int = 16,
        tubelet_size: int = 2,
        in_chans: int = 3,
        embed_dim: int = 768,
    ):
        super().__init__()
        self.patch_size = patch_size
        self.tubelet_size = tubelet_size

        self.proj = torch.nn.Conv3d(
            in_channels=in_chans,
            out_channels=embed_dim,
            kernel_size=(tubelet_size, patch_size, patch_size),
            stride=(tubelet_size, patch_size, patch_size),
        )

    def forward(self, x: Float[Tensor, &#34;B C T H W&#34;]):
        B, C, T, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x</code></pre>
</details>
<div class="desc"><p>Image to Patch Embedding</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="biobench.vjepa.PatchEmbed3D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: jaxtyping.Float[Tensor, 'B C T H W']) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: Float[Tensor, &#34;B C T H W&#34;]):
    B, C, T, H, W = x.shape
    x = self.proj(x).flatten(2).transpose(1, 2)
    return x</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="biobench.vjepa.VJEPA"><code class="flex name class">
<span>class <span class="ident">VJEPA</span></span>
<span>(</span><span>ckpt: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class VJEPA(registry.VisionBackbone):
    def __init__(self, ckpt: str):
        super().__init__()
        self.n_frames = 16
        if ckpt == &#34;vitl16&#34;:
            size = 224
            vit = vit_large(img_size=size, num_frames=self.n_frames)
        elif ckpt == &#34;vith16&#34;:
            size = 224
            vit = vit_huge(img_size=size, num_frames=self.n_frames)
            size = 224
        elif ckpt == &#34;vith16-384&#34;:
            size = 384
            vit = vit_huge(img_size=size, num_frames=self.n_frames)
        else:
            raise ValueError(f&#34;ckpt &#39;{ckpt}&#39; not recognized.&#34;)
        self.backbone = vit
        self.size = size

        ckpt_url = f&#34;https://dl.fbaipublicfiles.com/jepa/{ckpt}/{ckpt}.pth.tar&#34;
        state = torch.load(download(ckpt_url), map_location=&#34;cpu&#34;)
        state = {
            k.replace(&#34;module.&#34;, &#34;&#34;): v for k, v in state[&#34;target_encoder&#34;].items()
        }
        self.load_state_dict(state)

    def img_encode(
        self, batch: Float[Tensor, &#34;batch 3 width height&#34;]
    ) -&gt; registry.EncodedImgBatch:
        x = einops.repeat(batch, &#34;b c w h -&gt; b c f w h&#34;, f=self.n_frames)
        x = self.backbone(x)

        # Reshape to (b, D, N, C) then average over D=8
        depth = self.n_frames // 2  # 8
        x = einops.rearrange(x, &#34;b (d n) c -&gt; b d n c&#34;, d=depth)
        x = x.mean(dim=1)

        # Return image features.
        return registry.EncodedImgBatch(x.max(dim=1).values, x)

    def make_img_transform(self):
        import torch
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=self.size),
            v2.CenterCrop(self.size),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])</code></pre>
</details>
<div class="desc"><p>A frozen vision model that embeds batches of images into batches of vectors.</p>
<p>To add new models to the benchmark, you can simply create a new class that satisfies this interface and register it.
See <code><a title="biobench.registry" href="registry.html">biobench.registry</a></code> for a tutorial on adding new vision backbones.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="biobench.registry.VisionBackbone" href="registry.html#biobench.registry.VisionBackbone">VisionBackbone</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="biobench.registry.VisionBackbone" href="registry.html#biobench.registry.VisionBackbone">VisionBackbone</a></b></code>:
<ul class="hlist">
<li><code><a title="biobench.registry.VisionBackbone.img_encode" href="registry.html#biobench.registry.VisionBackbone.img_encode">img_encode</a></code></li>
<li><code><a title="biobench.registry.VisionBackbone.make_img_transform" href="registry.html#biobench.registry.VisionBackbone.make_img_transform">make_img_transform</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="biobench.vjepa.VisionTransformer"><code class="flex name class">
<span>class <span class="ident">VisionTransformer</span></span>
<span>(</span><span>img_size: int = 224,<br>patch_size: int = 16,<br>num_frames: int = 1,<br>tubelet_size: int = 2,<br>in_chans: int = 3,<br>embed_dim: int = 768,<br>depth: int = 12,<br>num_heads: int = 12,<br>mlp_ratio: float = 4.0,<br>qkv_bias: bool = True,<br>qk_scale=None,<br>drop_rate: float = 0.0,<br>attn_drop_rate: float = 0.0,<br>norm_layer=torch.nn.modules.normalization.LayerNorm,<br>init_std: float = 0.02,<br>out_layers=None,<br>uniform_power=False,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class VisionTransformer(torch.nn.Module):
    &#34;&#34;&#34;Vision Transformer&#34;&#34;&#34;

    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        num_frames: int = 1,
        tubelet_size: int = 2,
        in_chans: int = 3,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        qk_scale=None,
        drop_rate: float = 0.0,
        attn_drop_rate: float = 0.0,
        norm_layer=torch.nn.LayerNorm,
        init_std: float = 0.02,
        out_layers=None,
        uniform_power=False,
        **kwargs,
    ):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.out_layers = out_layers

        self.input_size = img_size
        self.patch_size = patch_size

        self.num_frames = num_frames
        self.tubelet_size = tubelet_size
        self.is_video = num_frames &gt; 1

        grid_size = self.input_size // self.patch_size
        grid_depth = self.num_frames // self.tubelet_size

        # Tokenize pixels with convolution
        if self.is_video:
            self.patch_embed = PatchEmbed3D(
                patch_size=patch_size,
                tubelet_size=tubelet_size,
                in_chans=in_chans,
                embed_dim=embed_dim,
            )
            self.num_patches = (
                (num_frames // tubelet_size)
                * (img_size // patch_size)
                * (img_size // patch_size)
            )
        else:
            self.patch_embed = PatchEmbed(
                patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim
            )
            self.num_patches = (img_size // patch_size) * (img_size // patch_size)

        # Position embedding
        self.uniform_power = uniform_power
        self.pos_embed = None
        self.pos_embed = torch.nn.Parameter(
            torch.zeros(1, self.num_patches, embed_dim), requires_grad=False
        )

        # Attention Blocks
        self.blocks = torch.nn.ModuleList([
            Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                act_layer=torch.nn.GELU,
                grid_size=grid_size,
                grid_depth=grid_depth,
                attn_drop=attn_drop_rate,
                norm_layer=norm_layer,
            )
            for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)

        # ------ initialize weights
        if self.pos_embed is not None:
            self._init_pos_embed(self.pos_embed.data)  # sincos pos-embed
        self.init_std = init_std
        self.apply(self._init_weights)
        self._rescale_blocks()

    def _init_pos_embed(self, pos_embed):
        embed_dim = pos_embed.size(-1)
        grid_size = self.input_size // self.patch_size
        if self.is_video:
            grid_depth = self.num_frames // self.tubelet_size
            sincos = get_3d_sincos_pos_embed(
                embed_dim,
                grid_size,
                grid_depth,
                cls_token=False,
                uniform_power=self.uniform_power,
            )
        else:
            sincos = get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False)
        pos_embed.copy_(torch.from_numpy(sincos).float().unsqueeze(0))

    def _init_weights(self, m):
        if isinstance(m, torch.nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, torch.nn.Linear) and m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.LayerNorm):
            torch.nn.init.constant_(m.bias, 0)
            torch.nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, torch.nn.Conv2d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.Conv3d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)

    def _rescale_blocks(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def get_num_layers(self):
        return len(self.blocks)

    def no_weight_decay(self):
        return {}

    def forward(self, x):
        &#34;&#34;&#34;
        :param x: input image/video
        &#34;&#34;&#34;

        # Tokenize input
        pos_embed = self.pos_embed
        if pos_embed is not None:
            pos_embed = self.interpolate_pos_encoding(x, pos_embed)
        x = self.patch_embed(x)
        if pos_embed is not None:
            x += pos_embed
        B, N, D = x.shape

        # Fwd prop
        outs = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if self.out_layers is not None and i in self.out_layers:
                outs.append(self.norm(x))

        if self.out_layers is not None:
            return outs

        if self.norm is not None:
            x = self.norm(x)

        return x

    def interpolate_pos_encoding(self, x, pos_embed):
        _, N, dim = pos_embed.shape

        if self.is_video:
            # If pos_embed already corret size, just return
            _, _, T, H, W = x.shape
            if H == self.input_size and W == self.input_size and T == self.num_frames:
                return pos_embed

            # Convert depth, height, width of input to be measured in patches
            # instead of pixels/frames
            T = T // self.tubelet_size
            H = H // self.patch_size
            W = W // self.patch_size

            # Compute the initialized shape of the positional embedding measured
            # in patches
            N_t = self.num_frames // self.tubelet_size
            N_h = N_w = self.input_size // self.patch_size
            assert N_h * N_w * N_t == N, &#34;Positional embedding initialized incorrectly&#34;

            # Compute scale factor for spatio-temporal interpolation
            scale_factor = (T / N_t, H / N_h, W / N_w)

            pos_embed = torch.nn.functional.interpolate(
                pos_embed.reshape(1, N_t, N_h, N_w, dim).permute(0, 4, 1, 2, 3),
                scale_factor=scale_factor,
                mode=&#34;trilinear&#34;,
            )
            pos_embed = pos_embed.permute(0, 2, 3, 4, 1).view(1, -1, dim)
            return pos_embed

        else:
            # If pos_embed already corret size, just return
            _, _, H, W = x.shape
            if H == self.input_size and W == self.input_size:
                return pos_embed

            # Compute scale factor for spatial interpolation
            npatch = (H // self.patch_size) * (W // self.patch_size)
            scale_factor = math.sqrt(npatch / N)

            pos_embed = torch.nn.functional.interpolate(
                pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                    0, 3, 1, 2
                ),
                scale_factor=scale_factor,
                mode=&#34;bicubic&#34;,
            )
            pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
            return pos_embed</code></pre>
</details>
<div class="desc"><p>Vision Transformer</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="biobench.vjepa.VisionTransformer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;
    :param x: input image/video
    &#34;&#34;&#34;

    # Tokenize input
    pos_embed = self.pos_embed
    if pos_embed is not None:
        pos_embed = self.interpolate_pos_encoding(x, pos_embed)
    x = self.patch_embed(x)
    if pos_embed is not None:
        x += pos_embed
    B, N, D = x.shape

    # Fwd prop
    outs = []
    for i, blk in enumerate(self.blocks):
        x = blk(x)
        if self.out_layers is not None and i in self.out_layers:
            outs.append(self.norm(x))

    if self.out_layers is not None:
        return outs

    if self.norm is not None:
        x = self.norm(x)

    return x</code></pre>
</details>
<div class="desc"><p>:param x: input image/video</p></div>
</dd>
<dt id="biobench.vjepa.VisionTransformer.get_num_layers"><code class="name flex">
<span>def <span class="ident">get_num_layers</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_num_layers(self):
    return len(self.blocks)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="biobench.vjepa.VisionTransformer.interpolate_pos_encoding"><code class="name flex">
<span>def <span class="ident">interpolate_pos_encoding</span></span>(<span>self, x, pos_embed)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpolate_pos_encoding(self, x, pos_embed):
    _, N, dim = pos_embed.shape

    if self.is_video:
        # If pos_embed already corret size, just return
        _, _, T, H, W = x.shape
        if H == self.input_size and W == self.input_size and T == self.num_frames:
            return pos_embed

        # Convert depth, height, width of input to be measured in patches
        # instead of pixels/frames
        T = T // self.tubelet_size
        H = H // self.patch_size
        W = W // self.patch_size

        # Compute the initialized shape of the positional embedding measured
        # in patches
        N_t = self.num_frames // self.tubelet_size
        N_h = N_w = self.input_size // self.patch_size
        assert N_h * N_w * N_t == N, &#34;Positional embedding initialized incorrectly&#34;

        # Compute scale factor for spatio-temporal interpolation
        scale_factor = (T / N_t, H / N_h, W / N_w)

        pos_embed = torch.nn.functional.interpolate(
            pos_embed.reshape(1, N_t, N_h, N_w, dim).permute(0, 4, 1, 2, 3),
            scale_factor=scale_factor,
            mode=&#34;trilinear&#34;,
        )
        pos_embed = pos_embed.permute(0, 2, 3, 4, 1).view(1, -1, dim)
        return pos_embed

    else:
        # If pos_embed already corret size, just return
        _, _, H, W = x.shape
        if H == self.input_size and W == self.input_size:
            return pos_embed

        # Compute scale factor for spatial interpolation
        npatch = (H // self.patch_size) * (W // self.patch_size)
        scale_factor = math.sqrt(npatch / N)

        pos_embed = torch.nn.functional.interpolate(
            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                0, 3, 1, 2
            ),
            scale_factor=scale_factor,
            mode=&#34;bicubic&#34;,
        )
        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return pos_embed</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="biobench.vjepa.VisionTransformer.no_weight_decay"><code class="name flex">
<span>def <span class="ident">no_weight_decay</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def no_weight_decay(self):
    return {}</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul>
<li><a href="#vjepa-frozenfeature-backbone-port">V‑JEPA frozen‑feature backbone port</a><ul>
<li><a href="#key-implementation-choices">Key implementation choices</a></li>
<li><a href="#limitations-divergences-from-fair-reference">Limitations / divergences from FAIR reference</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="biobench" href="index.html">biobench</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="biobench.vjepa.download" href="#biobench.vjepa.download">download</a></code></li>
<li><code><a title="biobench.vjepa.get_1d_sincos_pos_embed" href="#biobench.vjepa.get_1d_sincos_pos_embed">get_1d_sincos_pos_embed</a></code></li>
<li><code><a title="biobench.vjepa.get_1d_sincos_pos_embed_from_grid" href="#biobench.vjepa.get_1d_sincos_pos_embed_from_grid">get_1d_sincos_pos_embed_from_grid</a></code></li>
<li><code><a title="biobench.vjepa.get_2d_sincos_pos_embed" href="#biobench.vjepa.get_2d_sincos_pos_embed">get_2d_sincos_pos_embed</a></code></li>
<li><code><a title="biobench.vjepa.get_3d_sincos_pos_embed" href="#biobench.vjepa.get_3d_sincos_pos_embed">get_3d_sincos_pos_embed</a></code></li>
<li><code><a title="biobench.vjepa.trunc_normal_" href="#biobench.vjepa.trunc_normal_">trunc_normal_</a></code></li>
<li><code><a title="biobench.vjepa.vit_huge" href="#biobench.vjepa.vit_huge">vit_huge</a></code></li>
<li><code><a title="biobench.vjepa.vit_large" href="#biobench.vjepa.vit_large">vit_large</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="biobench.vjepa.Attention" href="#biobench.vjepa.Attention">Attention</a></code></h4>
<ul class="">
<li><code><a title="biobench.vjepa.Attention.forward" href="#biobench.vjepa.Attention.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="biobench.vjepa.Block" href="#biobench.vjepa.Block">Block</a></code></h4>
<ul class="">
<li><code><a title="biobench.vjepa.Block.forward" href="#biobench.vjepa.Block.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="biobench.vjepa.MLP" href="#biobench.vjepa.MLP">MLP</a></code></h4>
<ul class="">
<li><code><a title="biobench.vjepa.MLP.forward" href="#biobench.vjepa.MLP.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="biobench.vjepa.PatchEmbed" href="#biobench.vjepa.PatchEmbed">PatchEmbed</a></code></h4>
<ul class="">
<li><code><a title="biobench.vjepa.PatchEmbed.forward" href="#biobench.vjepa.PatchEmbed.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="biobench.vjepa.PatchEmbed3D" href="#biobench.vjepa.PatchEmbed3D">PatchEmbed3D</a></code></h4>
<ul class="">
<li><code><a title="biobench.vjepa.PatchEmbed3D.forward" href="#biobench.vjepa.PatchEmbed3D.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="biobench.vjepa.VJEPA" href="#biobench.vjepa.VJEPA">VJEPA</a></code></h4>
</li>
<li>
<h4><code><a title="biobench.vjepa.VisionTransformer" href="#biobench.vjepa.VisionTransformer">VisionTransformer</a></code></h4>
<ul class="">
<li><code><a title="biobench.vjepa.VisionTransformer.forward" href="#biobench.vjepa.VisionTransformer.forward">forward</a></code></li>
<li><code><a title="biobench.vjepa.VisionTransformer.get_num_layers" href="#biobench.vjepa.VisionTransformer.get_num_layers">get_num_layers</a></code></li>
<li><code><a title="biobench.vjepa.VisionTransformer.interpolate_pos_encoding" href="#biobench.vjepa.VisionTransformer.interpolate_pos_encoding">interpolate_pos_encoding</a></code></li>
<li><code><a title="biobench.vjepa.VisionTransformer.no_weight_decay" href="#biobench.vjepa.VisionTransformer.no_weight_decay">no_weight_decay</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
