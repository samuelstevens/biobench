<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>biobench.helpers API documentation</title>
<meta name="description" content="Useful helpers for more than two tasks that don&#39;t fit anywhere else.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>biobench.helpers</code></h1>
</header>
<section id="section-intro">
<p>Useful helpers for more than two tasks that don't fit anywhere else.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="biobench.helpers.auto_batch_size"><code class="name flex">
<span>def <span class="ident">auto_batch_size</span></span>(<span>dataloader: torch.utils.data.dataloader.DataLoader,<br>*,<br>probe: Callable[[torch.Tensor], torch.Tensor],<br>schedule: collections.abc.Iterable[int] | None = None,<br>upper: int = 4096)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextlib.contextmanager
@beartype.beartype
def auto_batch_size(
    dataloader: torch.utils.data.DataLoader,
    *,
    probe: collections.abc.Callable[[torch.Tensor], torch.Tensor],
    schedule: collections.abc.Iterable[int] | None = None,
    upper: int = 4096,
):
    &#34;&#34;&#34;
    Context manager that **mutates `dataloader.batch_size` in-place** so you always run with the largest batch that fits GPU RAM.

    Parameters
    ----------
    dataloader:
        The *already constructed* loader you use in your loop. Its `batch_sampler.batch_size` attribute is patched on the fly.
    probe:
        A 1-argument callable used to test memory. Typical usage: `lambda x: backbone.img_encode(x).img_features`.
    schedule:
        An iterator of candidate batch sizes.  If `None`, use the canonical schedule.
    &#34;&#34;&#34;
    logger = logging.getLogger(&#34;auto-bsz&#34;)

    if dataloader.batch_sampler is None:
        raise ValueError(&#34;dataloader must have a batch_sampler&#34;)

    oom_signatures = (
        &#34;out of memory&#34;,
        &#34;cuda error: invalid configuration argument&#34;,  # SPM-efficient-attn OOM
    )

    dataloader.batch_sampler.batch_size = min(
        dataloader.batch_sampler.batch_size, upper
    )

    orig_bsz = int(dataloader.batch_sampler.batch_size)
    ok_bsz = orig_bsz
    good_bszs = [ok_bsz]
    schedule_iter = schedule or _default_batchsize_schedule(orig_bsz)

    torch.cuda.empty_cache()  # be nice

    t_start = time.perf_counter()

    for tried_bsz in schedule_iter:
        # quick sanity: do not create impossible 0-batch situations
        if tried_bsz &lt;= ok_bsz:
            continue

        if tried_bsz &gt; upper:
            tried_bsz = upper

        # patch sampler attr
        dataloader.batch_sampler.batch_size = tried_bsz
        logger.info(&#34;Trying batch_size=%d&#34;, tried_bsz)

        # pull ONE mini-batch, send through probe
        try:
            batch = next(iter(dataloader))
            probe(batch)  # forward only; discard output

            # If the loader produced fewer items than we asked for, we&#39;ve reached the dataset size -- any larger batch will give the same tensor, so stop growing.
            effective_bsz = infer_batch_size(batch)
            if effective_bsz is None:
                raise RuntimeError(
                    &#34;Unable to deduce batch size from probe batch; ensure it contains at least one torch.Tensor.&#34;
                )

            if effective_bsz &lt; tried_bsz:
                logger.info(
                    &#34;Dataset exhausted at %d examples (asked for %d); capping batch size.&#34;,
                    effective_bsz,
                    tried_bsz,
                )
                ok_bsz = effective_bsz
                good_bszs.append(ok_bsz)
                break

            ok_bsz = tried_bsz
            good_bszs.append(ok_bsz)
            logger.info(&#34;batch_size=%d succeeded&#34;, ok_bsz)

            # honor explicit ceiling
            if upper is not None and ok_bsz &gt;= upper:
                logger.info(&#34;Reached ok_bsz (%d) &gt;= upper (%d)&#34;, ok_bsz, upper)
                ok_bsz = upper
                break

        except RuntimeError as err:
            msg = str(err).lower()
            if any(sig in msg for sig in oom_signatures):
                logger.info(&#34;OOM at batch_size=%d; reverting to %d&#34;, tried_bsz, ok_bsz)
                torch.cuda.empty_cache()
                break
            else:
                raise

    # (re-)verify ok_bs once more in a clean context
    while good_bszs:
        ok_bsz = good_bszs.pop()
        dataloader.batch_sampler.batch_size = ok_bsz
        try:
            batch = next(iter(dataloader))
            probe(batch)
            break  # we know ok_bsz is actually good.

        except RuntimeError as err:
            if any(sig in str(err).lower() for sig in oom_signatures):
                logger.info(&#34;Still OOM at %d; trying previous candidate&#34;, ok_bsz)
            else:
                raise

    elapsed = time.perf_counter() - t_start
    logger.info(&#34;Selected batch_size %d after %.2f s&#34;, ok_bsz, elapsed)

    # Final tidy-up to avoid residual OOMs
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()  # frees cached blocks from other procs
    gc.collect()  # clears Python refs / fragments

    try:
        yield ok_bsz  # user code runs here with patched batch_size
    finally:
        # always restore original value
        dataloader.batch_sampler.batch_size = orig_bsz</code></pre>
</details>
<div class="desc"><p>Context manager that <strong>mutates <code>dataloader.batch_size</code> in-place</strong> so you always run with the largest batch that fits GPU RAM.</p>
<h2 id="parameters">Parameters</h2>
<p>dataloader:
The <em>already constructed</em> loader you use in your loop. Its <code>batch_sampler.batch_size</code> attribute is patched on the fly.
probe:
A 1-argument callable used to test memory. Typical usage: <code>lambda x: backbone.img_encode(x).img_features</code>.
schedule:
An iterator of candidate batch sizes.
If <code>None</code>, use the canonical schedule.</p></div>
</dd>
<dt id="biobench.helpers.balanced_random_sample"><code class="name flex">
<span>def <span class="ident">balanced_random_sample</span></span>(<span>labels: jaxtyping.Int[ndarray, 'n_labels'], n: int) ‑> jaxtyping.Int[ndarray, 'n']</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
def balanced_random_sample(
    labels: Int[np.ndarray, &#34; n_labels&#34;], n: int
) -&gt; Int[np.ndarray, &#34; n&#34;]:
    &#34;&#34;&#34;
    Select n random examples while balancing the number of examples per class.
    &#34;&#34;&#34;
    # Count the occurrences of each class
    class_counts = collections.Counter(labels)
    unique_classes = list(class_counts.keys())
    n_classes = len(unique_classes)

    if not n_classes:
        return np.array([], dtype=int)

    # Calculate ideal number of samples per class
    samples_per_class = n // n_classes

    # Handle remainder by allocating extra samples to random classes
    remainder = n % n_classes
    extra_samples = np.zeros(n_classes, dtype=int)
    if remainder &gt; 0:
        extra_indices = np.random.choice(n_classes, remainder, replace=False)
        extra_samples[extra_indices] = 1

    # Calculate final samples per class
    final_samples = np.array([samples_per_class] * n_classes) + extra_samples

    # Initialize result array
    selected_indices = []

    # For each class, select random samples
    for i, class_label in enumerate(unique_classes):
        # Get all indices for this class
        class_indices = np.where(labels == class_label)[0]

        # Calculate how many to take (minimum of available samples and desired samples)
        n_to_take = min(len(class_indices), final_samples[i])

        # Randomly sample without replacement
        if n_to_take &gt; 0:
            sampled_indices = np.random.choice(class_indices, n_to_take, replace=False)
            selected_indices.extend(sampled_indices)

    # If we still don&#39;t have enough samples (due to some classes having too few examples),
    # sample from the remaining examples across all classes
    if len(selected_indices) &lt; n:
        # Create a mask of already selected indices
        mask = np.ones(len(labels), dtype=bool)
        mask[selected_indices] = False
        remaining_indices = np.where(mask)[0]

        # How many more do we need?
        needed = n - len(selected_indices)

        # Sample without replacement from remaining indices
        if needed &gt; 0 and len(remaining_indices) &gt; 0:
            additional_indices = np.random.choice(
                remaining_indices, min(needed, len(remaining_indices)), replace=False
            )
            selected_indices.extend(additional_indices)

    return np.array(selected_indices, dtype=int)</code></pre>
</details>
<div class="desc"><p>Select n random examples while balancing the number of examples per class.</p></div>
</dd>
<dt id="biobench.helpers.batched_idx"><code class="name flex">
<span>def <span class="ident">batched_idx</span></span>(<span>total_size: int, batch_size: int) ‑> Iterator[tuple[int, int]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -&gt; collections.abc.Iterator[tuple[int, int]]:
    &#34;&#34;&#34;
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    &#34;&#34;&#34;
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop</code></pre>
</details>
<div class="desc"><p>Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>total_size</code></strong></dt>
<dd>total number of examples</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>maximum distance between the generated indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A generator of (int, int) tuples that can slice up a list or a tensor.</p></div>
</dd>
<dt id="biobench.helpers.bump_nofile"><code class="name flex">
<span>def <span class="ident">bump_nofile</span></span>(<span>margin: int = 512) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def bump_nofile(margin: int = 512) -&gt; None:
    &#34;&#34;&#34;
    Make RLIMIT_NOFILE.soft = RLIMIT_NOFILE.hard - margin (if that is higher than the current soft limit).  No change if margin would push soft &lt; 1. Raises RuntimeError if hard &lt;= margin.
    &#34;&#34;&#34;
    if margin &lt; 0:
        raise ValueError(&#34;margin must be non-negative&#34;)

    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)

    if hard &lt;= margin:
        raise RuntimeError(
            f&#34;hard limit ({hard}) is &lt;= margin ({margin}); ask an admin to raise the hard limit.&#34;
        )

    target_soft = hard - margin
    if soft &lt; target_soft:
        resource.setrlimit(resource.RLIMIT_NOFILE, (target_soft, hard))</code></pre>
</details>
<div class="desc"><p>Make RLIMIT_NOFILE.soft = RLIMIT_NOFILE.hard - margin (if that is higher than the current soft limit).
No change if margin would push soft &lt; 1. Raises RuntimeError if hard &lt;= margin.</p></div>
</dd>
<dt id="biobench.helpers.fs_safe"><code class="name flex">
<span>def <span class="ident">fs_safe</span></span>(<span>string: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def fs_safe(string: str) -&gt; str:
    &#34;&#34;&#34;Makes a string safe for filesystems by removing typical special characters.&#34;&#34;&#34;
    return string.replace(&#34;:&#34;, &#34;_&#34;).replace(&#34;/&#34;, &#34;_&#34;)</code></pre>
</details>
<div class="desc"><p>Makes a string safe for filesystems by removing typical special characters.</p></div>
</dd>
<dt id="biobench.helpers.get_cache_dir"><code class="name flex">
<span>def <span class="ident">get_cache_dir</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def get_cache_dir() -&gt; str:
    cache_dir = &#34;&#34;
    for var in (&#34;BIOBENCH_CACHE&#34;, &#34;HF_HOME&#34;, &#34;HF_HUB_CACHE&#34;):
        cache_dir = cache_dir or os.environ.get(var, &#34;&#34;)
    return cache_dir or &#34;.&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="biobench.helpers.infer_batch_size"><code class="name flex">
<span>def <span class="ident">infer_batch_size</span></span>(<span>batch: object) ‑> int | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def infer_batch_size(batch: object) -&gt; int | None:
    &#34;&#34;&#34;
    Return the leading dimension of the *first* tensor found inside `batch`. Works for arbitrary nested structures.
    &#34;&#34;&#34;
    if isinstance(batch, torch.Tensor):
        return batch.shape[0]

    if isinstance(batch, (list, tuple)):
        for item in batch:
            bs = infer_batch_size(item)
            if bs is not None:
                return bs

    if isinstance(batch, dict):
        for item in batch.values():
            bs = infer_batch_size(item)
            if bs is not None:
                return bs

    if dataclasses.is_dataclass(batch):
        return infer_batch_size(dataclasses.asdict(batch))

    # Fallback: inspect attributes (namedtuple, SimpleNamespace, custom)
    if hasattr(batch, &#34;__dict__&#34;):
        return infer_batch_size(vars(batch))

    return None</code></pre>
</details>
<div class="desc"><p>Return the leading dimension of the <em>first</em> tensor found inside <code>batch</code>. Works for arbitrary nested structures.</p></div>
</dd>
<dt id="biobench.helpers.warn_if_nfs"><code class="name flex">
<span>def <span class="ident">warn_if_nfs</span></span>(<span>path: str | os.PathLike)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def warn_if_nfs(path: str | os.PathLike):
    &#34;&#34;&#34;
    If *path* is on an NFS mount, emit a RuntimeWarning.

    Works on Linux (/proc/mounts) and macOS/BSD (`mount` CLI); silently returns on other OSes or if detection fails.
    &#34;&#34;&#34;
    p = pathlib.Path(path).resolve()

    # Linux: /proc/self/mountinfo
    if sys.platform.startswith(&#34;linux&#34;):
        try:
            with open(&#34;/proc/self/mountinfo&#34;) as fd:
                entries = [line.split() for line in fd]
            # fields: 4= mount point, - separator -, fstype
            mounts = {fields[4]: fields[-3] for fields in entries}
        except Exception:
            return

    # macOS / BSD: `mount`
    elif sys.platform in {&#34;darwin&#34;, &#34;freebsd&#34;}:
        try:
            out = subprocess.check_output([&#34;mount&#34;, &#34;-p&#34;], text=True)
            mounts = {}
            for line in out.splitlines():
                mp, _dev, fstype, *_ = line.split()  # mount-point, ...
                mounts[mp] = fstype
        except Exception:
            return
    else:
        return  # unsupported OS

    # find longest mount-point prefix of *p*
    mount_point = max(
        (mp for mp in mounts if p.is_relative_to(mp) or mp == &#34;/&#34;),
        key=len,
        default=&#34;/&#34;,
    )
    if mounts.get(mount_point) in NFS_TYPES:
        warnings.warn(
            f&#34;SQLite database &#39;{path}&#39; appears to be on an NFS mount (fs type: {mounts[mount_point]}). Concurrent writers over NFS can corrupt the journal; consider using a local SSD or tmpfs instead.&#34;,
            RuntimeWarning,
            stacklevel=2,
        )</code></pre>
</details>
<div class="desc"><p>If <em>path</em> is on an NFS mount, emit a RuntimeWarning.</p>
<p>Works on Linux (/proc/mounts) and macOS/BSD (<code>mount</code> CLI); silently returns on other OSes or if detection fails.</p></div>
</dd>
<dt id="biobench.helpers.write_hparam_sweep_plot"><code class="name flex">
<span>def <span class="ident">write_hparam_sweep_plot</span></span>(<span>task: str,<br>model: str,<br>clf,<br>x: str = 'param_ridgeclassifier__alpha',<br>y: str = 'mean_test_score') ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def write_hparam_sweep_plot(
    task: str,
    model: str,
    clf,
    x: str = &#34;param_ridgeclassifier__alpha&#34;,
    y: str = &#34;mean_test_score&#34;,
) -&gt; str:
    import matplotlib.pyplot as plt
    import polars as pl

    if not hasattr(clf, &#34;cv_results_&#34;):
        return &#34;&#34;

    df = pl.DataFrame(clf.cv_results_)

    fig, ax = plt.subplots()

    if &#34;n_resources&#34; in df.columns:
        for n_resources in df.get_column(&#34;n_resources&#34;).unique().sort():
            ax.scatter(
                x=df.filter(pl.col(&#34;n_resources&#34;) == n_resources)[x],
                y=df.filter(pl.col(&#34;n_resources&#34;) == n_resources)[y],
                label=f&#34;{n_resources} ex.&#34;,
            )
        fig.legend()
    else:
        ax.scatter(x=df[x], y=df[y])

    ax.set_xlabel(x)
    ax.set_ylabel(y)
    ax.set_xscale(&#34;log&#34;)
    ax.set_title(model)

    fig.tight_layout()
    filepath = os.path.join(&#34;logs&#34;, f&#34;{task}_{fs_safe(model)}_hparam.png&#34;)
    fig.savefig(filepath)
    return filepath</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="biobench.helpers.progress"><code class="flex name class">
<span>class <span class="ident">progress</span></span>
<span>(</span><span>it, *, every: int = 10, desc: str = 'progress')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = &#34;progress&#34;):
        &#34;&#34;&#34;
        Wraps an iterable with a logger like tqdm but doesn&#39;t use any control codes to manipulate a progress bar, which doesn&#39;t work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
        &#34;&#34;&#34;
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)

    def __iter__(self):
        start = time.time()
        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if isinstance(self.it, collections.abc.Sized):
                    pred_min = (len(self) - (i + 1)) / per_min
                    self.logger.info(
                        &#34;%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)&#34;,
                        i + 1,
                        len(self),
                        (i + 1) / len(self) * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info(&#34;%d/? | %.1f it/m&#34;, i + 1, per_min)

    def __len__(self) -&gt; int:
        return len(self.it)</code></pre>
</details>
<div class="desc"><p>Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>it</code></strong></dt>
<dd>Iterable to wrap.</dd>
<dt><strong><code>every</code></strong></dt>
<dd>How many iterations between logging progress.</dd>
<dt><strong><code>desc</code></strong></dt>
<dd>What to name the logger.</dd>
</dl></div>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="biobench" href="index.html">biobench</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="biobench.helpers.auto_batch_size" href="#biobench.helpers.auto_batch_size">auto_batch_size</a></code></li>
<li><code><a title="biobench.helpers.balanced_random_sample" href="#biobench.helpers.balanced_random_sample">balanced_random_sample</a></code></li>
<li><code><a title="biobench.helpers.batched_idx" href="#biobench.helpers.batched_idx">batched_idx</a></code></li>
<li><code><a title="biobench.helpers.bump_nofile" href="#biobench.helpers.bump_nofile">bump_nofile</a></code></li>
<li><code><a title="biobench.helpers.fs_safe" href="#biobench.helpers.fs_safe">fs_safe</a></code></li>
<li><code><a title="biobench.helpers.get_cache_dir" href="#biobench.helpers.get_cache_dir">get_cache_dir</a></code></li>
<li><code><a title="biobench.helpers.infer_batch_size" href="#biobench.helpers.infer_batch_size">infer_batch_size</a></code></li>
<li><code><a title="biobench.helpers.warn_if_nfs" href="#biobench.helpers.warn_if_nfs">warn_if_nfs</a></code></li>
<li><code><a title="biobench.helpers.write_hparam_sweep_plot" href="#biobench.helpers.write_hparam_sweep_plot">write_hparam_sweep_plot</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="biobench.helpers.progress" href="#biobench.helpers.progress">progress</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
